The paper proposes a system for generating conversational questions: questions conditioned on the whole flow of the conversation containing co-references rather than repeated explicit mentions. The experiments are based around the CoQA data conversational question answering data set.
The system extends the seq2seq with copy mechanism architecture with mechanisms that are targeted towards 1.) improving the system's ability to produce pronomial references

The paper proposes a system for generating conversational questions: questions conditioned on the whole flow of the conversation containing co-references rather than repeated explicit mentions. The experiments are based around the CoQA data conversational question answering data set.
They extend the seq2seq with copy mechanism architecture with mechanisms that are targeted towards 1.) improving the system's ability to produce pronomial references 2.) biasing to the system to shift its focus to later parts of the grounding paragraph as the dialogue progresses



The paper proposes a system for generating conversational questions: questions conditioned on the whole flow of the conversation containing co-references rather than repeated explicit mentions. The experiments are based around the CoQA data conversational question answering data set.
They extend the seq2seq with copy mechanism architecture with mechanisms that are targeted towards 1.) improving the system's ability to produce pronomial references 2.) biasing to the system to shift its focus to later parts of the grounding paragraph as the dialogue progresses 3.) biasing the system to focus on snippets around the correct answers when generating texts.

Problem 1.) is approached by running a SOTA coreference resolution system and adding explicit supervision at training time to maximize the attention probability and the probability of the correct pronoun at training time. Similarly 3.) is approached by collecting extra supervision through crowd sourced annotation of gold standard spans (IOB tags) in the paragraph for each answer. For 2.) they apply a positional encoding strategy to the input by training two positional embeddings: 1.) turn number in the dialogue, 2.) chunk number in the paragraph i.e.: they split the grounding paragraph into equal sized chunks. Then a self-attention mechanism is added to help the model learn that later turns require information from parts of the paragraph towards the end.

A major strength of the paper for me is when the authors move on to presenting the results. They compare the results of their full model to ablated versions and show that 1.) hierarchical encoding and attentions improves over flat, 2.) Show that when they only implement the extra coreference loss or the "conversation flow loss" both help independently, 3.) the model implementing all tricks performs the best. I think this approach to presenting results should be standard in the ACL community. Furthermore they show that the model performs well on the added auxiliary tasks as well, suggesting that the right bias was added at training time. Finally they also perform human evaluation, which is crucial in the generation field.

The first negative for me in the paper first of all is the framing. The authors write that they develop a novel loss function, which is an unnecessarily grand statement. It is not an issue in and of it self, but it would be more informative to explain it along the lines of adding extra supervision to bias the attention probabilities to be high for certain parts of the text. These biases come from apriori exploring the data set itself very carefully and designing the fitting the architecture to the benchmark: 1.) Answers in CoQA tend to be clearly located in the paragraph and as such it affords to have IOB tags signal spans, 2.) history size is fitted to the data set 
