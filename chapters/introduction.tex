%!TEX root = ../dissertation.tex

%EK: edits November 28

\chapter{Introduction}
\label{introduction}

Traditional distributional representations of linguistic units consider features extracted from large text-corpora in a single language.
The aim of the presented work is to learn representations of words, phrases up to sentences from multiple sources of information.
Specifically I focus on jointly learning representations for multiple languages, grounding in visual modality and the interaction of these
two views. Grounding in visual modality is largely motivated by evidence of perceptual grounding in human concept acquisition and
representation \cite{barsalou2003grounding} and as such it brings computational language learning systems closer to human-like
learning. Harnessing the visual modality to learn language representations that link linguistic knowledge
to the external world \cite{kiela2014improving,baroni2016grounding,elliott2017imagination,kiela2017learning,yoo2017improving}  has been empirically shown to
improve performance on several semantic tasks.
Traditionally, given a task for each language separate corpora are created and separate sets of parameters are learned.
Learning shared cross-lingual representations, however, allows researchers and practitioners
to train a single model allowing to transfer knowledge between languages leading to practical consequences such as mitigating the low-resource problem, cross-lingual
applications and processing data with code-mixing.
%Multilingual systems also offer a fertile ground computational typology as shared models are forced to exploit cross-lingual regularities.
The first part of the thesis Chapter 1 and 2. focuses on learning grounded representations for a single language.
In Chapter 1. we start by learning visual representations for words  using a novel computational cognitive model of cross-situational word learning that takes words
and high-level continuous image representations as input. Chapter 2. introduces a modern recurrent and convolutional neural network based model that learns from both
visual-grounding signals and word-word co-occurrences. Furthermore, we introduce a technique to interpret the learned representations of such architecture and investigate
if certain linguistic phenomena is encoded in the learned model. The second part of the thesis focuses on multimodal multilingual models. Chapter 3. provides evidence
that grounded learning can potentially improve machine translation and  in Chapter 4. we show under what conditions multilinguality an help improve grounded representations.

\section{Distributed representations of language}
My work focuses on learning grounded distributed sentence representations.
The first half of the thesis is about learning the representations of words and followed-up by learning sentence embeddings for a single language. Later we move on to learning multilingual visually grounded representations. In other works of mine I contributed to generating referring expressions grounded in Wikipedia entities and learning language through interacting with text-games grounded in a knowledge-base.

\subsection{Distributed word-representations}\todo{I have the material included that i wanted to in this section.}
\label{sec:words}
The distributional approach to word meaning hypothesizes that semantically related words
tend to appear in similar contexts. This idea goes back in linguistics tradition to the the
earlier days of American structuralism \cite{nevin2002legacy}. In the seminal paper
"Distributional structure" \cite{harris1954distributional} back in 1954 Harris already claims
that distribution should be taken as an explanation for word meaning and that similarity classes
can be constructed based on co-occurrence statistics.
\cite{cruse1986lexical} writes that 'It is  assumed  that  the  semantic properties  of
a lexical  item  are  fully  reflected  in  the  appropriate  aspects  of  the  relations
it  contracts  with  actual  and  potential  contexts' and that 'the  meaning  of  a word
is constituted  by  its  contextual  relations'. One of the first computational verification
attempts of  this distributional hypothesis was put forward in \cite{miller1991contextual} \todo{say something about the paper man}.
Here we discuss computational models of distributional word representations using the 'count-based' vs.
'prediction-based' distinction borrowed from \cite{baroni2014don}.
Early computational linguistics models of distributional semantics fall in the category of count-based approaches:
they count the number of times target words appear occur in different contexts resulting in a co-occurence matrix.
To these matrices various re-weighting schemes are applied usually followed by some matrix factorization algorithm.
The earliest approaches include Hyperspace Analogue to Language \cite{lund1996producing},
which constructs a term-term co-occurrence matrix and Latent Semantic Analysis \cite{dumais2004latent},
which applies the tf-idf re-weighting scheme on a term-document matrix followed by singular-value decomposition.
More recent approaches apply different re-weighting schemes such as pointwise mutual information and local mutual
information \cite{evert2005statistics} or different matrix factorization algorithms such as non-negative
matrix factorization \cite{baroni2014don}. For a comprehensive set of empirical experiments on count-based
approaches please consult \cite{bullinaria2007extracting} and \cite{bullinaria2012extracting}.
In more recent years -- and more related to the present thesis --
various deep learning methods have been applied to learn distributed word-representations usually referred to
as ´word-embeddings´ in the literature. Contrary to count-based methods prediction-based approaches fit into the
standard supervised learning pipeline: they optimize a set of parameters to maximize the probability of words
given contexts or contexts given words where the word-embeddings themselves form a subset of the parameters
of the full model. The first modern approach to neural language models on realistic data sets was introduced
in \cite{bengio2003neural}. It is a feed-forward multilayer perceptron with continuous word-embeddings,
a single hidden layer and a softmax output layer. The model is trained to maximize the probability of
the target word given the previous two words as context -- 2-gram language model -- trained by stochastic
gradient descent \cite{cauchy1847methode} through the backpropagation  algorithm \cite{rumelhart1985learning}.
The superior performance of the feed-forward neural language model has soon been shown to
improve performance in speech recognition \cite{schwenk2005training}.
The later convolutional architecture of  \cite{collobert2008unified} based on the time-delay neural network
model \cite{waibel1990phoneme} takes several steps towards the by now standard practices in neural NLP.
Contrary to the simple feed-forward network language model the convolution over-time structure can handle
sequences of variable length essential in NLP applications with sentences containing varying number of words.
The paper also introduces the idea of jointly learning many tasks at the same time such as part-of-speech
tagging, chunking, named entity recognition and semantic role labeling through multi-task learning.
Finally \cite{collobert2008unified} were the first to show the utility of pre-trained word-embeddings
in other tasks. Their architecture was later refined in \cite{collobert2011natural} and the pre-trained
full model was made available alongside the standalone word-embeddings in the SENNA toolkit.
The architecture, however, that arguably became most popular in NLP and is used in all my papers except
for Chapter~\ref{TAL}. is the recurrent neural network. It was argued that the simple recurrent network
architecture introduced by Elman is difficult to train for practical applications on longer sequences
\cite{bengio1994learning}, never the less the RNNLM implementation \cite{mikolov2010recurrent} established
a new state-of-the-art. It is worth noting here that to successfully
apply the standard backpropagation through time algorithm  \cite{williams1995gradient} to simple recurrent
networks on real world language modeling \cite{mikolov2012statistical} only needed to add the now
widely used gradient clipping trick. Later the word-embeddings learned by the recurrent language model were
shown to represent intriguing syntactic and semantic regularities \cite{mikolov2013linguistic} \todo{like what???}.
In earlier work where pre-trained word-embeddings were also shown for the first time through a
larger-scale empirical investigation by \cite{turian2010word} to transfer to sequence prediction tasks
when used as features in state-of-the-art systems.
However, it wasn't until the introduction of the much simpler and faster continuous bag-of-words and skip-thought
algorithms \cite{mikolov2013efficient} packaged into the easy to use word2vec toolkit that
word-embeddings became ubiquitous in computational linguistics and NLP research.
These algorithms rely on simple log-linear models as opposed to the more expensive
neural networks leading to faster training on larger corpora. The GloVe \cite{pennington2014glove}
model represents a hybrid between count- and prediction-based techniques;
it is a bi-linear model that optimizes word-embeddings to predict the re-weighted
word co-occurence statistics collected from corpora.

\subsection{Visually grounded representations of words}\todo{The references outline what i want i need to expand them still.}
\label{sec:visualwords}
\todo{ADD MORE FROM THIS https://arxiv.org/pdf/1806.06371.pdf}
The approaches to learn distributed representations discussed so far focus on extracting information exclusively
from text corpora. To human language learners, however, a plethora of perceptual information is available to aid the
learning process and to enrich the representations. The link between human word and concept representation and acquistion
and perceptual-motor systems has been well established through behavioral neuroscientific experiments \cite{pulvermuller2005brain}.
The earliest words in children's language acquistion tend to be names of concrete perceptual phenomena
such as objects, colors and simple actions \cite{bornstein2004cross} and children generalize to the names
of novel objects based on perceptual cues such as shape \cite{landau1998object}.
In general the \emph{embodiment} based theories of concept representation and acquisition in the
cognitive scientific literature puts forward the view that a wide variety of cognitive processes
are grounded in preception and action \cite{meteyard2008role}. The role of sensori-motor information in
language acquisition and representation is a highly debated topic \cite{meteyard2012coming}.
A number of
computational cognitive models of child language acquistion learn word meanings from small scale or synthetic
multi-modal data such as the model of \cite{yu2005emergence}, which uses visual information to learn the meanings of object
names or the method of \cite{roy2002learning} to learn to associate word sequences with simple shapes in a synthetically
generated data setting.
Interestingly already the articles introducing Latent Semantic Analysis
\cite{landauer1997solution} and Hyperspace Analogue to Language \cite{lund1996producing} already mention that a possible
limitation of the presented distributional semantic models is the lack of grounding in extra-linguistic reality.
\cite{landauer1997solution} puts it as "But still, to be more than an abstract system like mathematics words must
touch reality at least occasionally." The lack of relationship between symbols and the external reality is referred
to as the \emph{grounding problem} \cite{harnad1990symbol,perfetti1998limits}.
On the defense of purely textual models \cite{louwerse2011symbol} argue that the corpora
used to train distributional semantic models are generated by humans and as such reflect the perceptual world.
For a counter argument consider how many pieces of text to states obvious perceptual facts such as "bananas are yellow"
\cite{bruni2014multimodal}.
In practice much work on multi-modal distributional semantics have found that text-only spaces tend to
represent more encyclopedic knowledge, whereas mutli-modal representations capture more concrete aspects
\cite{andrews2009integrating,baroni2008concepts}. However, one does not necessarily need to
reach a conclusion on whether grounded or distributional models are superior, rather combining their merits in a
pragmatic way is an attractive alternative \cite{riordan2011redundancy}. Learning representations
from both linguistic and visual input is a step towards realistic models of language learners, note however,
it is as close as assuming children
learn language by sitting still and watching TV.
%T ADD MORE COGSCI NEUORO STUFF. maybe do the Symbol grounding problem \cite{harnad1990symbol} and empirical symbol grounding stuff \cite{glenberg2000symbol}.
The first approach to learn visual word representations from realistic data sets was introduced by \cite{feng2010visual}.
They develop a multi-modal topic model trained on a BBC News based containing the articles and the image illustrations.
The generative story of the multi-modal topic model starts by sampling mutli-modal topics, which generate both the
words and the image in the articles. To be able to apply topic models \cite{feng2010visual} represents images
in a discrete space using the bag-of-visual-words approach \cite{csurka2004visual}
a popular computer vision method at the time. They apply difference-of-Gaussians point detector to segment images
into local regions, each of which are represented by SIFT features \cite{lowe1999object}.
These features are then clustered with K-means, which leads to a bag-of-visual-words (BoVW) representation i.e.
a vector of counts, where each entry is the number of regions on the image that corresponds to certain SIFT-cluster.
Given count-based representations for both text documents and their paired images the standard Latent Dirichlet Allocation
topic modeling algorithm is applied; each word is represented by a vector, where each entry is the probability of that
word given a particular topic. Note that these topics are latent variables inferred from both BoW and BoVW vectors
hence represent a joint visual-textual multimodal space inferred from text documents paired with pictures. \cite{feng2010visual}
shows that the multi-modal LDA model outperforms the text-only LDA representations by a large-margin on word association
and word-similarity experiments. The perceptually grounded word representations \cite{bruni2012distributional} applies
a similar technique combining distributional semantic models and BoVW pipelines, but the data setting is
different in their case. They use images labeled with tags by annotators and construct a representation for each tag-word
by summing over the BoVW features of all images that are tagged with the word leading to visual-only
word-representations. For text-only models they construct several types of distributional semantic spaces from text-only
corpora unrelated to the images. Finally they use the concatenation of these two spaces as the multi-modal model.
On word-similarity benchmarks they show that the text-only model performs better than visual-only and that the combination
of the two surpasses both. They also find that distributional semantics models perform poorly on finding the typical
colors of concrete nouns, whereas the visual and multi-modal models perform perfectly. In these experiments
distributional semantics models fail to capture the obvious fact that ``the grass is green''
contradicting the theoretical argument that perceptual information is available in large collections of texts
and as such grounded representations are superfluos \cite{louwerse2011symbol}.
Combining bag-of-visual-words and count-based distributed word-representations was the standard methodology
in many other works on multi-modal word representations at the time \cite{bruni2011distributional,leong2011going,leong2011measuring}.
\cite{bruni2014multimodal} introduces a general multi-modal distributional semantics framework taking as input
separate textual and visual spaces followed by re-weighting and matrix factorization. Such a unified view allowed
to improve multi-modal representations using independent advancements in distributional semantics and image
feature-representations: \cite{kiela2014learning} uses skip-gram word-embeddings for the text-space
and the representations learned by convolutional neural network image classifier \cite{oquab2014learning}.
They show that CNN features outperform BoVW features on word-similarity experiments.
All approaches described so far require both visual and textual information for the same concepts.
The multi-modal skip-gram \cite{lazaridou2015combining} model was developed to alleviate such limiation:
it is a multi-task extension of the skip-gram algorithm predicting the both the context of words, but also
the visual representations of concrete nouns. Visual representations are constructed to averaging
the CNN representations \cite{krizhevsky2012imagenet} of 100 pictures sampled from ImageNet \cite{deng2009imagenet}.
This architecture was later proposed as a model of child language learning in and
was applied to the CHILDES corpus \cite{macwhinney2014childes} with modifications to model referential
uncertainty and social cues \cite{lazaridou2016multimodal} and compared to human learning performance \cite{lazaridou2017multimodal}.
Chapter~\ref{ch:TAL} presents a computational cognitive model of word learning using only
visual information developed at the same time as the multi-modal skip-gram approach. We combine
the incremental word-learning model of \cite{fazly.etal.10csj} with modern CNN image features.
We show throuh word-similarity experiments that our approach performs on par with the skip-gram
algorithm and that our visual word-embeddings represent concrete nouns closer to human judgements
then abstract nouns.
%MMFEAT toolkit for internet search style multimodal word embeddings \cite{kiela2016mmfeat}. Good results with image search style on STS \cite{glavavs2017if}.
%Cross-modal ranking models: Deep belief network to learn a joint probabilistic generative model of images and tags \cite{srivastava2012learning}, cross-modal correspondence auto-encoder \cite{feng2014cross}, image annotation with joint embeddings \cite{weston2010large}. Grounded sentence representations for ranking with dependency tree recursive networks \cite{socher2014grounded}.Unified RNN-CNN architecture for captioning and ranking \cite{kiros2014multimodal}. Learning to hash image-caption pairs \cite{jiang2016deep,cao2016deep}.

\subsection{From words to sentences}\todo{This section is very weak now gappy.}
\label{sec:sentences}
Applying the distributional intuition to model the meaning of sentences is not as straightforward for words.
Intuitively the number of words in a corpus is much greater than the number sentences and formally
one can assume a large, but finite set of existing words and a countably infinite set of possible sentences to
construct from them.
In fact, from a method to represent sentences in a continuous space one would hope that it can represent
unseen sentences at test time.
Earlier work developed under the framework of compositional distributional semantics produce continuous representations
for phrases up to sentences using additive and multiplicative interactions of distributed
word-representations \cite{mitchell2008vector} or combine symbolic and continuous representations with
tensor-products \cite{clark2007combining}. The latter line of work culminated in a number of unified theory of
distributional semantics and type logical grammars \cite{coecke2010mathematical,clarke2012context,baroni2014frege}.
In current practice, however, a strong baseline is to apply
a simple reduction operation on sentence matrices constructed from word embeddings
e.g.: summing or averaging word-embeddings in a sentence.
However, in natural language processing taking word-order into account is trivially cirucial.
Neural network models that handle variable length sequences of word-embeddings: a.)
recurrent networks -- Elman network \cite{elman1991distributed}, LSTM \cite{hochreiter1997long},
GRU \cite{cho2014properties} -- take the input sequentially b.) convolutional neural networks
\cite{kalchbrenner2014convolutional,zhang2015character,conneau2016very,chen2013learning} process fixed sized
n-gram patterns up to a large window, c.) recursive neural networks \cite{goller1996learning.socher2011parsing,tai2015improved}
take a tree data structure as input e.g.: the sentence according to the traversal of a dependency tree, while
d.) graph neural networks operate on graphs \cite{marcheggiani2017encoding} such as syntactic/semantic-dependency graphs.
These architectures compute a fixed size representations of sentences and are usually trained to perform a certain
task such as sequence tagging or sentence classification. Contrary to word-embeddings the representations
and the composition function these models learn are task specific and not \emph{universal}.
For learning transferable and general distributed sentence representations the first notable approach is the
skip-thought vectors model \cite{kiros2015skip}. This approach extends the skip-gram approch to sentences: it
trains sentence embeddings to be predictive of the sentences around it in running texts.
This method was confirmed to be a successful unsupervised method for transfer learning in a number of sentence
classifications tasks beating simpler approaches such as bag-of-words approaches based on skip-gram or CBOW embeddings,
tf-idf vectors and auto-encoders \cite{hill2016learning}. Their results also show that the different tasks encoders
aret trained on have a large impact on their downstream performance.
Later approaches have also focused on identifying supervised tasks such as natural language inference
 \cite{conneau-EtAl:2017:EMNLP2017} that lead to representations that generalize well to other tasks or to combine a
 number of supervised tasks with unsupervised training through multi-task learning \cite{subramanian2018learning}.
 The state-of-the-art in learning universal sentence representations at the time of writing the present thesis is
 represented by ELMo approach \cite{peters2018deep}, which trains a stack convolution and recurrent layers large-scale
 bidirectional language modeling. For each task the forward and backward representations are extracted from the model for each word position and then fed to a task specific recurrent network. Such contextualized word-representations have been explored before ELMo using the LSTM states of machine translation encoders \cite{mccann2017learned} pose a powerful alternative to universal sentence encoders.

\subsection{Visually grounded sentence representations}\todo{super difficult}
\label{sec:visualsentences}

When aiming to learn general sentence representations text-only tasks are usually considered by researchers
leadning to the grounding problem as discussed in Section~\ref{sec:visualwords}.
Given that visual information has been shown to contain useful information for word-representations it
is a natural question to ask whether this observation generalizes to sentence embeddings.
In \cite{chrupala2015learning} we train a multi-task architecture consisting of a shared word-embedding matrix
and two pathways: 1.) common RNN language model trained to maximize the probability of a word given the history,
2.) grounded representation learning model predicting the image representation paired with the sentences.
We show that the word-representations learned by the two path architecture predict human word-similarity
judgements with higher accuracy. Furthermore, the combined model is more sensitive to word-order than
the visual-only model.  Lastly we show that images act as an anchor and the architecture performs
paraphrase retrieval well above chance level. Chapter~\ref{ch:COLI} is dedicated to develop techniques
of interpretation and provide an in-depth comparative analyisis of the linguistic knowledge represented by
the language-model and visual-pathways of this architecture.
Using a similar architecture as our visual-pathway \cite{kiela2017learning} shows that concatenating grounded sentence
embeddings with the skip-thought embeddings lead to improvements on a large number of semantic
sentence classifications tasks. Their findings were confirmed and improved upon using a self-attention
mechanism \cite{yoo2017improving}.

 We show that in the domain of image-captions
grounded learning improves translation quality and that learning multi-modal representations
provides gains on top of learning from larger bilingual corpora \cite{elliott2017imagination}.
This lead us to the hypothesis that much of the observed improvements of multi-modal translation
models over text-only baselines is due to grounded learning and not to the effective use of visual context.
LATER DESMOND SHOWS THAT MODELS ARE NOT SENSITIVE TO CONTEXT CITE THE EMNLP PAPER. SOMEOW MENTION THE FOLLOWUP
HUMAN STUDY TOO.


\subsection{Multilingual representations of words and sentences}\todo{this will be challenging, not sure yet how to build it up.}
Multilingual or cross-lingual word embedding approaches map words of different languages in a shared space. These representations allow to transfer knowledge between languages and
perform cross-lingual tasks such as cross-lingual document retrieval. The bulk of such approaches apply the techniques developed for monolingual word-embedding induction as a component
of their full pipeline. One of the early approaches trains two separate word spaces with using the negative-sampling implementation of the skip-gram algorithm and then maps these spaces onto eachother by minimizing the squared loss between the 5000 top most frequent word-vectors \cite{mikolov2013exploiting}. This main approach went through improvents
such as by switching mean-squared error to ranking loss \cite{lazaridou2015hubness} or enforcing orthogonality constraint on the mapping matrix
\cite{xing2015normalized}   (WE DID THE SAME OVER TIME). Another prominent technique to learn a mapping between word (and other types of) spaces is Canonical Correlation Analysis, first
used by \cite{faruqui2014improving}, which was later improved by the application of Deep CCA \cite{lu2015deep}. Another approach is to create a pseudo-bilingual corpora by replacing words
in a source language by their translation and train an embedding model on the resulting corpus
The uber multilingual word-embedding \cite{ammar2016massively}.

\subsection{Visually grounded multilingual representations}\todo{not much work here so this will be easy.}
Image similarity for multilingual lexicon induction using images as pivot.
Super large-scale image-based word translation \cite{hewitt2018learning}.
First guys who did it \cite{bergsma2011learning}. These guys use VGG features and this image dispersion nonsense \cite{kiela2014improving}.
This is newer and different somehow \cite{vulic2016multi}.
Learning the meanings of nouns from image-search + similarity based techniques is fine, but verbs and adjectives
suck \cite{hartmann2017limitations}. Deep partial canonical correlation analysis for \cite{rotman2018bridging}
multilingual image-sentence embddings, using the multi-view idea.


\subsection{Grounding and other modalities}
Grounding words in auditory signals \cite{kiela2015multi,lopopolo2015sound} and
 olfactory perception \cite{kiela2015grounding}r. Growing body of literature in learning joint representations
 of speech and images \cite{harwath2016unsupervised,chrupala2017representations,harwath2018jointly}


Mention semantic parsing and the WebNLG paper i did with Thiago. Mention the TextWorld thing. Grzegorz
and Afra´s work speech, plus the fail paper that im on too.

\subsection{Interpreting continuous representations}\todo{This section could be infinite, but i tried to give a consice view on the matter.}
\label{sec:interpret}
Developing techniques for interpreting machine learning models have multiple goals. From the practical point of view as learning algorithms
make their way into critical applications such as medicine humans and machines need to be able to co-operate to avoid catastrophic outcomes \cite{caruana2015intelligible}.
As such there is a growing interest in deriving methods to \emph{explain} the decision of such architectures.

These methods assign a real-valued "relevance" score to each unit in the input signal, which signifies how much impact it had on the final prediction of the model.
One of the first paradigms in generating such relevance scores is gradient based methods: they take the gradient of the output of the network with respect to the input \cite{simonyan2013deep}.
Deep neural models of language tasks learn distributed representations of input symbols and as such further operations have to applied to reduce the resulting gradient vectors to
single scalars e.g. using $\ell_2$ norm \cite{bansal2016ask}. Another prominent and well studied approach still based on gradient information is layerwise
relevance propagation \cite{bach2015pixel}. The output of the final layer
is written as the sum of the relevance-scores from the input and similarly to the back-propagation algorithm the relevance of each neuron
recursively depends on the lower-layer all the way down to the input signal. Different versions of relevance propagation
run the backward pass with different rules taking as input gradient information and activation values. It
was later theoretically analyzed and generalized in to deep taylor decomposition method \cite{binder2016layer} and shown to be equivalent to taking the gradients with respect
to the input at in \citep{simonyan2013deep} and multiplying it elementwise with the input itself \cite{shrikumar2017learning}.
LRP was also later derived for recurrent architectures \cite{arras2017explaining} to describe sentiment classifiers.
Another backpropagation based algorithm is DeepLIFT, which instead of using the gradients for computing the relevance scores it uses the difference between the activations
that result from a specific compared  to a baseline input. Perturbation based methods are also gradient free. LIME \cite{ribeiro2016should} and its NLP specific extension
LIMSSE \cite{poerner2018evaluating} perturbs the
the input creating a local neighborhood around it and fits interpretable linear models to explain the predictions of any complex black box classifier.
Even simpler perturbation based techniques apply perturbations to the input and measure the difference between the original input and the various
perturbed candidates. The \emph{omission} method we develop in Chapter~\ref{ch:COLI} removes words from the input while the \emph{occlusion} \cite{li2016understanding}
method replaces these inputs with a baseline.  However, our aim in Chapter~\ref{ch:COLI} was not to develop a method for explanation, but to
shed light on the linguistic characteristics of the input grounded learning models learn in contrast to text-only language models.
This is in line with another angle of model interpretability; training a complex opaque model and then using it to discover patterns in the
input data that are crucial in solving the task. Deep neural networks learn to solve tasks from close to raw input that humans receive and as such
uncovering the regularities they learn can shed light into the patterns humans might extract from data to cope with certain tasks.
Recent methodology in probing the learned representations of LSTM language models, in fact, resemble psycholinguistic studies.
A number of experiments using the agreement prediction paradigm \cite{bock1991broken} suggest that
LSTM language models successfully learn syntactic regularities as opposed to memorizing surface patterns
\cite{linzen2016assessing,enguehard2017exploring,bernardy2017using,gulordava2018colorless}.

\section{Vision and Language applications}
Mention the bunch of work thats here. Do image-captioning and image-sentence ranking and then metion that we do the latter always. Say the importance of attention in this field and mention the DIDEC corpus. Mention VQA and fine-grained reasoning and the FigureQA paper.



\section{Methods}

\subsection{Data sets}
For image-feature extraction all approaches presented in the thesis use some
CNN architecture pre-trained on a version of the ImageNet data set \cite{deng2009imagenet} prepared for a large-scale image classification challenge \cite{russakovsky2015imagenet}. ImageNet annotates the noun synsets from WordNet \cite{miller1995wordnet} with images collected from the internet. At the time of writing this thesis on average the data set consists of 500 images per node. The subsection used in the pre-trained networks is the ILSVRC2012 containing 1.2 million images annotated with 1000 classes. For learning grounded language representation we use data sets introduced for image-captioning. These data sets annotate images found in online resources with descriptions through crowd-sourcing. The smallest data set we use is Flickr8K \cite{hodosh2013framing} with 8000 images, which was later extended to contain 30K images resulting in the Flickr30K benchmark \cite{young2014image} and finally the largest collection we use is the COCO image-caption benchmark \cite{chen2015microsoft} with 123K images. All data sets contain 5 captions per image.
The descriptions collected for these data sets are largely \emph{conceptual}, \emph{concrete} and \emph{generic}. This means that descriptions do not focus too much on \emph{perceptual} information such as colors, contrast or picture quality; they do not mention many \emph{abstract} notions about images such as mood and finally the descriptions are not \emph{specific} meaning that they do not mention the names of cities, people or brands of objects. What they do end up mentioning are entities depicted in the images (frizbee, dog, boy) their attributes (yellow, fluffy, young) and their relations between each other. The images depict common real-life scenes such as bus turning left or people playing soccer in the park. As such annotation collected independently from different workers end up focusing on different aspects of these scenes. For a comprehensive overview on image-description data sets please consult \cite{bernardi2016automatic}.
For our multi-lingual experiments we use the Multi30K data set \cite{elliott2016multi30k,elliott2017findings}; a multilingual extension of Flickr30K. It consists of a {\it translation} and a {\it comparable} portions both containing all images from the original Flickr30K. The \emph{translation} portion annotates a single English description with a German, French and Czech caption, while the \emph{comparable} the original 5 English and additional 5 German descriptions collected independently using the same crowd-sourcing process.


\subsection{Architectures}
Just do the usual GRU + CNN thing. cite Jamie (heart).

\subsubsection{Recurrent network}
Do the GRU mention that the LSTM is strictly more powerful according to Yoav, but it doesnt seem to matter at all according to Chung.

\subsubsection{Convolutional network}
Just super quick run-down on VGG and ResNet.

\subsubsection{Optimization}
Just mention that all this shit is optimized with some version of SGD and we use Adam all the time. Mention that Adam sucks but its just some common thing to do. Say this thing about the MSE or cosine loss that we start with that but i results in hubness actually as Angeliki points it out. So we move to the contrastive kinda losses.

\subsection{Transfer learning}
Don't really worry about it too much just say something about how awesome is that the CNNs trained on image net actually end up being so useful for us.

\subsection{Multi-task learning}
Multi-task learning has been an important paradigm since the earliest works on training neural architectures for natural-language processing \cite{collobert2008unified}. Multi-task learning involves optimizing multiple loss function jointly usually with the goal of exploiting commonalities between tasks. As described in the seminal work of \cite{caruana1997multitask}:

\begin{quote}
Multitask Learning is an approach to inductive transfer that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias. It does this by learning tasks in parallel while using a shared representation; what is learned for each task can help other tasks be learned better.
\end{quote}

Introducing inductive bias using different tasks means that the learning algorithm prefers a specific set of hypotheses over others just as in the case of other forms of regularization such as $\ell_1$ or $\ell_2$ penalties. Here I only discuss the specific instantiation of the mutli-task learning paradigm that are applicable to the chapters presented in this thesis and for a more comprehensive overview please consult \cite{ruder2017overview}. The technique used in the thesis
is hard-parameter sharing \cite{caruana1997multitask}
