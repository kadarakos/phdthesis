%!TEX root = ../dissertation.tex

%EK: edits November 28

\chapter{Introduction}
\label{ch:introduction}

The ability to understand natural language plays a central role in humans' conception of intelligent machines.
Alan Turing already in the 1950s in his now famous Turing Test(s) gives natural language understanding a 
key role in tricking humans into thinking that they are interracting with their fellow specimen rather 
than a machine. One of the goals of Natural Language Processing (NLP) is to develop algorithms
and build systems to help machines understand what humans are talking about; to 
understand the \emph{meaning} of natural language utterrances. 
In this thesis I explore computational techniques to 
\textbf{learn the meaning of words and phrases up to sentences}
considering the \textbf{visual world} as a naturally occuring 
\textbf{meaning representation briding between languages}. 
The methods presented in this thesis seek to find relationships between \textbf{images} and 
\textbf{natural utterrances} in various languages. 


The  Chapters of the thesis follow a progression 
starting with a single language at the word-level and arriving to multilingual visually grounded sentence
representations:

\begin{description}
\item[Chapter~\ref{ch:introduction}] the present chapter introduces the topic and contributions of 
the present thesis.

\item[Chapter~\ref{ch:background}] presents the related work and technical background in greater detail. 

\item[Chapter~\ref{ch:TAL}] presents
a cognitive model of language learning that learns \emph{visually grounded word representations}.

\item[Chapter~\ref{ch:CL} ] focuses on \emph{visually grounded sentence representations} and their 
intepretations from a linguistic perspective using the black-box architecture that is the basis 
for the chapters to follow: combination of a Convolutional Neural Network to extract visual features and
a Recurrent Neural Network to learn sentence embeddings.

\item[Chapter~\ref{ch:IJCNLP}] applies visually grounded representation learning approach that forms the
basis of Chapter~\ref{ch:CL} to \emph{improve machine translation} in the domain of visually descriptive language. 

\item[Chapter~\ref{ch:ConLL}] shows the clear benefints of learning visually grounded representations
for multiple languages jointly.

\item[Chapter~\ref{ch:EMNLP}] extends the investigations of Chapter~\ref{ch:ConLL} to the 
cross-domain setup breaking the assumption that for each language the same images are annotated
with different languages.
\end{description}
 


\section{Learning representations}
The foundational methodology applied in all chapters is \emph{statistical learning}. 
The approach taken during the earlier days of NLP was characterized by rule-based systems building on such 
foundations as Chomskyan theories of grammar or Montegue Semantics. 
Since the 1980s partly due to such theories falling out of fashion, but also to the increase in the amount 
of available computational power Machine Learning (ML) approahces revolutionized
the field.  \textbf{Learning} in general proved to be a crucial component to Artifical Intelligence and also
specifically in NLP. Machine Learning algorithms are designed with the goal that given an increasing 
number of examples a system improves its performance according to some measure of success.
Reflecting the structure of ML itself and the popularity of ML within the field, NLP research follows
a task-oriented methodology: researchers borrow or collect data sets, define measures of success and develop or
apply learning algorithms. In Chapters~\ref{ch:TAL}, \ref{ch:IJCNLP}, \ref{ch:ConLL} and
\ref{ch:EMNLP}  closely follow this blueprint.

From the rule based times of "engineering grammars" lot of researchers moved onto "engineering features"
to \textbf{represent} the textual data fed to general purpose pattern-recognition algorithms such as
decision-trees, support-vector machines or hidden Markov models. 
A large set of these features are still based on various formal-linguistic theories.
Intuitively, different applications such as machine translation or goal-oriented dialogue systems 
require different input representations. Furthermore, one would assume that various languages require 
different feature-extraction pipelines reflecting the typological differences across languages. 

\emph{Linguistic representation learning} challenges this intuition and is interested in 
discovering fundamental principles that allows machines to  \emph{learn} 
linguistic representaitons from \emph{data}, which are more-or-less
generally applicable. This line of work, as well as the approaches presented in the thesis,
fit in general with the \textbf{representation learning} 
framework consisting of machine learning approaches that learn useful representations 
for various tasks from (close to) raw input.

The expression "representation learning" is somewhat synonymous with "deep learning" at the time
of writing this thesis. When mentioning representation learning in the deep learning context
it is usually meant that the goal is to learn a function from raw input to target labels. In the 
context of this thesis, however, the emphasis is on learning representations of words, phrases and 
sentences that are potentially \emph{generally useful}, meaning 
that they can be used as input to in many tasks. This is sometimes referred to as 
\emph{transfer-learning} where we seek to indentify unsupervised learning objectives, 
supervised tasks, self-supervision schemes or the combinations of these to  learn representations that
perform well on a large variety of problems. 

\section{Learning representations of words}

%\begin{quote}
%Is there a way, however, to represent text in a more general way?
%\end{quote}

Most attempts to build  general representations for words are based on the 
\emph{distributional hypothesis} of word-meaning. It states that the degree to which words are similar 
is a function of the similarity of the linguistic contexts they appear in. In other words, similar 
words appear in similar contexts. Computational models of distributional
semantics implement this intuition and learn real-valued word vectors based on co-occurrence
statistics in large text corpora.
To aid the reader with techincal and historical context 
on the topic Section~\ref{sec:count} introduces earlier \emph{count-based} methods building word-context co-occurrence vectors-spaces, 
while  Section~\ref{sec:pred} presents the  \emph{prediction-based} framework in a more 
detailed fashion as it closely related
to the approaches presented in this thesis. 
Section~\ref{sec:w2v} details efficient-linear 
models for predictive word-learning for two main reasons: 1.) they a had a tremendous impact
on shaping the current landscape of continuous linguistic representation learning and are still
widely used at the time of writing this thesis, 2.) our main point of comparison between
our word learning model in Chapter~\ref{ch:TAL} is one of such models detailed in the section.


Word-representations within the latter framework are an instance on \emph{representation learning}:
word representations are usually learned through
optimizing model parameters to predict contexts from words or words from contexts.
Such learned word-representations have proven successful in many applications especially 
in recent-years, however, they are not \emph{realistic} in a certain sense. While they capture many
aspects of syntax and semantics of natural language they bare no relationship to the real-world
outside of large amounts of text-documents.
This leads us to the main topic of the thesis namely \emph{visual grounding} introduced in the
following section.

\section{Visually grounded word representations}

%\begin{quote}
%Can we learn useful linguistic representations just by reading texts?
%\end{quote}

As exposed in more detail in Section~\ref{sec:visualwords} many theories of human cognition supported by empirical
evidence state that human language and concept representation and acquistion is \emph{grounded}
in perceptual and sensori-motor experiences. Cross-situational word learning, 
an influential cognitive account to human word-learning, supposes that humans learn the
meanings of words exploiting repeated exposure to linguistic contexts paired with preceptual reality.
Learning representations for linguistic units in a visually grounded manner brings
computational language learning systems closer to human-like learning.

Furthermore, consider the applicability of distributional language represenations in the larger scope of Artificial 
Intelligence. One of the dreams of AI is to develop technology to power intelligent embodied agents
taking the form of office assistants or emergency aid robots. These machines cannot implement natural
language as an arbitrary symbol manipulation system akin to a calculator's understanding of 
magnitudes or slopes. 
Similarly to humans they need
to link linguistic knowledge to the extra-linguistic world. 

Furthermore, certain aspects of meaning are better captured by \emph{encyclopedic}
knowledge such as domestication abundant in textual data, however, perceptual information
can provide complementary valuable insights into physical properties rarely mentioned in texts such as size,
shape and color. In practice harnessing the visual modality to learn language
representations that link linguistic knowledge
to the external world  has been empirically shown to improve performance on several semantic tasks as
detailed in Section~\ref{sec:distvis}.

%Together the aforementioned cognitive 
%and practical considerations drove many works, including the Chapters of the present thesis, 
%to explore grounded linguistic representations.

In terms of computional modeling the jump from 
distributional to grounded models is conceptually simple: one needs to collect data where
the \emph{contexts} of linguistic units are \emph{extra-linguistic} and represent these contexts such
that they can be provided as input to representation learning methods.
More concretely in terms of extra-linguistic context the present thesis focuses on \emph{visual-modality}.

Linguistic-visual multi-modal representations on the word-level have a well established albeit somewhat breif history as
detailed in Section~\ref{sec:distvis}. Such methods fall in both the \emph{count-based}
and \emph{prediction-based} framework using computer-vision techniques to represent the \emph{visual
modality} and NLP methods to represent texts. These separate spaces are then combined into a single multi-modal representation.

As the \textbf{first contribtuion} of the thesis in Chapter~\ref{ch:TAL} we present an incremental cross-situational
model of word-learning introducing modern computer-vision techniques to computational cross-situational modeling of 
human language learning. Through our experiments we show that our presented model is competitive with state-of-the-art
\emph{prediction-based} distributional models and that our model can name relevant concepts given images.

\section{Visually grounded sentence representations}

When moving from \emph{atomic} words to the \emph{compositional} world of sentences we need flexible
models that can represent word-order and hierarchical relationships.  In Chapters~\ref{ch:COLI}, \ref{ch:IJCNLP}  \ref{ch:ConLL} and \ref{ch:EMNLP} 
we use Recurrent Neural Networks to represent sentences; a powerful class of  sequence models.
Section~\ref{sec:sentences} provides the reader with 
historical and technical background to the considerations behind this choice.

The study of general sentence representation learning has a much briefer history then word-representations
and Section~\ref{sec:trans-sentence} situates the reader in the area. Most of these approahces to date are 
based also on the distributional hypohtesis and formulate general purpose representation learning as a sort of
linguistic context prediction, but on the sentence level. 

Section~\ref{sec:visualsentences}
describes the general framework of learning visually-grounded sentence representations and their utility. 
The basic idea is still context-prediction, however, we learn associations between sentences and their \emph{visual} 
contexts i.e model parameters are optimized such that realted image--sentence 
pairs get pushed close together and unrelated pairs far from each other in a learned joint space.

As the \textbf{second contribution} of this thesis in Chapter~\ref{ch:COLI} 
we train such an architecture and explore the representations learned by such an architecture.
Our main interest and contribution here is the development general techniques to 
\emph{interpret} linguistic representations learned by
Recurrent Neural Networks and the use these tools to contrast text-only language 
models with their grounded counter-parts trained on the same sentences.

\section{Visual modality briding between languages}

One of the intriguing aspects of using the visual modality as a naturally occuring meaning representation is that it
is also naturally \emph{universal} across languages. 
The visual modality anchors linguistic representations to perceptual reality, but also
also provides a natural bridge between various languages. Linguistic utterrances that are similar 
to each other, intuitively, appear in the context of perceptually similar scenes across languages. 
Using this intuition on the word-level the visual modality can be used to find possible translations 
for words when no dictionary is available. Section~\ref{sec:bilinglex} provides the reader with more
detail on bilingual lexicon induction using the visual modality. 
Extending this idea to sentence level gives rise to techniques that
use the visual modality as a pivot to translate full sentences. Approahces in this direction are discussed
in Section~\ref{sec:imgpivot}. 

From another perspetive utterrances in multiple languages corresponding perceptual representations 
can be conceptualized as \emph{mutliple-views} of the same underlying \emph{ideal datapoint}.
Learning to map these multiple views in the same feature-space can lead to better representations as
they have to be more \emph{general} due to the model having to solve multiple tasks at the same time. 
This \emph{multi-view learning} perspective is explained in more detail in Section~\ref{sec:multiview} 
focusing on the specific case of multi-modal and multi-lingual representations.

The \textbf{third contribution} in the thesis combines visually grounded sentence representation learning
with machine translation. More specifically in Chapter~\ref{ch:IJCNLP} we present a \emph{multi-task} 
learning architecture that jointly learns to associate English sentences with images and to translate from
English to German. We show that grounded learning improves translation quality in our domain and that 
it provides orthogonal improvements to having extra parallel English-German translation training data.

The \textbf{fourth contribution} of this thesis learns visually grounded sentence representations for 
multiple languages jointly.  In Chapter~\ref{ch:ConLL} we show that higher quality grounded 
representations can be learned by training on multiple languages jointly. We find a consistent pattern
of imrpovement where by multilingual outperform bilingual, which outperform monolingual grounded
sentence representations. Furthermore, we provide empirical evidence that the quality of 
visually grounded sentence embeddings on lower -resource languages can be improved by jointly training
together with data sets from higher-resource languages.

Lastly our \textbf{fifth contribution} in Chapter~\ref{ch:EMNLP} is exploring the benefit of multilinguality
in visually grounded representation learning as in Chapter~\ref{ch:ConLL}, but in the cross-domain setting.
Here we consider a \emph{disjoint} setting where the image--caption data sets for different languages do
not share images. We assess how the method applied in Chapter~\ref{ch:ConLL} performs under 
domain-shift. Furthermore, we introduce a technique we call \emph{pseudopairs}, whereby we generate
new image--caption data sets by creating pairs across data sets using the sentence similarities under
the learned representations. We find that even though this technique does not require any additional 
external data source, models or other pipeline elements it improves image--sentence ranking performance.




\begin{comment}
\section{Background: Distributed linguistic representations}
\label{background}
As opposed to the continuous nature of the visual world, linguistic units
are inherently discrete. This is reflected in the techniques used in the
computational processing of natural language.
Information in linguistic utterances is traditionally encoded
in sparse high-dimensional count vectors,  each
dimension corresponding to the number of times a specific feature occurs in a
sample. For each word in a sentence common features to consider take the form of:
``this token is a noun'', ``the preceding word is cat'',
``the following token has dependency label direct object''.
More often then not these vectors do not even hold counts, but 0-1
indicator features signaling the presence or absence of a particular property.

In such a setting the relationships between features are not handled.
As a result machine learning algorithms are presented with an input representation where
``the preceding word is cat'' and ``the preceding word is dog''
are completely independent properties.
Furthermore, interactions between features has to be
explicitly represented as a separate entry in the vector, which once again has
no clear relationship to its parts as far as the learning algorithms are concerned.

Let us consider for a simple example the problem of language modeling.
Our  aim is to find a probabilistic model
that assigns high probability to likely sequences and low probability to gibberish.
The representation of words and phrases in common n-gram language models is on the
very extreme of discrete: they are represented by their \emph{identity}.
Imagine that in the training set there were a number of sentences expressing the concept
of ``someone is walking a dog'', a little less mentions of ``someone walking a cat''
and no occurrence of ``someone walking a serval'', but the word ``serval'' did occur.
I have never seen anyone walking a serval, however, as a human I do know that it is a dog-sized
feline creature and as a consequence I can imagine someone taking a walk with one.
However, a language model trained with discrete symbolic representation cannot
\emph{generalize} in such a way as the
relationship between the words ``dog'', ``cat'' and ``serval'' is not represented.

Consider now the case of walking a dolphin.
Both ``walking a serval'' and ``walking a dolphin'' would be impossible
by the hypothesized language model, however, us humans would say that walking the serval is
kind of possible, but walking the dolphin is just surreal.
We know it from reading about both animals that they can be domesticated
, however, by their physical properties while one affords to be walked
the other does not. Dogs, cats, servals and dolphins are similar in some
dimensions and different in others.
What the present thesis is concerned about is
\emph{learning continuous representations} of words, phrases up to sentences
from multiple sources of information to encode such useful properties.
Such representations allow machine learning algorithms to represent smooth
functions over linguistic inputs where the local neighborhoods of linguistic
units correspond to syntactic and semantic regularities.

\section{Grounded and multilingual representations}
The well established tradition of continuous word representations consider
features extracted from large text-corpora in a single language focusing on
co-occurrence statistics between words. 
This distributional approach to word meaning hypothesizes that semantically related words
tend to appear in similar contexts. This idea goes back in linguistics tradition to the
earlier days of American structuralism \citep{nevin2002legacy}. In the seminal paper
"Distributional structure" \citep{harris1954distributional} back in 1954 Harris already claims
that distribution should be taken as an explanation for word meaning and that similarity classes
can be constructed based on co-occurrence statistics.
\cite{cruse1986lexical} writes that 'It is  assumed  that  the  semantic properties  of
a lexical  item  are  fully  reflected  in  the  appropriate  aspects  of  the  relations
it  contracts  with  actual  and  potential  contexts' and that 'the  meaning  of  a word
is constituted  by  its  contextual  relations'. Computational models of distributional
semantics implement this intuition and learn real-valued word vectors based on co-occurrence
statistics in large text corpora. Section~\ref{sec:words} presents 
such representations based on the \emph{count-based} in Section~\ref{sec:count}
versus \emph{prediction-based} in Section~\ref{sec:pred}
distinction borrowed from \cite{baroni2014don}. Most architectures presented  
in this thesis are based on the developements in Neural Language Models detailed
in Section~\ref{sec:NNLM}. Here we present the methods focusing on the historical context,
but combined with some standard notation
providing background for later chapters. Section~\ref{sec:w2v} introduces efficient-linear 
models for predictive word-learning for two main reasons: 1.) they a had a tremendous impact
on shaping the current landscape of continuous linguistic representation learning and are still
heavily used at the time of writing this thesis, 2.) our main point of comparison between
our word learning model in Chapter~\ref{ch:TAL} is one of such models detailed in the section.


My thesis focuses integrating the \emph{visual modality} and jointly learning representations 
of words up to sentences in \emph{multiple languages}. Furthermore, I study the beneficial interaction of
visually grounded and multilingual representation learning.
Grounding linguistic representattions in visual modality is largely motivated by evidence from
perceptual grounding in human concept acquisition and representation \citep{barsalou2003grounding}.
As such it brings computational language learning systems closer to human-like learning.
Furthermore, as pointed out by the dolphin versus serval example in the Preface
certain aspects of meaning are better captured by \emph{encyclopedic}
knowledge such as domestication abundant in textual data, however, perceptual information
can provide complementary valuable insights into physical properties rarely mentioned in texts such as size,
shape and color. In practice harnessing the visual modality to learn language
representations that link linguistic knowledge
to the external world \citep{kiela2014improving,baroni2016grounding,elliott2017imagination,kiela2017learning,yoo2017improving}
has been empirically shown to improve performance on several semantic tasks.

\paragraph{Images bridging languages}

Traditionally, given a task for each language separate corpora are created and
separate sets of parameters are learned.
Learning shared cross-lingual representations, however, allows researchers
and practitioners to train a single model allowing to transfer knowledge
between languages leading to practical consequences
such as mitigating the low-resource problem, cross-lingual applications and
processing data with code-mixing.
Perceptual grounding and multilingual representations have their separate benefits,
however, they can be beneficial to each other as well.
In my work I view images as universal meaning representations, a natural common
ground bridging between languages. From another perspective images and multiple
languages can be seen as multiple latent views of the same underlying semantic
concepts. This approach allows us to take advantage of complementary information
and potential transfer between these views.

\paragraph{Contributions}
In Chapter~\ref{ch:TAL} we start by learning visual representations for words using a
novel computational cognitive model of cross-situational word learning that
takes words and high-level continuous image representations as input. Our approach
integrates recent advances of computer vision into incremental cognitive models
of language learning. We show that on the data sets that we consider our model
is competitive with state-of-the-art distributional semantics models (word2vec)
on word-similarity benchmarks. Furthermore we show that our model is able to
name relevant concepts given images.

Chapter~\ref{ch:COLI} introduces a recurrent and convolutional neural network
based model that learns from both visual-grounding signals and
word-word co-occurrences. We develop techniques to interpret the learned
representations of such an architecture and investigate if certain linguistic
phenomena is encoded in the learned model.

Chapter~\ref{ch:IJCNLP} provides empirical evidence that grounded learning
can improve machine translation quality.
In Chapter~\ref{ch:ConLL} we show under what conditions multilinguality an help improve grounded
representations.
%Multilingual systems also offer a fertile ground computational typology as shared models are forced to exploit cross-lingual regularities.

%My work focuses on learning grounded distributed sentence representations.
%The first half of the thesis is about learning the representations of words and followed-up by learning sentence embeddings for a single language. Later we move on to learning multilingual visually grounded representations. In other works of mine I contributed to generating referring expressions grounded in Wikipedia entities and learning language through interacting with text-games grounded in a knowledge-base.

\end{comment}

\chapter{Background}
\label{ch:background}

\section{Distributed word-representations}
\label{sec:words}
The distributional approach to word meaning hypothesizes that semantically related words
tend to appear in similar contexts. This idea goes back in linguistics tradition to the
earlier days of American structuralism \citep{nevin2002legacy}. In the seminal paper
"Distributional structure" \citep{harris1954distributional} back in 1954 Harris already claims
that distribution should be taken as an explanation for word meaning and that similarity classes
can be constructed based on co-occurrence statistics.
\cite{cruse1986lexical} writes that "It is  assumed  that  the  semantic properties  of
a lexical  item  are  fully  reflected  in  the  appropriate  aspects  of  the  relations
it  contracts  with  actual  and  potential  contexts" and that "the  meaning  of  a word
is constituted  by  its  contextual  relations". Computational models of distributional
semantics implement this intuition and learn real-valued word vectors based on co-occurrence
statistics in large text corpora. 
Here we present such representations using the \emph{count-based}
versus \emph{prediction-based} distinction borrowed from \cite{baroni2014don}.
%One of the first computational verification
%attempts of  this distributional hypothesis was put forward in \cite{miller1991contextual} \todo{say something about the paper man}.

\subsection{Count-based approaches}
\label{sec:count}
Early computational linguistics models of distributional semantics fall in the category of
count-based approaches: they count the number of times target words appear occur in different
contexts resulting in a co-occurrence matrix. In the co-occurrence matrix
$W$ each row $W_i$ represents a word $i$ and each column $W_{,j}$ a context. It has size
is a $W \in \mathbb{N}^{|V| \times |C|}$ where $V$ is the set of words and $C$ is
the set of contexts.
The entry $W_{i,j}$ is the number of times word $i$ appears in context $j$.
Contexts are typically words appearing within a certain window size or text documents.
To the counts in $W$ various re-weighting schemes are applied followed by some matrix factorization algorithm
resulting in a lower dimensional dense representation $W \in \mathbb{R}^{|V| \times d}$, where $d < |C|$.
%Both re-weighting and low-rank approximation reduces the dimensionality of the sparse matrix and leads
%to faster compute and better generalization.
The earliest approaches include Hyperspace Analogue to Language \citep{lund1996producing},
which constructs a term-term co-occurrence matrix and Latent Semantic Analysis \citep{dumais2004latent},
which applies the tf-idf re-weighting scheme on a term-document matrix followed by singular-value decomposition.
More recent approaches apply different re-weighting schemes such as point wise mutual information and local mutual
information \citep{evert2005statistics} or different matrix factorization algorithms such as non-negative
matrix factorization \citep{baroni2014don}. For a comprehensive set of empirical experiments on count-based
approaches please consult \cite{bullinaria2007extracting} and \cite{bullinaria2012extracting}.

\subsection{Prediction-based approaches}
\label{sec:pred}
In more recent years
%-- and more related to the present thesis --
various deep learning methods have been applied to learn distributed word-representations usually referred to
as ´word-embeddings´ in the literature. Contrary to count-based methods prediction-based approaches 
fit into the
standard machine learning pipeline: they optimize a set of parameters to maximize the probability of words
given contexts or contexts given words, where the word-embeddings $\mathbf{W}$
themselves form a subset of the parameters of the full model. 

\subsubsection{Neural language models}
\label{sec:NNLM}
The first modern approach to learn distributed word-representations from realistic data
was the neural language introduced by \cite{bengio2003neural}; 
laying down the framework for recent developments.
They present a feed-forward multilayer perceptron with continuous word-embeddings,
a single hidden layer and a softmax output layer and train it as a language model.
More precisely the model is parametrized by a 1.) word-embedding matrix
$\mathbf{W} \in \mathbb{R}^{|V| \times d}$, where each row corresponds to a word-vector
$\mathbf{W_i}$ of size $d$ and each cell $\mathbf{W_{i,j}}$ to a learned feature, 2.)
learned hidden and output weight-matrices $\mathbf{H}$ and $\mathbf{U}$. Similarly
to the case of count-based embeddings $d < |V|$.
The network takes as input the concatenation of $n$ word-vectors preceding the target word as context
representation $\hat{\mathbf{w}} = [\mathbf{w_1}; \ldots; \mathbf{w_{n-1}}]$
and outputs the probability distribution over the current word $w_n$.
The computation of the network is shown in Equation~\ref{eq:bengio}:

\begin{equation}
\label{eq:bengio}
P(w_n|w_{<n}) \approx \text{softmax}(\mathbf{U} \text{tanh}(\mathbf{H} \mathbf{\hat{w}} + b_h ) + b_u)
\end{equation}

where tanh is the hyperbolic tangent function applied elemetwise

\begin{equation}
\label{eq:tanh}
\text{tanh(x)} = \frac{e^{2x} - 1 }{e^{2x} + 1 }
\end{equation}

and softmax is a commonly used function to map a vector of unnormalized scores to categorical probability 
distribution:

\begin{equation}
softmax(\mathbf{x}) = \frac{e^{x_{i}}}{\sum_{k=1} e^{x_k}}
\end{equation}

The model is trained to maximize the probability of
the target word given the previous fixed number of words as context over a training corpus
-- $n$-gram language model -- trained with stochastic gradient descent \citep{cauchy1847methode}
through the backpropagation algorithm \citep{rumelhart1985learning}.
The superior performance of the feed-forward neural language model has soon been shown to
improve performance in speech recognition \citep{schwenk2005training}.

Following a similar recipe the convolutional architecture of \cite{collobert2008unified}
based on the time-delay neural network model \citep{waibel1990phoneme} took several steps towards the
by now standard practices in deep NLP.
Contrary to the simple feed-forward network language model the convolution over-time structure can handle
sequences of variable length, which is essential in NLP applications where sentences
are composed from a varying number of words.
The paper also introduces the idea of jointly learning many tasks at the same time such as part-of-speech
tagging, chunking, named entity recognition and semantic role labeling through \emph{multi-task learning}.
Their architecture was later refined in \cite{collobert2011natural} and the pre-trained
full model was made available alongside the standalone word-embeddings in the SENNA toolkit.
Finally, \cite{collobert2008unified} were the first to show the utility of pre-trained word-embeddings
in other tasks. 

We also make extensive use of the multi-task learning strategy:  Chapters~\ref{ch:COLI} for
a language modeling and image-sentence ranking, in Chapter~\ref{ch:IJCNLP} for machine translation
and image sentence ranking and in Chapters~\ref{ch:ConLL} and \ref{ch:EMNLP} for image sentence ranking
in multiple languages.


The architecture, however, that arguably became most popular in NLP and is used in
Chapters~\ref{ch:COLI}, \ref{ch:IJCNLP}, \ref{ch:ConLL} and \ref{ch:EMNLP} 
is the recurrent neural network (RNN). The architecture was
first introduced by the late Jeffrey L. Elman in his seminal paper  'Finding Strucutre in Time'
 \citep{elman1990finding}
as a model of human language learning and processing. Recurrent neural networks
take a variable length sequence as input and perform a stateful computation over the it's elements "reading"
the input from left-to-right.
In case of language modeling at each time-step $t$ the network takes an input word vector $\mathbf{w_t}$
and the it's previous hidden state $\mathbf{h_{t-1}}$ maintained through previous 
time-steps to compute the current
state $\mathbf{h_t}$. and to predict the probability distribution over the following word $w_t$.
It is parametrized a word-embedding matrix $\mathbf{W} \in \mathbb{R}^{|V| \times d}$, 
input to hidden weight matrix $\mathbf{W_i}$, hidden to hidden weight 
matrix $\mathbf{W_h}$ and finally the hidden to output
weight matrix $\mathbf{U}$ used to predict the unnormalized probabilities over the vocabulary entries:

\begin{align}
P(w_n|w_{n<n}) &\approx \text{softmax}(\mathbf{U} \mathbf{h_t} + \mathbf{b_o}) \\
\mathbf{h_t} &= \text{tanh}(\mathbf{W_h}\mathbf{h_{t-1}} + \mathbf{W_i}\mathbf{w_t} + \mathbf{b_h})
\end{align}

This model is also trained to maximize the probability of the traning sequences, but rather than standard
backpropagation it is optimized with the backpropagation through time algorithm (BPTT)
\citep{robinson1987utility,werbos1988generalization,williams1995gradient}.

\cite{elman1991distributed} shows when trained on simple natural language like input the
hidden states of the network $\mathbf{h_t}$ encode grammatical relations and hierarchical
constituent structure. In Chapter~\ref{ch:COLI} we also train a recurrent language model
and compare it to a its grounded counterpart 
on real-world data and explore similar questions about the learned opaque representations 
as \cite{elman1991distributed} .

Despite the early successes, however, it turned out to be difficult to train
Elman networks on practical applications with longer sequences
due to the vanishing and exploding gradient phenomena \citep{bengio1994learning}.
Never the less the RNNLM implementation of \cite{mikolov2010recurrent}
established a new state-of-the-art. Interestingly the successful
application the standard BPTT algorithm
to simple RNNs on real world language modeling \cite{mikolov2012statistical}
seems  only to have required more computational resources and 
the now widely used gradient clipping trick.

At the time of writing this thesis an overwhelming amount of empirical evidence
shows that long short-term memory (LSTM) \citep{hochreiter1997long,gers1999learning}
and gated recurrent unit networks (GRU) \citep{cho2014learning}
vastly outperform the simple Elman network in practice. We use GRUs in
Chapters~\ref{ch:COLI}, \ref{ch:IJCNLP}, \ref{ch:ConLL} and \ref{ch:EMNLP}.
Gated recurrent unit networks use a particular \emph{memory structure}, which
add an inductive bias to the network so it can learn to \emph{carry information}
keeping a portion of the components of the hidden state $\mathbf{h_t}$
constant over time:

\begin{align}
\tag{update-gate}
\mathbf{z_t} &= \sigma(\mathbf{W_z} \mathbf{w_t} + \mathbf{U_z} \mathbf{h_{t-1}} + \mathbf{b_z}) \\
\tag{reset-gate}
\mathbf{r_t} &= \sigma(\mathbf{W_r} \mathbf{w_t} + \mathbf{U_r} \mathbf{h_{t-1}} + \mathbf{b_r})  \\
\tag{memory content}
\mathbf{\tilde{h_t}} &= \text{tanh}(\mathbf{W_h} \mathbf{w_t} + \mathbf{r_t} \odot  \mathbf{U_h} \mathbf{h_{t-1}}) \\
\tag{hidden state}
\mathbf{h_t} &= (1-\mathbf{z_t}) \odot \mathbf{h_{t-1}}  \mathbf{z_t} \odot \mathbf{\tilde{h_t}}
\end{align}

where $\sigma$ is the sigmoid function:

\begin{equation}
\sigma(x) = \frac{e^x}{e^x + 1}
\end{equation}

As the sigmoid function has a range of $[0,1]$ it is intuitive to undestand the GRU as a sort of
sequential computer with soft/continuous read-write memory operations: the reset-gate $\mathbf{r_t}$
decides how much of each component of the previous state is relevant to be mixed in with the
current input, resulting in the current candidate memory state $\mathbf{\tilde{h_t}}$. The output-gate
then trades of the previous state with the current candidate.



\subsubsection{Efficient linear models}
\label{sec:w2v}
Despite the success of deep learning architectures in NLP it wasn't until the introduction 
of the much simple and fast continuous bag-of-words
and skip-gram with negative sampling (SGNS) algorithms \cite{mikolov2013efficient}
packaged into the easy to use \texttt{word2vec} toolkit that
word-embeddings became ubiquitous in computational linguistics and NLP research.
These algorithms rely on simple log-linear models as opposed to the more expensive
neural networks leading to faster training on larger corpora. The more successful SGNS
model has two learnable word embeddings matrices one for the target words
$\mathbf{W_w}  \in \mathbb{R}^{|V| \times d}$
and a separate one for the contexts $\mathbf{W_c} \in \mathbb{R}^{|V| \times d}$.
The model is trained to maximize the dot product $\mathbf{w}^T\mathbf{c}$
between true word-context pairs $(w,c)$ appearing in corpus and minimize the
dot product $\mathbf{w}^T\mathbf{\hat{c}}$ between randomly
samples contrastive examples $(w,\hat{c})$ through  maximizing the following loss function

\begin{equation}
J(\theta) = \prod_{(w,c) \in D} p(D=1|w,c;\theta)  \prod_{(w,c) \in D'}  p(D=0|w,c;\theta) 
\end{equation}

where  $p(D=1|w,c)$ is the probability that $(w,c)$ is from the true corpus, $p(D=0|w,c)$ is the
probability that it did not and $D'$ is the articial corpus of negative pairs.
This kind of negative-sampling or
contrastive-sampling strategy is used in ranking type loss functitons 
implemented in the approaches  in Chapters~\ref{ch:IJCNLP} \ref{ch:ConLL} and ~\ref{ch:EMNLP}
to match images $i$ and corresponding sentences $s$ and push contrastive examples $(i',s)$ 
or $i,s'$ far from each other in the learned multi-modal space.

Finally the GloVe approach \citep{pennington2014glove} is another simple linear model
with word $\mathbf{W_w}  \in \mathbb{R}^{|V| \times d}$ and context
$\mathbf{W_c} \in \mathbb{R}^{|V| \times d}$ embeddings, which
represents a hybrid between count- and prediction-based techniques:
it optimizes word-embeddings to predict the re-weighted
$\log$ co-occurrence counts $\log(\#(w,c))$ collected from corpora $D$:

\begin{equation}
\label{eq:glove}
\mathbf{w}^T\mathbf{c} + b_w + b_c = \log(\#(w,c))\;\;\; \forall (w,c \in D)
\end{equation}

where $b_w$ and $b_c$ are context specific bias terms.
Now let us express the Glove model in the traditional re-weighted
co-occurrence matrix factorization form.
Let the matrix $M \in \mathbb{R}^{|V| \times |C|}$ store the log-weighted
weighted counts and $\mathbf{b_w}$ and $\mathbf{b_c}$ be the vectors of all
$b_w$ and $b_c$ scalars, then

\begin{equation}
\label{eq:glove2}
M \approx \mathbf{W} \cdot \mathbf{C^T} + \mathbf{b_w} + \mathbf{b_c}
\end{equation}

In fact there has been work on finding relationships between
count- and prediction-based methods \citep{levy2014neural} and
using insights from both to develop novel improved
variants \citep{levy2015improving}.

%\paragraph{Transferring word representations}

%Later the word-embeddings $\mathbf{W}$ learned by recurrent language models were
%shown to represent intriguing syntactic and semantic regularities \cite{mikolov2013linguistic}.

%In earlier work where pre-trained word-embeddings were also shown for the first time through a
%larger-scale empirical investigation by \cite{turian2010word} to transfer to sequence prediction tasks
%when used as features in state-of-the-art systems.


\section{Visually grounded representations of words}
\label{sec:visualwords}
% \todo{ADD MORE FROM THIS https://arxiv.org/pdf/1806.06371.pdf}

\subsection{Language and perception}
\label{sec:langperc}

The approaches to learn distributed representations discussed so far focus on extracting information exclusively
from text corpora. To human language learners, however, a plethora of perceptual information is available to aid the
learning process and to enrich their mental representations. 
The link between human word and concept representation
and acquisition and the perceptual-motor systems has been well established through behavioral
neuroscientific experiments \citep{pulvermuller2005brain}.
The earliest words children learn tend to be names of concrete perceptual phenomena
such as objects, colors and simple actions \citep{bornstein2004cross}. Furthermore, children generalize to
the names of novel objects based on perceptual cues such as shape or color \citep{landau1998object}.
In general, the \emph{embodiment} based theories of concept representation and acquisition in the
cognitive scientific literature put forward the view that a wide variety of cognitive processes
are grounded in perception and action \citep{meteyard2008role}. That said, the precise role of
sensori-motor information in language acquisition and representation is a highly
debated topic \citep{meteyard2012coming}.

Motivated by such cognitive theories and experimental data 
various computational cognitive models of child 
language acquisition investigate the issue
of learning word meanings from small scale or synthetic multi-modal data. The model presented by
\cite{yu2005emergence} uses visual information to learn the meanings of object
names whereas the architecture of \cite{roy2002learning} learn to associate word sequences
with simple shapes in a synthetically generated data setting.

Interestingly even the articles introducing Latent Semantic Analysis
\citep{landauer1997solution} and Hyperspace Analogue to Language \citep{lund1996producing} mention
that a possible limitation of the presented distributional semantic models is the lack of
grounding in extra-linguistic reality. \cite{landauer1997solution} puts it as "But still, to be more than
an abstract system like mathematics words must touch reality at least occasionally."
The lack of relationship between symbols and the external reality is usually referred
to as the \emph{grounding problem} in the literature \citep{harnad1990symbol,perfetti1998limits}.
On the defense of purely textual models \cite{louwerse2011symbol} argues that the corpora
used to train distributional semantic models are generated by humans and as such reflect the perceptual world.
For a counter argument consider how many pieces of text to state obvious perceptual
facts such as "bananas are yellow" or how often do objects holding the "yellow" property
appear in similar textual contexts \citep{bruni2014multimodal}.

In practice much work on multi-modal distributional semantics have found that text-only spaces tend to
represent more encyclopedic knowledge, whereas multi-modal representations capture more concrete aspects
\citep{andrews2009integrating,baroni2008concepts}. In Chapter~\ref{ch:TAL}, where we develop
a cross-situational cognitive model of word-learning, we also find that the word-representations learned by
our model correlate better with human similarity judgements on more concrete words. In contrast 
word-embeddings learned by the SGNS algorithm perform better on more abstract words.

One does not necessarily need to
reach a conclusion, however, on whether grounded or distributional models are superior; 
combining their merits in a
pragmatic way is an attractive alternative \citep{riordan2011redundancy}. Learning representations
from both linguistic and visual input is a step towards realistic models of language learners.
%Note however, it is as close as assuming children learn language by sitting still and watching TV.

\subsection{Combined distributional and visual spaces}
\label{sec:distvis}

When learning multimodal word representations we wish to construct a matrix $W$, where each row $W_i$
corresponds to a word and each feature column $W_{,j}$ represents a distributional feature,
a perceptual feature or mixture of the two.

The first approach to learn visual word representations from realistic data sets
was introduced by \cite{feng2010visual}. They develop a multi-modal topic model trained on a
BBC News based data set containing text articles with image illustrations.
Each document considered by their model
is a pair $<D, I>$ comprised of a text document $D$ and image $I$.
The documents $D$ are represented as bag-of-words vectors (BoW) -- as common in the
\emph{count-based} distributional semantics framework -- while images are represented
as bag-of-visual-words (BoVW)  \citep{csurka2004visual}.  The BoVW representations are
obtained by first applying the difference-of-Gaussians point detector to segment images
into local regions, which are then represented by SIFT features \citep{lowe1999object}.
Finally, these extracted features are clustered with K-means resulting in the
bag-of-visual-words  representation i.e.
a vector of counts, where each entry is the number of regions on the image that
corresponds to a certain SIFT-cluster.  Given both the BoW  $\psi(D)$ and
BoVW $\phi(I)$ feature functions each pair $<D, I>$ can be represented as concatenation
of the application of the two $[\psi(D), \phi(I)]$.

 This if followed by training multi-modal
 topic model, specifically Latent Dirichlet Allocation \citep{blei2003latent}
on top of the joint representations of the documents and images.
The hierarchical bayesian generative story of multi-modal LDA  starts by sampling multi-modal topics,
which then generate both the English words and visual words.
After convergence each word is represented by a vector,
where each component corresponds to the conditional probability of that word given 
a particular multi-modal topic. \cite{feng2010visual}
shows that the multi-modal LDA model outperforms the text-only LDA representations by a
large-margin on word association and word-similarity experiments.

The perceptually grounded word representations of \cite{bruni2012distributional}
also combines distributional semantic models and BoVW pipelines.
Rather than having a collection of documents $<D, I>$ they consider a set of words for, which
both distributional and image features are available. For visual word representations
they use images labeled with tags by annotators. Similarly to \cite{feng2010visual} they
apply the BoVW feature extraction pipeline with SIFT and color features to
represent images. Contrary to \cite{feng2010visual}, however,
a visual-only representation is created for each tag-word by summing over the features
for all images corresponding to the tag.
For text-only models they construct several types of distributional semantic spaces
from text-only corpora unrelated to the images.
Note that as in the previous example they wish to create a multimodal
word representation and applied separate functions to extract textual $\psi$ and visual
$\phi$ features. Finally they create the multi-modal space through concatenation.

On word-similarity benchmarks they show that the text-only model performs better than visual-only
and that the combination of the two surpasses both.
They also find that distributional semantics
models perform poorly on finding the typical colors of concrete nouns,
whereas the visual and multi-modal models perform perfectly. In these experiments
distributional semantics models fail to capture the obvious fact that ``the grass is green''
providing evidence against the theoretical argument that perceptual information is available in
large collections of texts and so grounded representations are superfluous \citep{louwerse2011symbol}.


Combining BoVW and count-based distributional word-representations remained the standard methodology
in many other works on multi-modal word representations at the time
\citep{bruni2011distributional,leong2011going,leong2011measuring}.
\cite{bruni2014multimodal} frames multi-modal distributional semantics under a general framework: 
create separate textual and visual features for words followed by
re-weighting and matrix factorization. Researchers can apply vector-space
models to create the distributional space $W^d$ and computer vision techniques to create
visual feature representations for the same words $W^v$. The separate feature spaces
are mixed together  first  by concatenation $W = [W^d;W^v]$, which is
optionally followed by singular value decomposition
to combine the two modalities.\footnote{Note that discovering latent factors through the application
of signular value decomposition on the concatenated textual and visual spaces in similar in spirit
to applying topic models such as LDA.} 

This framing allowed researchers to take advantage of evolving NLP and computer vision techniques
and experiment by plugging in various distributional semantics methods as textual feature extractors
$\phi$ and image processing techniques as $\psi$.
For example \cite{kiela2014learning} runs the SGNS algorithm
on large text corpora and takes the word-embedding parameters
$\mathbf{W} \in \mathbb{R}^{|V| \times d}$ as the distributional space and
they apply pre-trained convolutional neural networks (CNN) as black-box image
feature extractors. 

These architectures learn a hierarchy of blocks of image filters followed by
pre-defined pooling operations optimized for a particular task.
It has been observed in the computer vision community that the lower layers of
deep CNNs trained across various data sets and tasks tend to learn filter maps
that resemble Gabor-filters and color-blobs. Intuitively these low-level features appear to be general
and as such afford \emph{transfer}. Around the time there was extensive work on exploring the transferability
of CNN features to various computer vision tasks through fine-tuning
 \citep{donahue2014decaf,oquab2014learning} or simply taking the last layer representation of CNNs
as high-level features as inputs to linear classifiers \citep{girshick2014rich,sharif2014cnn}.
Given such successes in vision it is natural to apply CNNs in the visually grounded language learning
community as black-box image feature extractors. Similarly to \cite{bruni2014multimodal} in
\cite{kiela2014learning} the visual features of words are computed as the summary the feature vectors
extracted from all images they co-occur with $\mathbf{I} \in \mathbb{R}^{|V| \times i}$, where $i$ is
the size of the CNN image-embedding.
Applying the simple concatenation operation to the two spaces $[\mathbf{W};\mathbf{I}]$
they show that CNN features outperform BoVW features on word-similarity
experiments with the multi-modal word-representations.

All approaches described so far require both visual and textual information for the same concepts.
The multi-modal skip-gram \cite{lazaridou2015combining} model was developed to alleviate such limiation:
it is a multi-task extension of the skip-gram algorithm predicting the both the context of words, but also
the visual representations of concrete nouns. Visual representations are constructed to averaging
the CNN representations \cite{krizhevsky2012imagenet} of 100 pictures sampled from ImageNet \cite{deng2009imagenet}.
This architecture was later proposed as a model of child language learning in and
was applied to the CHILDES corpus \cite{macwhinney2014childes} with modifications to model referential
ncertainty and social cues \cite{lazaridou2016multimodal} and compared to human learning performance \cite{lazaridou2017multimodal}.



Chapter~\ref{ch:TAL} presents a computational cognitive model of word learning using only
visual information developed at the same time as the multi-modal skip-gram approach.
Similarly to \cite{feng2010visual} we assume pairs of text and images
$<D, I>$. However, in our data set images represent every day scenes and are paired with descriptive
sentences to mimic the language environment of children on a high level.
Our model implements the extreme approach of cross-situational language
learning and assumes that the representations of words are learned exclusively through the co-occurrences
between visual features and words. Rather than building a
matrix of image-features per word and then summarizing as in \cite{kiela2014learning},
we run an online expectation-maximization based \citep{dempster1977maximum} 
algorithm to align words with image features mimicking child language learning.
In essence, our approach combines the cross-situational incremental word-learning model
of \cite{fazly.etal.10csj} with the modern convolutional image representations.
The result of the learning process is a word embedding matrix $\mathbf{W_i}$,
where each row $\mathbf{W_i}$ corresponds to a word, each column $\mathbf{W_{,j}}$
to a CNN feature and each entry $\mathbf{W_{i,j}}$ to the strength
of the relationship between the word $i$ and image-feature $j$.
We show through word-similarity experiments that, while our approach performs on
par with the SGNS trained on the same data set, there is a qualitative difference between
the learned embeddings: our visual word-embeddings represent concrete nouns closer to human
judgements then abstract nouns. As each word embedding $\mathbf{W_i}$ is represented in the
image-feature space we conduct show that our model can label images with related nouns through
simple nearest neighbor search.

\section{From words to sentences}
\label{sec:sentences}
Applying the distributional intuition to model the meaning of sentences is not as straightforward
as it is for words.
Imagine building a sentence-embedding matrix $W$ where each row $W_i$ corresponds to
a possible sentence $i$ and each column $W_{,j}$ to a feature $j$.
How many rows should we include? Intuitively the number of words in a corpus is much
lower than the number sentences: one can assume a large, but finite set
of existing words and an infinite set of
potential sentences to \emph{compose} from them.
Words can be thought of as \emph{atomic units} and
in downstream applications one can use a lookup operation on a word embedding matrix
to represent units in the input. However, it is infeasible to lookup full sentences.
In fact, from a method that represents sentences in a continuous space one would expect
to also represent and generalize to unseen sentences at test time.
Furthermore, given two sentences $\mathit{John \; loves \; Mary}$ and
$\mathit{Mary \; loves \; John}$ we wish our sentence encoding function
$\phi$ to represent the meaningful difference stemming from the underlying syntactic structure
$\phi(\mathit{John \; loves \; Mary}) \neq \phi(\mathit{Mary \; loves \; John})$.
As such we seek to learn a sentence encoder $\phi$ that is sensitive to
syntactic structure or in other words \emph{compositionality} i.e.:
the notion that the meaning of an expression is
a function of its parts and the rules combining them \citep{montague1970english}.

The compositional distributional semantics  framework  produce
continuous representations for phrases up to sentences using additive and multiplicative interactions
of count-based distributed word-representations \citep{mitchell2008vector} or combine symbolic and
continuous representations with tensor-products \citep{clark2007combining}.
The latter line of work culminated in a number of unified theories of
distributional semantics and formal type logical and categorical grammars
\citep{coecke2010mathematical,clarke2012context,baroni2014frege}.
These approaches assume that words are represented by distributional word embeddings
and define compositional operations on top of them motivated by particular formal
semantic considerations. From the point of view of theoretical linguistics arguably 
one of the most intriguing aspects of such theories of meaning is that 
it provides an elegant data driven solution to deal with the representation of the lexical entries of
content words -- nouns, verbs and adjectives -- which is otherwise a major challenge in formal semantics
requiring sophistitcated formalisms such as typed $\lambda$-calculus with intricate and vast
type-systems \citep{asher2011lexical}. Within applied NLP, however, this line of work have not resulted in
practical machine learning approaches to solve natural language tasks
on real world data sets. Probably ths is due to the different scope and
the computationally expensive high-order
tensor operations involved \citep{bowman2016modeling}.

Lastly, before going forward with the more recent neural models in the next section,
it is only fair to mention that bag-of-words
based representations bypassing the issue of compositionality form a set of very strong baselines
for a number of sentence level tasks \citep{hill2016learning}.
These simple baselines include using multinomial-naive bayes uni- and bi-gram 
log-count features within support vector machines \citep{wang2012baselines} or
feeding the average of the word-embeddings in a sentence into a softmax classifier \citep{joulin2016bag}.


\section{Neural sentence representations}
\label{sec:trans-sentence}

In Section \ref{sec:words} we discussed the feed-forward \citep{bengio2003neural}
and recurrent network based models \citep{mikolov2010recurrent} from the perspective
of learning word-representations. However, both architectures learn embeddings
of sequences and  as such they do not only learn to represent
words, but also phrases and full sentences. An intriguing property of such approaches is that
they represent various linguistic objects in the same space as activations of the neural models.\footnote{Note that this property is not unique to neural models. For example in the Pregroup algebra based compositional distributional
semantics framework of \cite{coecke2010mathematical} 
sentences of any grammatical structure live in the same inner-product space.} 
When learning transferable
sentence representations there are two main considerations we will discuss:
1.) which architecture to choose and 2.) what objective to optimize.

Various neural network architectures have been proposed that
handle variable sized data structures useful for language processing: a.) recurrent networks 
take the input sequentially one word or character at a time
b.) convolutional neural networks
\citep{kalchbrenner2014convolutional,zhang2015character,conneau2016very,chen2013learning}
process sequences in fixed sized n-gram patterns up to a large window,
c.) recursive neural networks \citep{goller1996learning,socher2011parsing,tai2015improved}
take a tree data structure as input such as a sentence according to the traversal of a
constituency
d.) graph neural networks operate on graphs \citep{marcheggiani2017encoding} such as
syntactic/semantic-dependency or abstract meaning representations.
All the aforementioned architectures take word-embeddings as input 
and compute fixed vectors for sentences. 
These representations are tuned to a specific task such as sequence tagging,
sentence classification, machine translation or language modeling.


In Chapters \ref{ch:COLI}, \ref{ch:IJCNLP}, \ref{ch:ConLL} and \ref{EMNLP} we decided to apply
recurrent neural networks as sentence encoders.
Recursive neural networks provide a principled approach to
compute representations along the nodes of constituency
or dependency parse trees \citep{socher2013recursive,socher2014grounded,le2015compositional,tai2015improved}.
In practice, however, these architectures require parse trees as input,
which makes them impractical for our mission of learning visually grounded
representations for multiple languages.
For each language considered, the training procedure requires finding a good
pipeline from text pre-processing to parsing to generate the input representations
for the networks. Furthermore, tree structures by nature do not
afford straightforward batched computation directly and tend to run dramatically
slower than recurrent or convolutional models. In terms of performance the jury
is still out, however, so far only modest improvements have been observed
over recurrent models on specific tasks in specific settings
\citep{li2015tree,tai2015improved}. For graph structures neural networks the
same argument holds. Two equally practical alternatives to RNNs
that operate on raw sequences are convolutional neural networks
\citep{bai2018empirical} and transformers \citep{vaswani2017attention} and both could
replace the RNNs in Chapters~\ref{ch:IJCNLP}, \ref{ch:ConLL} and \ref{ch:EMNLP}.



As mentioned earlier contrary to word-embeddings the representations
and the composition function these models learn are task specific and not \emph{universal}.
For learning transferable and general distributed sentence representations the
first notable approach is the
skip-thought vectors model \citep{kiros2015skip}.
It extends distributional semantics intuition of the SGNS approach to the sentence level:
they train a sentence encoder on contiguous sentence sequences
-- such as books -- which learns representations predictive of the sentences around it.
More specifically
they train a recurrent encoder to encode sentence $s_i$ and use two separate recurrent decoders to
generate the pre-context $s_{i-1}$ and post-context $s_{i+1}$.
This method was confirmed to be a successful self-supervised method for transfer
learning in a number of sentence
classifications tasks beating simpler approaches such as bag-of-words approaches based on skip-gram
or CBOW embeddings, tf-idf vectors and auto-encoders \citep{hill2016learning}. A convolutional
variant of the encoder was introduced in \cite{gan2016unsupervised} and several other works
train simple sum/average pre-trained word-embedding encoders using the same context prediction-style
loss \citep{kenter2016siamese,hill2016learning}. The larger context of paragraphs is
explored in \cite{jernite2017discourse} where the task is to predict discourse coherence
labels in a self-supervised fashion.

Some later approaches have  moved away from distributional cues
and identifyied supervised tasks that lead to representations that transfer
well to a wide variety of other tasks. The task of Natural Language Inference \citep{bowman2015large,williams2017broad} as an objective
was identified to learn good sentence embeddings \citep{conneau-EtAl:2017:EMNLP2017,kiros2018inferlite}
and \cite{subramanian2018learning} combines a number of other supervised tasks
with self-supervised training through multi-task learning.


The state-of-the-art in learning universal sentence representations
at the time of writing the present thesis is
represented by neural language models with an extreme number of parameters
trained over huge corpora \citep{peters2018deep,devlin2018bert}.
These approaches go back to
exploiting only distributional cues and train a stack convolutional and/or recurrent layers
and/or transfomer layers on large-scale bidirectional language modeling.
When trasferring the networks to novel tasks these networks are either fine-tuned, a
separate smaller network is trained on top of their representations or they are used and
fixed feature extractors \citep{howard2018universal,peters2019tune}.

\section{Visually grounded sentence representations}
\label{sec:visualsentences}

As discussed in Section~\ref{sec:trans-sentence}
universal sentence representations are in general learned from text-only corpora. The most 
successful current trend being large-scale language modeling based on
the distributional semantics intuition of the general usefulness of linguistic context prediction.
However, this leaves the resulting sentence representations blind to the language-external 
reality leading to the grounding problem
as discussed in Section~\ref{sec:visualwords}. Given that visual information
has been shown to contain useful information for word-representations it
is a natural question to ask whether this observation generalizes to sentence
embeddings. The idea of context prediction coming from the distributional 
hypothesis can be adopted to visual grounding in a conceptualy straightforward manner:
train sentence embeddings to be predictive of their visual context. 

The structure of the data sets required for such a setup has the same form
as the visual-textual multi-modal LDA approach of \cite{feng2010visual} or our
cross-situational word learning model \citep{kadar2015learning}:
pairs of sentences $s$ and images $i$ $<s, i>$.
In particular the standardized benchmark data sets used for this purpose 
in all chapters were originally introduced for automatic image-captioning.
These data sets annotate images found in online resources with descriptions
through crowd-sourcing.
These descriptions are largely \emph{conceptual}, \emph{concrete}
and \emph{generic}.
This means that descriptions do not focus too much on \emph{perceptual}
information such as colors, contrast or picture quality; they do not mention
many \emph{abstract} notions about images such as mood and finally the descriptions
are not \emph{specific} meaning that they do not mention the names of cities,
people or brands of objects.
What they do end up mentioning are entities depicted in the images
(frisbee, dog, boy) their attributes (yellow, fluffy, young) and the
relations between them.
The images depict common real-life scenes such as a \emph{bus turning left} or \emph{people
playing soccer in the park}. As such, annotation collected independently from
different crowd-source workers end up focusing on different aspects of 
these scenes\footnote{For a comprehensive overview on image-description data sets
please consult \cite{bernardi2016automatic}}.
.

To learn to predict images from these captions -- and conversely caption rom the images --
the architecture we chose in
Chapters \ref{ch:COLI}, \ref{ch:IJCNLP}, \ref{ch:ConLL} and \ref{EMNLP}  combines 
a recurrent neural network $\phi(s, \theta)$ to
represent sentences $s$ and a pre-trained convolutional neural networks 
followed by an affine transformation we train for the task  $\psi(i, \theta)$ 
to extract features from images $i$ . 
The image-context prediction from sentences in 
Chapter \ref{ch:COLI} is formulated  as minimizing the cosine distance
between the learned sentence $\mathbf{s} = \phi(s, \theta)$ and image representations 
$\mathbf{i} = \psi(i, \theta)$ in a training set of image--caption pairs $<i,s> \in D$ :

\begin{equation}
J_{cos}(\theta) = \sum_{<i,s> \in D} 1 - \frac{\mathbf{i} \cdot \mathbf{s}}{\| \mathbf{i}\| \| \mathbf{s} \|} \\
\end{equation}

In later Chapters~\ref{ch:IJCNLP} we turn to the sum-of-hinges, while in \ref{ch:ConLL} 
and \ref{ch:EMNLP} to the max-of-hinges ranking objectives. These loss functions 
push relevant pairs $<i, s>$ close, while contrastive pairs  $<i, \hat{s}>$ and $<\hat{i}, s>$
far from each other in a joint embeddings space. Let us overload the notation $<i, s>$ 
let it also refer to a batch of pairs:

\begin{equation}
J_{rank}(\theta) = \sum_{<i,s> \in D} f_{rank}(\mathbf{i}, \mathbf{s}) + f_{rank}(\mathbf{s}, \mathbf{i}) 
\end{equation}

where the $f_{rank}$ is either the sum-of-hinges loss $J_{sum}$

\begin{equation}
\label{eq:sumviol}
\begin{split}
J_{sum}(\theta) = \sum_{<\hat{i}, s>}[\max(0, \alpha - s(\mathbf{i},\mathbf{s}) + s(\hat{\mathbf{i}}, \mathbf{s}))] \;+ \\ \sum_{<i, \hat{s}>}[\max(0, \alpha - s(\mathbf{i},\mathbf{s}) + s(\mathbf{i}, \hat{\mathbf{s}}))]
\end{split}
\end{equation}

or the max-of-hinges objective the sum-of-hinges loss $J_{max}$:

\begin{equation}
\label{eq:maxviol}
\begin{split}
J_{sum}(\theta) = \max_{<\hat{i}, s>}[\max(0, \alpha - s(\mathbf{i},\mathbf{s}) + s(\hat{\mathbf{i}}, \mathbf{s}))] \;+ \\ \max_{<i, \hat{s}>}[\max(0, \alpha - s(\mathbf{i},\mathbf{s}) + s(\mathbf{i}, \hat{\mathbf{s}}))]
\end{split}
\end{equation}

At each batch the positive pairs come from the ground truth data whereas contrastive pairs are taken
as all possible not true pairs in a batch. The $\alpha$ is the margin parameter for both loss functions.
We empirically found both ranking losses to perform better than $J_{cos}$ and $J_{max}$ to perform
consistently better in our experiments than $J_{sum}$. The use of these various objetctives across
chapters is somewhat arbitrary and reflect the evolving common practices in the field at the time. 

Similarly in Chapter~\ref{ch:COLI} as the pre-trained CNN image-feature extractor we apply the VGG-16 
architecture \citep{conneau2016very},  in Chapter~\ref{ch:IJCNLP} the Inception-V3 \citep{Szegedy2015},
while in ~\ref{ch:ConLL} and ~\ref{ch:EMNLP} the ResNet-50 \citep{he2016deep}. These choices are
again somewhat arbitrary. Chapter~\ref{ch:COLI} is based our Imaginet model \citep{chrupala2015learning}
and VGG features were the standard choice at that time.  For Chapter~\ref{ch:IJCNLP} we empirically 
determined Inception-V3 to give the best performance, while for Chapters~\ref{ch:ConLL} and \ref{ch:EMNLP}
we used ResNet-50 as the features extracted from this architecture were given as part of  
a Shared Task based on the data set we used.

Other than image--sentence context prediction we also apply the $J_{max}$ 
objective for sentence--sentence ranking in Chapters~\ref{ch:ConLL} and \ref{ch:EMNLP}.
The data for this objective are sentences that belong to the same image.

%Note the similarity between the ranking losses and the negative sampling
%in the skip-gram model \citep{mikolov2013distributed}.

The representations learned by such image--sentence and sentence--sentence ranking models have 
been shown \citep{kiela2017learning}
to improve performance when combined with skip-thought embeddings
on a large number of semantic sentence classifications tasks compared to
skip-thought only. These findings were confirmed and improved upon using a
self-attention mechanism on top of the RNN encoder \citep{yoo2017improving}.
Furthermore, in Chapter~\ref{ch:IJCNLP} we show that in the visually descriptive domain 
grounded learning improves translation quality and that learning multi-modal
representations provides gains on top of learning from larger bilingual corpora.
In later chapters --  Chapters~\ref{ch:ConLL} and \ref{ch:EMNLP} -- we demonstrate
that better visually grounded representations can be learned when training on multiple
languages jointly.




%For our multi-lingual experiments we use the Multi30K data set \cite{elliott2016multi30k,elliott2017findings}; a multilingual extension of Flickr30K. It consists of a {\it translation} and a {\it comparable} portions both containing all images from the original Flickr30K. The \emph{translation} portion annotates a single English description with a German, French and Czech caption, while the \emph{comparable} the original 5 English and additional 5 German descriptions collected independently using the same crowd-sourcing process.

%\paragraph{Our contribution}


%In \citep{chrupala2015learning} we train a multi-task architecture consisting
%of a shared word-embedding matrix and two pathways: 1.) common recurrent language
%model trained to maximize the probability of a word given the history,
%2.) grounded representation learning model predicting the image representation paired with the sentences.
%This model combines the distributional hypothesis for learning the meaning of words
%and the grounded learning on the sentence level.
%We show that the word-representations learned by the two path architecture
%predict human word-similarity judgements with higher accuracy then the one-path
%architectures. Furthermore, the combined model is more sensitive to word-order than
%the visual-only model.  Lastly we show that images act as an
%anchor and the architecture performs paraphrase retrieval well above chance
%level.

%Chapter~\ref{ch:COLI} is dedicated to develop techniques to interpret the
%opaque continuous representations of this model. It provides an in-depth
%comparative analysis of the linguistic knowledge represented by the
%language-model and visual-pathways of this architecture.




%\section{Multilingual representations of words and sentences}\todo{this will be challenging, not sure yet how to build it up.}
%Multilingual or cross-lingual word embedding approaches map words of different languages in a shared space. These representations allow to transfer knowledge between languages and
%perform cross-lingual tasks such as cross-lingual document retrieval. The bulk of such approaches apply the techniques developed for monolingual word-embedding induction as a component
%of their full pipeline. One of the early approaches trains two separate word spaces with using the negative-sampling implementation of the skip-gram algorithm and then maps these spaces onto eachother by minimizing the squared loss between the 5000 top most frequent word-vectors \cite{mikolov2013exploiting}. This main approach went through improvents
%such as by switching mean-squared error to ranking loss \cite{lazaridou2015hubness} or enforcing orthogonality constraint on the mapping matrix
%\cite{xing2015normalized}   (WE DID THE SAME OVER TIME). Another prominent technique to learn a mapping between word (and other types of) spaces is Canonical Correlation Analysis, first
%used by \cite{faruqui2014improving}, which was later improved by the application of Deep CCA \cite{lu2015deep}. Another approach is to create a pseudo-bilingual corpora by replacing words
%in a source language by their translation and train an embedding model on the resulting corpus
%The uber multilingual word-embedding \cite{ammar2016massively}.

\section{Visually grounded multilingual representations}
\label{sec:vismulti}

On top of the visual modality anchoring linguistic representations to perceptual reality it
also provides a universal meaning representation bridging between languages.
The  intuition being that words or sentences with similar meanings appear
within similar perceptual contexts independently of the language. We first discuss
images used as pivots for translation on word-level in Section~\ref{sec:bilinglex} and on 
sentence-level in Section~\ref{sec:imgpivot}.  
In automatic machine translation a pivot based approach is applied
when there are parallel corpora available between language pairs 
and $C \rightarrow B$, but there is no data for $A\rightarrow B$. The problem is
solved by first translating $A$ to $C$ and then $C$ to $B$. 
Image--pivoting the refers to a setup where we assume the exitence of image--caption 
data sets $A \leftrightarrow I_A$ and 
$B \leftrightarrow I_B$ and translate the images are used as pivots for translation 
going $A \rightarrow I$ then $I \rightarrow C$. In Section~\ref{sec:multiview} we discuss
the multi-view learning perspective of considering images annotated with multiple descriptions in
different langauges as multiple views on the underlying semantic concepts.
Our aim in Chapters~\ref{ch:ConLL} and \ref{ch:EMNLP} is to learn 
better visually grounded sentence representations by learning from these multiple-views 
simultanuously. 


\subsection{Bilingual lexicon induction}
\label{sec:bilinglex}

On word level images as a common ground have been applied to induce bilingual
lexicons without parallel corpora. The lack of bi-text in this setting have
been traditionally solved by methods relying on textual features
such as orographic similarity \citep{haghighi2008learning} or similar diachronic distributional
statistical trends between words \citep{schafer2002inducing}.
However, images tagged with various labels in a multitude of languages are
available on the internet and a model of image-similarity can be used to exploit
images as pivots to find translations between such tags. Alternatively visual representations
for words are computed through summarizing the image-features co-occurring with them
-- as discussed in Section~\ref{sec:visualwords}. This leads to representing
words in multiple languages within the same image-feature space.

The first approach to construct a dictionary based on image similarity was
developed by \cite{bergsma2011learning}. They use Google image search to find
relevant images for the names of physical objects in multiple languages.
These images are represented by BoVW vectors based
on SIFT and color features. To find word-translations given a word in the
source language and a list of possible translations in the target language
for each word Google is queried to find $n$ images per word. For each image the
feature vectors are extracted. This is followed by computing the similarity
between the feature vectors of the images corresponding to the source word and
all target words. Finally the word in the target vocabulary with the highest
similarity is chosen.

This method is vastly improved upon by a later approach applying
pre-trained convolutional neural networks to represent words
in the visual space \citep{kiela2015visual}. Their approach closely follows
the monolingual visual-word representations of \cite{kiela2014improving}:
each word is represented as the summary of CNN representations of images they
co-occur with. Given a word in the source word the candidate translation is found
simply by performing nearest neighbor search on the target vocabulary.

Exploring the limitations of image-pivoting for bilingual lexicon induction
\cite{hartmann2017limitations} present a negative result
showing that such techniques scale poorly to non-noun words such as
adjectives or verbs. However, combining image-pivot based bilingual
word-representations with more traditional multi-lingual word-embedding
techniques lead to superior performance compared to their uni-modal
counterparts \cite{vulic2016multi}. 

At the time of writing this thesis
the first large-scale vision based bilingual dictionary induction data set
was put forward by \citep{hewitt2018learning}. They create a data set
of \~200K words and 100 images per each using Google Image Search and perform
experiments with 32 languages.
They confirm the finding of \cite{hartmann2017limitations} that image-pivoting
is most effective for nouns, however, find that using their larger data set
adjectives can also be translated reliably. Similarly to \cite{vulic2016multi}
they also show through combining
visual and textual word-representations through a re-ranking
experiment that visual representations improve the state-of-the-art
text-only methods in the case of \emph{concrete} concepts.

\subsection{Image pivots for translation}
\label{sec:imgpivot}

Images have also been used as a pivot to aid translation of visually
descriptive sentences.
%A common technique in statistical machine translation
%is to apply a heuristic re-ranking algorithm
%to a sampled list of candidate translations from a translation model given the source sentence.
%\cite{hitschler2016multimodal} consider the setting of image-caption translation where an
%image-caption pair is given on the source side and on top of the parallel source-target sentence corpus
%additional target-side image-caption corpus is available. The show that using the images in the re-ranking
%pipeline improves translation performance upon using solely textual-similarity.
\cite{nakayama2017zero} apply image-pivoting in such a zero-resource machine
translation setting. The task is to translate from English to German without
aligned parallel corpora, however, separate image--description data sets are
available in both languages. They solve the problem by
training two components: 1.) visual-sentence encoder that matches images and their descriptions,
2.) image-description generator maximizing the likelihood of gold standard captions given the images.
At test time the visual-sentence encoder representation of the source sentence is fed to the
image--description generater to produce the translation.
Their results were improved later by modeling
the image-pivot based zero-resource translation setup as a
multi-agent communication game between encoder and decoder
\citep{chen2018zero,lee2017emergent}.


\subsection{Multi-view perspective}
\label{sec:multiview}


Images and their descriptions in multiple languages can be taken as different views of the
same underlying semantic concepts. From this multi-view perspective learning common representations
of multiple languages and perceptual stimuli can potentially exploit the complementary information
between views to learn better representations. Being able to extract
a shared representation from only a single view leads to practical applications such as
cross-modal and cross-lingual retrieval or similarity calculation.

Multi-view learning traditionally considers paired samples $\mathbf{X}$ and $\mathbf{Y}$,
where each pair of rows $<\mathbf{X_i}, \mathbf{Y_i}>$ are two measurements of the same
underlying phenomena. The two main approaches put forward in recent literature are based on 
autoencoders, canonical correlation analyisis \citep{wang2015deep}.

To illustrate the autoencoder approach let us consider \cite{ngiam2011multimodal} who introduce the idea
of multi-modal autoencoders to learn multi-modal joint representations of audio and video. Let
$\phi$ be an encoder neural network extracting features from both modalities and $\psi$ and $\chi$ be 
modality specific decoder. Their model performs the following optimization:

\begin{equation}
\min_ {\theta_{\phi}, \theta_{\psi}, \theta_{\chi}} \frac{1}{N} \sum^N_i (||X_i - \psi(\phi(X_i))||^2 +||Y_i - \chi(\phi(Y_i))||^2)
\end{equation}

where $\theta_{\phi}, \theta_{\psi}, \theta_{\chi}$ are the encoder and decoder paramters and $N$
is the size of the training set.
This approach learns shared representations such the one view can be reconstructed from another.

In the deep canonical correlation analysis (DCCA) based approaches \cite{andrew2013deep} a modality or view
specific networks $\psi$ and $\phi$ are applied to extract non-linear features 
and the canonical correlation between (CCA) these representations is maximized:

\begin{align}
\max_ {\theta_{\phi}, \theta_{\psi}, \mathbf{U}, \mathbf{V}} \; \; &  \frac{1}{N} \text{tr}(\mathbf{U}^T \phi(\mathbf{X}) \psi(\mathbf{Y})  \mathbf{V}^T) \\
\text{subject to} \; \; & \mathbf{U}^T \left(\frac{1}{N} \phi(\mathbf{X}) \phi(\mathbf{X})^T + r_x \mathbf{I} \right) = \mathbf{I} \\
& \mathbf{V}^T \left(\frac{1}{N} \psi(\mathbf{Y}) \psi(\mathbf{Y})^T + r_x \mathbf{I} \right) = \mathbf{I} \\
& \mathbf{u}_i^T  \phi(\mathbf{X}) \psi(\mathbf{Y})  \mathbf{v}_j = 0, \; \; \forall_{i,j} \;  i \neq j
\end{align}

Where $\theta_{\phi}, \theta_{\psi}$ are neural network parameters and $\mathbf{U}, \mathbf{V}$
are projection matrices or \emph{directions} of the CCA. This optimization process amounts to maximizing
the correlation between the projections of the  two data views subject to the contraint that 
the projected dimensions are uncorrelated.

A third direction that is also explored is combining the reconstruction objective of autoencoders 
with an additional correlation loss, but without the whitening constraints of CCA 
 \cite{ap2014autoencoder,rajendran2015bridge,chandar2016correlational}.


The deep partial canonical correlation
analysis approach learns multilingual English-German sentence embeddings conditioned on the
representation of the images they are aligned to \citep{rotman2018bridging}.
They show that their model using the visual modality as an extra view finds
better multilingual sentence and word representations as demonstrated by
cross-lingual paraphrasing and word-similarity results.
The bridge correlational neural network approach \citep{rajendran2015bridge}
learns common representations even when the different views only need to be
aligned with one pivot view. In the vision and language domain they preform
image-sentence retrieval experiments in French or German where
the image-caption data set is only available for English, however,
there is parallel corpora between German or French and English. In other words English acts as a pivot.
\cite{gella2017image} considers images as a pivot bridging English and German and train a multilingual
image-sentence ranking model -- recurrent-convolutional architecture as in Chapters~\ref{ch:COLI}, \ref{ch:IJCNLP} and \ref{ch:ConLL}.
There results suggest that the multilingual models outperform the monolingual ones on image-sentence
ranking, however, they do not show consistent gains across languages and model settings.

\paragraph{Our contribution}

In Chapter \ref{ch:ConLL} we implement a similar setup to \cite{gella2017image}
and do show that both English and German image-sentence
ranking performance is reliably improved by bilingual joint training.
We expand the results further and provide evidence that more gains can be
achieved by adding more views: on top of English and German, we
add French and Czech captions. Revisiting the issue of alignment we find that
the unlike in previous experiments \citep{gella2017image,calixto2017multilingual,rotman2018bridging}
image-pivoting leads to better visual-sentence embeddings even when the images
are not matched between languages. Finally the performance on the lower resource
French and Czech languages is improved when we add the larger
English and German image-caption sets, showing successful multilingual
transfer in the vision-language domain.

Chapter~\ref{ch:IJCNLP}, where we show that grounded learning improves the translation
of image descriptions is also related to the multi-view learning topic: we use
images as an additional view that helps us learn better sentence representations.


%\section{Grounding and other modalities}
%Grounding words in auditory signals \cite{kiela2015multi,lopopolo2015sound} and
% olfactory perception \cite{kiela2015grounding}r. Growing body of literature in learning joint representations
% of speech and images \cite{harwath2016unsupervised,chrupala2017representations,harwath2018jointly}


%Mention semantic parsing and the WebNLG paper i did with Thiago. Mention the TextWorld thing. Grzegorz
%and Afra´s work speech, plus the fail paper that im on too.

\section{Interpreting continuous representations}
\label{sec:interpret}
The word, phrase and sentence representations learned through neural architectures are notoriously opaque.
Contrary to count-based methods the features extracted by deep networks from text input
appear as arbitrary dense vectors to the human eye.
Empirical evidence shows that incorporating various views such as multiple languages and vision to
learn representations improves performance on downstream tasks.
But where does the improvement come from? What are the linguistic
regularities represented in the recurrent states that lead to the final performance metrics?
Did the model learn to exploit trivial artifacts in the training data or did it learn to generalize?
What characterizes the individual features recurrent networks extract from the input strings?
The main topic of my thesis is learning visually grounded representations for linguistic units.
For a complete picture it is crucial to assess the difference in the representations
between textual only and multimodal representations not only from a quantitative point of view,
but also from a qualitative linguistics angle.
This is what Chapter \ref{ch:COLI} is dedicated to.

Developing techniques for interpreting machine learning models have multiple goals.
From the practical point of view as learning algorithms make their way into critical applications
such as medicine humans and machines need to be able to co-operate to avoid catastrophic
outcomes \citep{caruana2015intelligible}. As such there is a growing interest in deriving methods
to \emph{explain} the decision of such architectures.
These methods assign a real-valued "relevance" score to each unit in the input signal,
which signifies how much impact it had on the final prediction of the model.
One of the first paradigms in generating such relevance scores is gradient based methods:
they take the gradient of the output of the network with respect to the input \cite{simonyan2013deep}.
Deep neural models of language tasks learn distributed representations of input symbols
and as such further operations have to applied to reduce the resulting gradient vectors to
single scalars e.g.: using $\ell_2$ norm \citep{bansal2016ask}.

Another prominent and well studied
approach still based on gradient information is layerwise
relevance propagation (LRP) \citep{bach2015pixel}. The output of the final layer
is written as the sum of the relevance-scores from the input and similarly to the back-propagation
algorithm the relevance of each neuron
recursively depends on the lower-layer all the way down to the input signal.
Different versions of LRP run the backward pass with different rules taking as
input gradient information and activation values. It was later theoretically analyzed and generalized
into the deep Taylor decomposition method \citep{binder2016layer}.
This has been shown to be equivalent to taking the
gradients with respect to the input as in \cite{simonyan2013deep} and multiplying it elementwise
with the input itself \citep{shrikumar2017learning}.
LRP was also later derived for recurrent architectures \citep{arras2017explaining} to describe
sentiment classifiers. Another backpropagation based algorithm is DeepLIFT, which instead of
using the gradients for computing the relevance scores it uses the difference between the activations
that result from a specific value compared to a baseline input.

Perturbation based methods are also gradient free: these involve generating
pseudo-samples according to some procedure and comparing measuring how the models
react differently to these samples versus the real one.
LIME \citep{ribeiro2016should} and its NLP specific extension LIMSSE
\citep{poerner2018evaluating} perturbs
the input creating a local neighborhood around it and fits interpretable linear models to explain
the predictions of any complex black box classifier.
Even simpler perturbation based techniques apply perturbations to the input and measure the
difference between the original input and the various perturbed candidates such as the erasure
\citep{li2016understanding} and our omission \citep{Kadar2016} method.

Apart from practical considerations training a complex opaque models
from close to raw input then can shed help discover patterns in the
input data that are crucial in solving the task.
Deep neural networks learn to solve tasks from close to raw input, similar to
what humans receive. As such
uncovering the regularities they learn can also shed light into the patterns
humans might extract from data to cope with certain tasks.
Recent methodology in probing the learned representations of LSTM language models,
in fact, resemble psycholinguistic studies.
A number of experiments using the agreement prediction paradigm
\citep{bock1991broken} suggest that LSTM language models successfully learn
syntactic regularities as opposed to memorizing surface patterns
\citep{linzen2016assessing,enguehard2017exploring,bernardy2017using,gulordava2018colorless}.

\paragraph{Our contribution}
The \emph{omission} method we develop in Chapter~\ref{ch:COLI} removes words
from the input while the \emph{occlusion} \citep{li2016understanding}
method replaces these inputs with a baseline.
However, our aim in Chapter~\ref{ch:COLI} was not to develop a method for explanation, but to
shed light on the linguistic characteristics of the input grounded learning
models learn in contrast to text-only language models.

\section{Published work}

\subsection{Chapters}

Each of the following Chapters was presented before as a published long-paper. These are included with
the only modification of re-aligning and re-sizing a few figures.

\begin{description}
	\item[Chapter~\ref{ch:TAL}] \bibentry{kadar2015learning}
	\item[Chapter~\ref{ch:COLI}] \bibentry{kadar2017representation}
	\item[Chapter~\ref{ch:IJCNLP}] \bibentry{elliott2017imagination}
	\item[Chapter~\ref{ch:ConLL}] \bibentry{kadar2018lessons}
	\item[Chapter ] 
\end{description}

\subsection{Other publications completed during my PhD}

Worked on some of the experiments for the visually grounded sentence representation learning
model \texttt{IMAGINET}, which formed the basis in the investigations of Chapter~\ref{ch:COLI}:

\begin{itemize}
\item \bibentry{chrupala2015learning}
\end{itemize}

Other than methods I also contributed to two corpora within the vision and language domain.
During my internship at Microsoft Research Montreal my side-project was a synthetically generated visual-reasoning
dataset named FigureQA:

\begin{itemize}
\item \bibentry{kahou2017figureqa}
\end{itemize}

The second corpus I worked on was with our sister group in the communications department and a collaborations with 
Vrije Universiteit Amsterdam:  a data set of native Dutch spoken image-description with eye-tracking recordings.
The goal of this project was to provide a large data set for the community interested in referring-expression research
with rich annotations already existing in the Visual Genome data set:

\begin{itemize}
\item \bibentry{van2018didec}
\end{itemize}

Outside of the vision and language domain I worked in a couple of other fields.
Within our research group, we published on learning unsupervised sentence represenations
from speech signals:

\item \bibentry{chrupala2018difficulty}

Furthermore, we published a paper about attemtping reproduce a 
recurrent architecture for language modeling:

\begin{itemize}
\item \bibentry{kadar2018revisiting}
\end{itemize}

I had the opportunity to work on implementing recurrent networks for
generation in the context of referring-expression generation:

\begin{itemize}
\item \bibentry{ferreira2018neuralreg}
\end{itemize}

Finally, the largest project other than my main PhD project I worked
on was during my main project during my Microsoft Research Montreal internship.
It is linear-logic programming based system for 
generating text-based adventure games to test
generalization in (deep) reinforcement learning:

\begin{itemize}
\item \bibentry{cote2018textworld}
\end{itemize}

%\section{Vision and Language applications}
%Mention the bunch of work thats here. Do image-captioning and image-sentence ranking and then metion that we do the latter always. Say the importance of attention in this field and mention the DIDEC corpus. Mention VQA and fine-grained reasoning and the FigureQA paper.



%\section{Methods}

%\section{Data sets}
%For image-feature extraction all approaches presented in the thesis use some
%CNN architecture pre-trained on a version of the ImageNet data set \cite{deng2009imagenet} prepared for a large-scale image classification challenge \cite{russakovsky2015imagenet}. ImageNet annotates the noun synsets from WordNet \cite{miller1995wordnet} with images collected from the internet. At the time of writing this thesis on average the data set consists of 500 images per node. The section used in the pre-trained networks is the ILSVRC2012 containing 1.2 million images annotated with 1000 classes. For learning grounded language representation we use data sets introduced for image-captioning. These data sets annotate images found in online resources with descriptions through crowd-sourcing. The smallest data set we use is Flickr8K \cite{hodosh2013framing} with 8000 images, which was later extended to contain 30K images resulting in the Flickr30K benchmark \cite{young2014image} and finally the largest collection we use is the COCO image-caption benchmark \cite{chen2015microsoft} with 123K images. All data sets contain 5 captions per image.
%The descriptions collected for these data sets are largely \emph{conceptual}, \emph{concrete} and \emph{generic}. This means that descriptions do not focus too much on \emph{perceptual} information such as colors, contrast or picture quality; they do not mention many \emph{abstract} notions about images such as mood and finally the descriptions are not \emph{specific} meaning that they do not mention the names of cities, people or brands of objects. What they do end up mentioning are entities depicted in the images (frizbee, dog, boy) their attributes (yellow, fluffy, young) and their relations between each other. The images depict common real-life scenes such as bus turning left or people playing soccer in the park. As such annotation collected independently from different workers end up focusing on different aspects of these scenes. For a comprehensive overview on image-description data sets please consult \cite{bernardi2016automatic}.
%For our multi-lingual experiments we use the Multi30K data set \cite{elliott2016multi30k,elliott2017findings}; a multilingual extension of Flickr30K. It consists of a {\it translation} and a {\it comparable} portions both containing all images from the original Flickr30K. The \emph{translation} portion annotates a single English description with a German, French and Czech caption, while the \emph{comparable} the original 5 English and additional 5 German descriptions collected independently using the same crowd-sourcing process.


%\section{Architectures}
%Just do the usual GRU + CNN thing. cite Jamie (heart).

%\section{Recurrent network}
%Do the GRU mention that the LSTM is strictly more powerful according to Yoav, but it doesnt seem to matter at all according to Chung.

%\section{Convolutional network}
%Just super quick run-down on VGG and ResNet.

%\section{Optimization}
%Just mention that all this shit is optimized with some version of SGD and we use Adam all the time. Mention that Adam sucks but its just some common thing to do. Say this thing about the MSE or cosine loss that we start with that but i results in hubness actually as Angeliki points it out. So we move to the contrastive kinda losses.

%\section{Transfer learning}
%Don't really worry about it too much just say something about how awesome is that the CNNs trained on image net actually end up being so useful for us.

%\section{Multi-task learning}
%Multi-task learning has been an important paradigm since the earliest works on training neural architectures for natural-language processing \cite{collobert2008unified}. Multi-task learning involves optimizing multiple loss function jointly usually with the goal of exploiting commonalities between tasks. As described in the seminal work of \cite{caruana1997multitask}:

%\begin{quote}
%Multitask Learning is an approach to inductive transfer that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias. It does this by learning tasks in parallel while using a shared representation; what is learned for each task can help other tasks be learned better.
%\end{quote}

%Introducing inductive bias using different tasks means that the learning algorithm prefers a specific set of hypotheses over others just as in the case of other forms of regularization such as $\ell_1$ or $\ell_2$ penalties. Here I only discuss the specific instantiation of the mutli-task learning paradigm that are applicable to the chapters presented in this thesis and for a more comprehensive overview please consult \cite{ruder2017overview}. The technique used in the thesis
%is hard-parameter sharing \cite{caruana1997multitask}
