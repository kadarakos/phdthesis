%!TEX root = ../dissertation.tex

%EK: edits November 28
\chapter{Introduction}

\section{General introduction}

The ability to understand natural language plays a central role in humans' conception of intelligent machines.
Alan Turing already in the 1950s in his now famous Turing Test(s) gives natural language understanding a 
key role in tricking humans into thinking that they are interracting with their fellow specimen rather 
than rooms full of transistors. One of the goals of Natural Language Processing (NLP) is to develop algorithms
and build systems to help machines understand what humans are talking about; to 
understand the \emph{meaning} of natural language utterrances. 
In this thesis I explore computational techniques to \textbf{learn the meaning of words, phrases up to sentences}
considering the \textbf{visual world} as a naturally occuring 
\textbf{meaning representation briding between langauages}.
The methods presented in this thesis seek to find relationships between \textbf{images} and 
\textbf{natural utterrances} in various languages. The  Chapters of the thesis follow a progression 
starting with a single language at the word-level and arriving to multilingual visually grounded sentence
representations.

\subsection{Learning representations}
The main foundational methodology applied in all chapters is \emph{statistical learning}. 
The approach taken during the earlier days of NLP was characterized by rule-based systems building on such 
foundations as Chomskyan theories of grammar or Montegue Semantics. 
Since the 1980s partly due to such theories falling out of fashion, but also to the increase in the amount 
of available computational power Machine Learning (ML) approahces revolutionized
the field.  \textbf{Learning} in general proved to be a crucial component to Artifically Intelligence and also
specifically to NLP. ML algorithms are designed with the goal that given an increasing 
number of examples a system improves its performance according to some measure of success.
Reflecting the structure of ML itself and the popularity of ML within the field, NLP research follows
a task-oriented methodology: researchers collect data sets, define measures of success and develop or
apply learning algorithms. In Chapters~\ref{ch:TAL}, \ref{ch:IJCNLP} and \ref{ch:ConLL} closely follow this blueprint.

From the rules based times of "engineering grammars" lot of researchers moved onto "engineering features"
to \textbf{represent} the textual data fed to general purpose pattern-recognition algorithms such as
decision-trees, suport-vector machines or hidden Markov models. A large set of these features are still based on
various formal-linguistic theories.
Intuitively, different applications such as machine translation or goal-oriented dialogue systems 
require different input representations. Furthermore, one would assume that various languages require 
different feature-extraction pipelines reflecting the typological differences across languages. 
The direction of \emph{linguistic representation learning} challenges this intuition and is interested in finding
ways to \emph{learn} representations linguistic representaitons from \emph{data}, which are more-or-less
generally applicable. This line of work, as well as the approaches presented in the thesis,
fit in general with the \textbf{representation learning} 
framework, which consits of machine learning approaches that learn useful representations 
for various tasks from (close to) raw input.

The expression "representation learning" is more or less synonymous with "deep learning" at the time
of writing this thesis. When mentioning representation learning in the deep learning context
it is usually meant that the goal is to learn a function from raw input to target labels. In the 
context of my thesis the, however, emphasis is on learning representations of words, phrases and 
senteces that are potentially \emph{generally useful}, meaning 
that they can be used as input to in many tasks. This is sometimes referred to as 
\emph{transfer-learning} where we seek to indentify unsupervised learning objectives, 
supervised tasks or self-supervision schemes and their combinations that lead to representations that
perform well on a large variety of problems. 

\subsection{Learning representations of words}

%\begin{quote}
%Is there a way, however, to represent text in a more general way?
%\end{quote}

Most attempts to build  general representations for words are based on the 
\emph{distributional hypothesis} of word-meaning, which states that the degree to which words are similar 
is a function of the similarity of the linguistic contexts they appear in. In other words similar 
words appear in similar contexts. To aid the readerr with techincal and historical context 
on the topic Section~\ref{sec:count} introduces earlier \emph{count-based} methods building word-context co-occurrence vectors-spaces, 
while  Section~\ref{sec:pred} presents the  \emph{prediction-based} framework in more detail as it closely related
to the approaches presented in this thesis.

Word-representations within the latter framework are an instance on \emph{representation learning}:
word representations are usually learned through
optimizing model parameters to predict contexts from words or words from contexts.
Such learned word-representations have proven successful in many applications especially 
in recent-years, however, they are not \emph{realistic} in a certain sense. 
This leads us to the main topic of the thesis namely \emph{visual grounding} introduced in the
following section.

\subsection{Visually grounded word representations}

%\begin{quote}
%Can we learn useful linguistic representations just by reading texts?
%\end{quote}

As exposed in more detail in Section~\ref{sec:visualwords} many theories of human cognition supported by empirical
evidence state that human language and concept representation and acquistion is \emph{grounded}
in perceptual and sensori-motor experiences. Cross-situational word learning, 
an influential cognitive account to human word-learning, supposes that humans learn the
meanings of words exploiting repeated exposure to linguistic contexts paired with preceptual reality.

Furthermore, consider the applicability of distributional language represenations in the larger scope of Artificial 
Intelligence. One of the dreams of AI is to develop technology to power intelligent embodied agents
taking the form of office assistants or emergency aid robots. These machines cannot implement natural
language as an arbitrary symbol manipulation system akin to a calculator's understanding of quantities or slopes. 
Similarly to humans they need
to link linguistic knowledge to the extra-linguistic world. Together the aforementioned cognitive 
and these practical considerations drove many works, including the Chapters of the present thesis, 
to explore grounded linguistic representations.

In terms of computional modeling the jump from 
distributional to grounded models is simple to summarize: one needs to collect data where
the \emph{contexts} of linguistic units are \emph{extra-linguistic} and represent these contexts such
that they can be provided as input to representation learning methods.
More concretely in terms of extra-linguistic context the present thesis focuses on \emph{visual-modality}.

Linguistic-visual multi-modal representations on the word-level have a well established albeit somewhat breif history 
detailed in Section~\ref{sec:distvis}. Such methods fall in both the \emph{count-based}
and \emph{prediction-based} framework using computer-vision techniques to represent the \emph{visual
modality} and NLP methods to represent texts. These separate spaces are then combined into a single multi-modal representation.

As the \textbf{first contribtuion} of the thesis in Chapter~\ref{ch:TAL} we present an incremental cross-situational
model of word-learning introducing modern computer-vision techniques to computational cross-situational modeling of 
human language learning. Through our experiments we show that our presented model is competitive with state-of-the-art
\emph{prediction-based} distributional models and that our model can name relevant concepts given images.

\subsection{Visually grounded sentence representations}

When moving from \emph{atomic} words to the \emph{compositional} world of sentences we need flexible
models that can represent word-order and hierarchical relationships.  In Chapters~\ref{ch:COLI}, \ref{ch:IJCNLP} and \ref{ch:ConLL} 
we use Recurrent Neural Networks to represent sentences; a powerful class of models to represent sequences.
Section~\ref{sec:sentences} provides the reader with 
historical and technical background to the modeling considerations behind this choice.

The study of general sentence representation learning has a much briefer history then word-representations
and Section~\ref{sec:trans-sentence} situates the reader in the area. Most of these approahces to date are 
based also on the distributional hypohtesis and formulate general purpose representation learning as a sort of
linguistic context prediction, but on the sentence level. 

Section~\ref{sec:visualsentences}
describes the general framework of learning visually-grounded sentence representations and their utility. 
The basic idea is still context-prediction, however, we learn associations between sentences and their \emph{visual} 
contexts i.e model parameters are optimized such that realted image--sentence pairs get pushed close together and 
unrelated pairs far from each other in a learned joint space.

As the \textbf{second contribution} of this thesis in Chapter~\ref{ch:COLI} 
we train such an architecture and more importantly explore the learned representations learned by such an architecture.
Our main interest and contribution here the development general techniques to \emph{interpret} linguistic representations learned by
Recurrent Neural Networks and the use these tools to contrast text-only language models with their grounded
counter-parts trained on the same sentences.

\subsection{Visual grounding to bridge between languages}

One of the intriguing aspects of using the visual modality as a natural occuring meaning representation is that it
is also naturally universal across languages. 
In other words visual modality anchoring linguistic representations to perceptual reality, but also
also provides a natural bridge betweem languages of various languages. Linguistic utterrances that are similar 
to each other, intuitively, appear in perceptually similar scenes across languages. 
Using this intuition on the word-level the visual modality can be used to find possible translations 
for words when no dictionary is available. Section~\ref{sec:bilinglex} provides the reader with more
detail on bilingual lexicon induction. Extending this idea to sentence level gives rise to techniques that
use the visual modality as a pivot to translate full sentences. Approahces in this direction are discussed
in Section~\ref{sec:imgpivot}. 

From another point of view utterrances in multiple languages corresponding perceptual representataions 
can be conceptualized as \emph{mutliple-views} of the same underlying \emph{ideal datapoint}.
Learning to map these multiple views in the same feature-space can lead to better representations as
they have to be more \emph{general} due to the model having to solve multiple tasks at the same time. 
This \emph{multi-view learning} perspective is explained in more detail in Section~\ref{sec:multiview} 
focusing on the specific case of multi-modal and multi-lingual representations.

The \textbf{third contribution} in the thesis combines visually grounded sentence representation learning
with machine translation. More specifically in Chapter~\ref{ch:IJCNLP} we present a \emph{multi-task} 
learning architecture that jointly learns to associate English sentences with images and to translate from
English to German. 




\section{Background: Distributed linguistic representations}
\label{introducion}
As opposed to the continuous nature of the visual world, linguistic units
are inherently discrete. This is reflected in the techniques used in the
computational processing of natural language.
Information in linguistic utterances is traditionally encoded
in sparse high-dimensional count vectors, where each
dimension corresponds to the number of times a specific feature occurs in a
sample. For each word in a sentence common features to consider take the form of:
``this token is a noun'', ``the preceding word is cat'',
``the following token has dependency label direct object''.
More often then not these vectors do not even hold counts, but 0-1
indicator features signaling the presence or absence of a particular property.

In such a setting the relationships between features are not handled.
As a result machine learning algorithms are presented with an input representation where
``the preceding word is cat'' and ``the preceding word is dog''
are completely independent properties.
Furthermore, interactions between features has to be
explicitly represented as a separate entry in the vector, which once again has
no clear relationship to its parts as far as the learning algorithms are concerned.

Let us consider for a simple example the problem of language modeling.
Our  aim is to find a probabilistic model
that assigns high probability to likely sequences and low probability to gibberish.
The representation of words and phrases in common n-gram language models is on the
very extreme of discrete: they are represented by their \emph{identity}.
Imagine that in the training set there were a number of sentences expressing the concept
of ``someone is walking a dog'', a little less mentions of ``someone walking a cat''
and no occurrence of ``someone walking a serval'', but the word ``serval'' did occur.
I have never seen anyone walking a serval, however, as a human I do know that it is a dog-sized
feline creature and as a consequence I can imagine someone taking a walk with one.
However, a language model trained with discrete symbolic representation cannot
\emph{generalize} in such a way as the
relationship between the words ``dog'', ``cat'' and ``serval'' is not represented.

Consider now the case of walking a dolphin.
Both ``walking a serval'' and ``walking a dolphin'' would be impossible
by the hypothesized language model, however, us humans would say that walking the serval is
kind of possible, but walking the dolphin is just surreal.
We know it from reading about both animals that they can be domesticated
, however, by their physical properties while one affords to be walked
the other does not. Dogs, cats, servals and dolphins are similar in some
dimensions and different in others.
What the present thesis is concerned about is
\emph{learning continuous representations} of words, phrases up to sentences
from multiple sources of information to encode such useful properties.
Such representations allow machine learning algorithms to represent smooth
functions over linguistic inputs where the local neighborhoods of linguistic
units correspond to syntactic and semantic regularities.

\section{Grounded and multilingual representations}
The well established tradition of continuous word representations consider
features extracted from large text-corpora in a single language focusing on
co-occurrence statistics between words. 
This distributional approach to word meaning hypothesizes that semantically related words
tend to appear in similar contexts. This idea goes back in linguistics tradition to the
earlier days of American structuralism \citep{nevin2002legacy}. In the seminal paper
"Distributional structure" \citep{harris1954distributional} back in 1954 Harris already claims
that distribution should be taken as an explanation for word meaning and that similarity classes
can be constructed based on co-occurrence statistics.
\cite{cruse1986lexical} writes that 'It is  assumed  that  the  semantic properties  of
a lexical  item  are  fully  reflected  in  the  appropriate  aspects  of  the  relations
it  contracts  with  actual  and  potential  contexts' and that 'the  meaning  of  a word
is constituted  by  its  contextual  relations'. Computational models of distributional
semantics implement this intuition and learn real-valued word vectors based on co-occurrence
statistics in large text corpora. Section~\ref{sec:words} presents 
such representations based on the \emph{count-based} in Section~\ref{sec:count}
versus \emph{prediction-based} in Section~\ref{sec:pred}
distinction borrowed from \cite{baroni2014don}. Most architectures presented  
in this thesis are based on the developements in Neural Language Models detailed
in Section~\ref{sec:NNLM}. Here we present the methods focusing on the historical context,
but combined with some standard notation
providing background for later chapters. Section~\ref{sec:w2v} introduces efficient-linear 
models for predictive word-learning for two main reasons: 1.) they a had a tremendous impact
on shaping the current landscape of continuous linguistic representation learning and are still
heavily used at the time of writing this thesis, 2.) our main point of comparison between
our word learning model in Chapter~\ref{ch:TAL} is one of such models detailed in the section.


My thesis focuses integrating the \emph{visual modality} and jointly learning representations 
of words up to sentences in \emph{multiple languages}. Furthermore, I study the beneficial interaction of
visually grounded and multilingual representation learning.
Grounding linguistic representattions in visual modality is largely motivated by evidence from
perceptual grounding in human concept acquisition and representation \citep{barsalou2003grounding}.
As such it brings computational language learning systems closer to human-like learning.
Furthermore, as pointed out by the dolphin versus serval example in the Preface
certain aspects of meaning are better captured by \emph{encyclopedic}
knowledge such as domestication abundant in textual data, however, perceptual information
can provide complementary valuable insights into physical properties rarely mentioned in texts such as size,
shape and color. In practice harnessing the visual modality to learn language
representations that link linguistic knowledge
to the external world \citep{kiela2014improving,baroni2016grounding,elliott2017imagination,kiela2017learning,yoo2017improving}
has been empirically shown to improve performance on several semantic tasks.

\paragraph{Images bridging languages}

Traditionally, given a task for each language separate corpora are created and
separate sets of parameters are learned.
Learning shared cross-lingual representations, however, allows researchers
and practitioners to train a single model allowing to transfer knowledge
between languages leading to practical consequences
such as mitigating the low-resource problem, cross-lingual applications and
processing data with code-mixing.
Perceptual grounding and multilingual representations have their separate benefits,
however, they can be beneficial to each other as well.
In my work I view images as universal meaning representations, a natural common
ground bridging between languages. From another perspective images and multiple
languages can be seen as multiple latent views of the same underlying semantic
concepts. This approach allows us to take advantage of complementary information
and potential transfer between these views.

\paragraph{Contributions}
In Chapter~\ref{ch:TAL} we start by learning visual representations for words using a
novel computational cognitive model of cross-situational word learning that
takes words and high-level continuous image representations as input. Our approach
integrates recent advances of computer vision into incremental cognitive models
of language learning. We show that on the data sets that we consider our model
is competitive with state-of-the-art distributional semantics models (word2vec)
on word-similarity benchmarks. Furthermore we show that our model is able to
name relevant concepts given images.

Chapter~\ref{ch:COLI} introduces a recurrent and convolutional neural network
based model that learns from both visual-grounding signals and
word-word co-occurrences. We develop techniques to interpret the learned
representations of such an architecture and investigate if certain linguistic
phenomena is encoded in the learned model.

Chapter~\ref{ch:IJCNLP} provides empirical evidence that grounded learning
can improve machine translation quality.
In Chapter~\ref{ch:ConLL} we show under what conditions multilinguality an help improve grounded
representations.
%Multilingual systems also offer a fertile ground computational typology as shared models are forced to exploit cross-lingual regularities.

%My work focuses on learning grounded distributed sentence representations.
%The first half of the thesis is about learning the representations of words and followed-up by learning sentence embeddings for a single language. Later we move on to learning multilingual visually grounded representations. In other works of mine I contributed to generating referring expressions grounded in Wikipedia entities and learning language through interacting with text-games grounded in a knowledge-base.



\section{Distributed word-representations}
\label{sec:words}
%The distributional approach to word meaning hypothesizes that semantically related words
%tend to appear in similar contexts. This idea goes back in linguistics tradition to the
%earlier days of American structuralism \citep{nevin2002legacy}. In the seminal paper
%"Distributional structure" \citep{harris1954distributional} back in 1954 Harris already claims
%that distribution should be taken as an explanation for word meaning and that similarity classes
%can be constructed based on co-occurrence statistics.
%\cite{cruse1986lexical} writes that 'It is  assumed  that  the  semantic properties  of
%a lexical  item  are  fully  reflected  in  the  appropriate  aspects  of  the  relations
%it  contracts  with  actual  and  potential  contexts' and that 'the  meaning  of  a word
%is constituted  by  its  contextual  relations'. Computational models of distributional
%semantics implement this intuition and learn real-valued word vectors based on co-occurrence
%statistics in large text corpora. 
Here we present such representations using the \emph{count-based}
versus \emph{prediction-based} distinction borrowed from \cite{baroni2014don}.
%One of the first computational verification
%attempts of  this distributional hypothesis was put forward in \cite{miller1991contextual} \todo{say something about the paper man}.

\subsection{Count-based approaches}
\label{sec:count}
Early computational linguistics models of distributional semantics fall in the category of
count-based approaches: they count the number of times target words appear occur in different
contexts resulting in a co-occurrence matrix. In the co-occurrence matrix
$W$ each row $W_i$ represents a word $i$ and each column $W_{,j}$ a context. It has size
is a $W \in \mathbb{N}^{|V| \times |C|}$ where $V$ is the set of words and $C$ is
the set of contexts.
The entry $W_{i,j}$ is the number of times word $i$ appears in context $j$.
Contexts are typically words appearing within a certain window size or text documents.
To the counts in $W$ various re-weighting schemes are applied followed by some matrix factorization algorithm
resulting in a lower dimensional dense representation $W \in \mathbb{R}^{|V| \times d}$, where $d < |C|$.
%Both re-weighting and low-rank approximation reduces the dimensionality of the sparse matrix and leads
%to faster compute and better generalization.
The earliest approaches include Hyperspace Analogue to Language \citep{lund1996producing},
which constructs a term-term co-occurrence matrix and Latent Semantic Analysis \citep{dumais2004latent},
which applies the tf-idf re-weighting scheme on a term-document matrix followed by singular-value decomposition.
More recent approaches apply different re-weighting schemes such as point wise mutual information and local mutual
information \citep{evert2005statistics} or different matrix factorization algorithms such as non-negative
matrix factorization \citep{baroni2014don}. For a comprehensive set of empirical experiments on count-based
approaches please consult \cite{bullinaria2007extracting} and \cite{bullinaria2012extracting}.

\subsection{Prediction-based approaches}
\label{sec:pred}
In more recent years
%-- and more related to the present thesis --
various deep learning methods have been applied to learn distributed word-representations usually referred to
as ´word-embeddings´ in the literature. Contrary to count-based methods prediction-based approaches fit into the
standard supervised learning pipeline: they optimize a set of parameters to maximize the probability of words
given contexts or contexts given words, where the word-embeddings $\mathbf{W}$
themselves form a subset of the parameters of the full model.

\subsubsection{Neural language models}
\label{sec:NNLM}
The first modern approach to neural language models on realistic data sets was introduced
in \cite{bengio2003neural} laying down the framework for recent developments.
It is a feed-forward multilayer perceptron with continuous word-embeddings,
a single hidden layer and a softmax output layer.
More precisely the model is parametrized by a 1.) word-embedding matrix
$\mathbf{W} \in \mathbb{R}^{|V| \times d}$, where each row corresponds to a word-vector
$\mathbf{W_i}$ of size $d$ and each column $\mathbf{W_i,j}$ to a learned feature, 2.)
learned hidden and output weight-matrices $\mathbf{H}$ and $\mathbf{U}$. Similarly
to the case of count-based embeddings $d < |V|$.
The network takes as input the concatenation of $n$ word-vectors preceding the target word as context
representation $\hat{\mathbf{w}} = [\mathbf{w_1}; \ldots; \mathbf{w_{n-1}}]$
and outputs the probability distribution over the current word $w_n$.
The computation of the network is shown in Equation~\ref{eq:bengio}:

\begin{equation}
\label{eq:bengio}
P(w_n|w_{n<n}) \approx \text{softmax}(\mathbf{U} \text{tanh}(\mathbf{H} \mathbf{\hat{w}} + b_h ) + b_u)
\end{equation}
The model is trained to maximize the probability of
the target word given the previous fixed number of words as context over a training corpus
-- $n$-gram language model -- trained with stochastic gradient descent \citep{cauchy1847methode}
through the backpropagation algorithm \citep{rumelhart1985learning}.
The superior performance of the feed-forward neural language model has soon been shown to
improve performance in speech recognition \citep{schwenk2005training}.

Following a similar recipe the convolutional architecture of \cite{collobert2008unified}
based on the time-delay neural network model \citep{waibel1990phoneme} took several steps towards the
by now standard practices in deep NLP.
Contrary to the simple feed-forward network language model the convolution over-time structure can handle
sequences of variable length essential in NLP applications with sentences containing varying number of words.
The paper also introduces the idea of jointly learning many tasks at the same time such as part-of-speech
tagging, chunking, named entity recognition and semantic role labeling through \emph{multi-task learning}.
We also make use of the multi-task learning strategy in Chapters~\ref{ch:TAL}, \ref{ch:IJCNLP}, \ref{ch:ConLL}.
Finally, \cite{collobert2008unified} were the first to show the utility of pre-trained word-embeddings
in other tasks. Their architecture was later refined in \cite{collobert2011natural} and the pre-trained
full model was made available alongside the standalone word-embeddings in the SENNA toolkit.

The architecture, however, that arguably became most popular in NLP and is used in
Chapters~\ref{ch:COLI}, \ref{ch:IJCNLP}, \ref{ch:ConLL} is the recurrent neural network. The architecture was
first introduced by the late Jeffrey L. Elman in his seminal paper \citep{elman1990finding}
as a model of human language learning and processing. Recurrent neural networks
take a variable length sequence as input and perform a stateful computation over the it's elements.
In case of language modeling at each time-step $t$ the network takes an input word $w_t$
and the previous hidden state $\mathbf{h_{t-1}}$ maintained through previous time-steps to compute the current
state $\mathbf{h_t}$ and to predict the probability distribution over the following word $w_t$.
It is parametrized a word-embedding matrix $\mathbf{W} \in \mathbb{R}^{|V| \times d}$, input to hidden weight
matrix $\mathbf{W_i}$, hidden to hidden weight matrix $\mathbf{W_h}$ and finally the hidden to output
weight matrix $\mathbf{U}$ used to predict the unnormalized probabilities over the vocabulary entries:
\begin{align}
P(w_n|w_{n<n}) &\approx \text{softmax}(\mathbf{U} \mathbf{h_t} + \mathbf{b_o}) \\
\mathbf{h_t} &= \text{tanh}(\mathbf{W_h}\mathbf{h_{t-1}} + \mathbf{W_i}\mathbf{w_t} + \mathbf{b_h})
\end{align}
\cite{elman1991distributed} shows when trained on simple natural language like input the
hidden states of the network $\mathbf{h_t}$ encode grammatical relations and hierarchical
constituent structure. In Chapter~\ref{ch:COLI} we train recurrent network architectures
on real-world data and explore similar questions about the learned opaque representations.
Despite the early successes, however, it turned out to be difficult to train
Elman networks on practical applications with longer sequences
due to the vanishing and exploding gradient phenomena \citep{bengio1994learning}.
Never the less the RNNLM implementation of \cite{mikolov2010recurrent}
established a new state-of-the-art and interestingly the successfully
application the standard backpropagation through time algorithm  \citep{williams1995gradient}
to simple recurrent networks on real world language modeling \cite{mikolov2012statistical}
seems to only have required more computational resources and the now widely used gradient clipping trick.

At the time of writing this thesis an overwhelming amount of empirical evidence
shows that long short-term memory (LSTM) \citep{hochreiter1997long,gers1999learning}
and gated recurrent units (GRU) \citep{cho2014learning}
vastly outperform the simple Elman network in practice. We use GRUs in
Chapters~\ref{ch:COLI}, \ref{ch:IJCNLP}, \ref{ch:ConLL}.

\subsubsection{Efficient linear models}
\label{sec:w2v}
It wasn't until the introduction of the much simpler and faster continuous bag-of-words
and skip-gram with negative sampling (SGNS) algorithms \cite{mikolov2013efficient}
packaged into the easy to use \texttt{word2vec} toolkit that
word-embeddings became ubiquitous in computational linguistics and NLP research.
These algorithms rely on simple log-linear models as opposed to the more expensive
neural networks leading to faster training on larger corpora. The more successful SGNS
model has two word embeddings matrices one for the words
$\mathbf{W_w}  \in \mathbb{R}^{|V| \times d}$
and a separate one for the contexts $\mathbf{W_c} \in \mathbb{R}^{|V| \times d}$.
The model is trained to maximize the dot product $\mathbf{w}^T\mathbf{c}$
between true word-context pairs $(w,c)$ appearing in corpus and minimize the
dot product $\mathbf{w}^T\mathbf{\hat{c}}$ between randomly
samples contrastive examples $(w,\hat{c})$. This kind of negative-sampling or
contrastive-sampling strategy is used in Chapters~\ref{ch:IJCNLP} and \ref{ch:ConLL} to
match images and corresponding sentences.

Finally the GloVe approach \citep{pennington2014glove} is another simple linear model
with word $\mathbf{W_w}  \in \mathbb{R}^{|V| \times d}$ and context
$\mathbf{W_c} \in \mathbb{R}^{|V| \times d}$ embeddings, which
represents a hybrid between count- and prediction-based techniques:
it optimizes word-embeddings to predict the re-weighted
$\log$ co-occurrence counts $\log(\#(w,c))$ collected from corpora $D$:

\begin{equation}
\label{eq:glove}
\mathbf{w}^T\mathbf{c} + b_w + b_c = \log(\#(w,c))\;\;\; \forall (w,c \in D)
\end{equation}

where $b_w$ and $b_c$ are context specific bias terms.
Now let us express the Glove model in the traditional re-weighted
co-occurrence matrix factorization form.
Let the matrix $M \in \mathbb{R}^{|V| \times |C|}$ store the log-weighted
weighted counts and $\mathbf{b_w}$ and $\mathbf{b_c}$ be the vectors of all
$b_w$ and $b_c$ scalars, then

\begin{equation}
\label{eq:glove2}
M \approx \mathbf{W} \cdot \mathbf{C^T} + \mathbf{b_w} + \mathbf{b_c}
\end{equation}

In fact there has been work on finding relationships between
count- and prediction-based methods \citep{levy2014neural} and
using insights from both to develop novel improved
variants \citep{levy2015improving}.

%\paragraph{Transferring word representations}

%Later the word-embeddings $\mathbf{W}$ learned by recurrent language models were
%shown to represent intriguing syntactic and semantic regularities \cite{mikolov2013linguistic}.

%In earlier work where pre-trained word-embeddings were also shown for the first time through a
%larger-scale empirical investigation by \cite{turian2010word} to transfer to sequence prediction tasks
%when used as features in state-of-the-art systems.


\section{Visually grounded representations of words}
\label{sec:visualwords}
% \todo{ADD MORE FROM THIS https://arxiv.org/pdf/1806.06371.pdf}

\subsection{Language and perception}
\label{sec:langperc}

The approaches to learn distributed representations discussed so far focus on extracting information exclusively
from text corpora. To human language learners, however, a plethora of perceptual information is available to aid the
learning process and to enrich the representations. The link between human word and concept representation
and acquisition and the perceptual-motor systems has been well established through behavioral
neuroscientific experiments \citep{pulvermuller2005brain}.
The earliest words in children's language acquisition tend to be names of concrete perceptual phenomena
such as objects, colors and simple actions \citep{bornstein2004cross}. Furthermore, children generalize to
the names of novel objects based on perceptual cues such as shape \citep{landau1998object}.
In general, the \emph{embodiment} based theories of concept representation and acquisition in the
cognitive scientific literature put forward the view that a wide variety of cognitive processes
are grounded in perception and action \citep{meteyard2008role}. That said the precise role of
sensori-motor information in language acquisition and representation is a highly
debated topic \citep{meteyard2012coming}.

Various computational cognitive models of child language acquisition investigate the issue
by learning word meanings from small scale or synthetic multi-modal data. The model presented by
\cite{yu2005emergence} uses visual information to learn the meanings of object
names whereas the architecture of \cite{roy2002learning} learn to associate word sequences
with simple shapes in a synthetically generated data setting.

Interestingly even the articles introducing Latent Semantic Analysis
\citep{landauer1997solution} and Hyperspace Analogue to Language \citep{lund1996producing} mention
that a possible limitation of the presented distributional semantic models is the lack of
grounding in extra-linguistic reality. \cite{landauer1997solution} puts it as "But still, to be more than
an abstract system like mathematics words must touch reality at least occasionally."
The lack of relationship between symbols and the external reality is usually referred
to as the \emph{grounding problem} in the literature \citep{harnad1990symbol,perfetti1998limits}.
On the defense of purely textual models \cite{louwerse2011symbol} argues that the corpora
used to train distributional semantic models are generated by humans and as such reflect the perceptual world.
For a counter argument consider how many pieces of text to states obvious perceptual
facts such as "bananas are yellow" \citep{bruni2014multimodal}.
In practice much work on multi-modal distributional semantics have found that text-only spaces tend to
represent more encyclopedic knowledge, whereas multi-modal representations capture more concrete aspects
\citep{andrews2009integrating,baroni2008concepts}. However, one does not necessarily need to
reach a conclusion on whether grounded or distributional models are superior, rather combining their merits in a
pragmatic way is an attractive alternative \citep{riordan2011redundancy}. Learning representations
from both linguistic and visual input is a step towards realistic models of language learners.
%Note however, it is as close as assuming children learn language by sitting still and watching TV.

\subsection{Combined distributional and visual spaces}
\label{sec:distvis}

When learning multimodal word representations we wish to construct a matrix $W$, where each row $W_i$
corresponds to a word and each feature column $W_{,j}$ represents a distributional feature,
a perceptual feature or mixture of the two.
The first approach to learn visual word representations from realistic data sets
was introduced by \cite{feng2010visual}. They develop a multi-modal topic model trained on a
BBC News based containing the articles and the image illustrations. Each document considered by their model
is a pair $<D, I>$ comprised of a text document $D$ and image $I$.
The generative story starts by sampling multi-modal topics,
which then generate both the text document $D$ and the corresponding images $I$.
To be able to apply topic models \cite{feng2010visual} represents images
in a discrete space using the bag-of-visual-words approach \citep{csurka2004visual},
a popular computer vision method at the time.
They apply difference-of-Gaussians point detector to segment images
into local regions, each of which are represented by SIFT features \citep{lowe1999object}.
These features are then clustered with K-means, which leads to a
bag-of-visual-words (BoVW) representation i.e.
a vector of counts, where each entry is the number of regions on the image that
corresponds to certain SIFT-cluster. Given count-based representations for both
text documents and their paired images the standard
Latent Dirichlet Allocation topic modeling algorithm is applied \citep{blei2003latent}.
For the algorithm each pair $<D, I>$ is represented by the concatenation
$[\psi(D);\phi(I)]$ of textual $\psi(D)$ and visual features $\phi(I)$.
After convergence a word is represented by a vector,
where each entry is the probability of that word given a particular topic.
Note that these topics are latent variables inferred from
both distributional and perceptual features of the documents hence represent a joint
visual-textual multimodal space. \cite{feng2010visual}
shows that the multi-modal LDA model outperforms the text-only LDA representations by a
large-margin on word association and word-similarity experiments.

The perceptually grounded word representations of \cite{bruni2012distributional}
also combines distributional semantic models and BoVW pipelines.
Rather than having a collection of documents $<D, I>$ they consider a set of words for, which
both distributional and image features are available. For visual word representations
they use images labeled with tags by annotators. Similarly to \cite{feng2010visual} they
apply the BoVW feature extraction pipeline with SIFT and color features to
represent images. Contrary to \cite{feng2010visual}, however,
a visual-only representation is created for each tag-word by summing over the features
for all images corresponding to the tag.
For text-only models they construct several types of distributional semantic spaces
from text-only corpora unrelated to the images.
Note that as in the previous example we wish to create a multimodal
word representation and applied separate functions to extract textual $\psi$ and visual
$\phi$ features. Finally they create the multi-modal space through concatenation.
On word-similarity benchmarks they show that the text-only model performs better than visual-only
and that the combination of the two surpasses both.
They also find that distributional semantics
models perform poorly on finding the typical colors of concrete nouns,
whereas the visual and multi-modal models perform perfectly. In these experiments
distributional semantics models fail to capture the obvious fact that ``the grass is green''
providing evidence against the theoretical argument that perceptual information is available in
large collections of texts and so grounded representations are superfluous \citep{louwerse2011symbol}.
Combining BoVW and count-based distributed word-representations was the standard methodology
in many other works on multi-modal word representations at the time
\citep{bruni2011distributional,leong2011going,leong2011measuring}.

\cite{bruni2014multimodal} frames multi-modal distributional semantics under a general framework
: taking as input separate textual and visual features for words followed by
re-weighting and matrix factorization. Researchers can apply vector-space
models to create the distributional space $W^d$ and computer vision techniques to create
visual feature representations for the same words $W^v$. The separate feature spaces
are mixed together by first concatenation $W = [W^d;W^v]$ optionally followed by singular value decomposition
to combines the visual and textual features.

For example \cite{kiela2014learning} runs the SGNS algorithm
on large text corpora and takes the word-embedding parameters
$\mathbf{W} \in \mathbb{R}^{|V| \times d}$ as the distributional space.
On top of experimenting with BoVW features they apply pre-trained convolutional
neural networks (CNN) as black-box image
feature extractors. These architectures learn a hierarchy of blocks of image filters followed by
pre-defined pooling operations optimized for a particular task.
It has been observed in the computer vision community that the lower layers of
deep CNNs trained across various data sets and tasks tend to learn filter maps
that resemble Gabor-filters and color-blobs. Intuitively these low-level features appear to be general
and as such afford \emph{transfer}. Around the time there was extensive work on exploring the transferability
of CNN features to various computer vision tasks through fine-tuning
 \citep{donahue2014decaf,oquab2014learning} or simply taking the last layer representation of CNNs
as high-level features as inputs to linear classifiers \citep{girshick2014rich,sharif2014cnn}.
Given such successes in vision it is natural to apply CNNs in the visually grounded language learning
community as black-box image feature extractors. Similarly to \cite{bruni2014multimodal} in
\cite{kiela2014learning} the visual features of words are computed as the summary the feature vectors
extracted from all images they co-occur with $\mathbf{I} \in \mathbb{R}^{|V| \times i}$, where $i$ is
the size of the CNN image-embedding.
Applying the simple concatenation operation to the two spaces $[\mathbf{W};\mathbf{I}]$
they show that CNN features outperform BoVW features on word-similarity
experiments with the multi-modal word-representations.

%All approaches described so far require both visual and textual information for the same concepts.
%The multi-modal skip-gram \cite{lazaridou2015combining} model was developed to alleviate such limiation:
%it is a multi-task extension of the skip-gram algorithm predicting the both the context of words, but also
%the visual representations of concrete nouns. Visual representations are constructed to averaging
%the CNN representations \cite{krizhevsky2012imagenet} of 100 pictures sampled from ImageNet \cite{deng2009imagenet}.
%This architecture was later proposed as a model of child language learning in and
%was applied to the CHILDES corpus \cite{macwhinney2014childes} with modifications to model referential
%uncertainty and social cues \cite{lazaridou2016multimodal} and compared to human learning performance \cite{lazaridou2017multimodal}.

\paragraph{Our contribution}
Chapter~\ref{ch:TAL} presents a computational cognitive model of word learning using only
visual information developed at the same time as the multi-modal skip-gram approach.
Similarly to \cite{feng2010visual} we assume pairs of text and images
$<D, I>$. However, in our data set images represent every day scenes and are paired with descriptive
sentences to mimic the language environment of children on a high level.
Our model implements the extreme approach of cross-situational language
learning and assumes that the representations of words are learned exclusively through the co-occurrences
between visual features and words. Rather than building a
matrix of image-features per word and then summarizing as in \cite{kiela2014learning},
we run an online expectation-maximization \citep{dempster1977maximum} based
algorithm to align words with image features mimicking child language learning.
In essence, our approach combines the cross-situational incremental word-learning model
of \cite{fazly.etal.10csj} with the modern convolutional image representations.
The result of the learning process is a word embedding matrix $\mathbf{W_i}$,
where each row $\mathbf{W_i}$ corresponds to a word, each column $\mathbf{W_{,j}}$
to a CNN feature and each entry $\mathbf{W_{i,j}}$ to the strength
of the relationship between the word $i$ and image-feature $j$.
We show through word-similarity experiments that, while our approach performs on
par with the SGNS trained on the same data set, there is a qualitative difference between
the learned embeddings: our visual word-embeddings represent concrete nouns closer to human
judgements then abstract nouns. As each word embedding $\mathbf{W_i}$ is represented in the
image-feature space we conduct show that our model can label images with related nouns through
simple nearest neighbor search.

\section{From words to sentences}
\label{sec:sentences}
Applying the distributional intuition to model the meaning of sentences is not as straightforward
as it is for words.
Imagine building a sentence-embedding matrix $W$ where each row $W_i$ corresponds to
a possible sentence $i$ and each column $W,j$ to a feature $j$.
How many rows should we include? Intuitively the number of words in a corpus is much
lower than the number sentences: one can assume a large, but finite set
of existing words and an infinite set of
potential sentences to \emph{compose} from them.
Words can be though of as \emph{atomic units} and
in downstream applications one can use a lookup operation on a word embedding matrix
to represent units in the input. However, it is infeasible to lookup full sentences.
In fact, from a method that represents sentences in a continuous space one would expect
to also represent and generalize to unseen sentences at test time.
Furthermore, given two sentences $\mathit{John \; loves \; Mary}$ and
$\mathit{Mary \; loves \; John}$ we wish our sentence encoding function
$f_{enc}$ to represent the meaningful difference stemming from the word order
$f_{enc}(\mathit{John \; loves \; Mary}) \neq f_{enc}(\mathit{Mary \; loves \; John})$.
As such we seek to learn a sentence encoder $f_{enc}$ that is sensitive to
syntactic structure or in other words \emph{compositionality} i.e.:
the notion that the meaning of an expression is
a function of its parts and the rules combining them \citep{montague1970english}.

Earlier work developed under the framework of compositional distributional semantics produce
continuous representations for phrases up to sentences using additive and multiplicative interactions
of count-based distributed word-representations \citep{mitchell2008vector} or combine symbolic and
continuous representations with tensor-products \citep{clark2007combining}.
The latter line of work culminated in a number of unified theories of
distributional semantics and formal type logical and categorical grammars
\citep{coecke2010mathematical,clarke2012context,baroni2014frege}.
These approaches assume that words are represented by distributional word embeddings
and define compositional operations on top of them motivated by particular formal
semantic considerations. This line of work have not resulted in
machine learning approaches to solve natural language tasks
on real world data probably due to the different scope and
the computationally expensive high-order
tensor operations involved \citep{bowman2016modeling}.

Finally, before going forward with the more recent neural models,
it is only fair to mention that bag-of-words
based representations bypassing the issue of compositionality form a set of strong baselines
for a number of sentence level tasks \citep{hill2016learning}.
These simple baselines include unigram tf-idf vectors, which represent sentences as
smoothed word-count vectors or the sum/average of SGNS/GloVe word-embeddings.

\paragraph{Transferable sentence representations}
\label{sec:trans-sentence}

Section \ref{sec:words} discusses the feed-forward \citep{bengio2003neural}
and recurrent network based models \citep{mikolov2010recurrent} from the perspective
of learning word-representations. However, both architectures learn embeddings
of sequences of larger sizes as such they do not only learn to represent
words, but phrases and potentially full sentences. When learning transferable
sentence representations there are two main considerations we will discuss:
1.) which architecture to choose and 2.) what objective to optimize.

Various neural network architectures have been proposed that
handle variable sized data structures: recurrent networks such as
take the input sequentially
b.) convolutional neural networks
\citep{kalchbrenner2014convolutional,zhang2015character,conneau2016very,chen2013learning}
process fixed sized n-gram patterns up to a large window,
c.) recursive neural networks \citep{goller1996learning.socher2011parsing,tai2015improved}
take a tree data structure as input such as a sentence according to the traversal of a
constituency tree d.) graph neural networks operate on graphs \citep{marcheggiani2017encoding} such as
syntactic/semantic-dependency representations.
These architectures take word-embeddings as input compute a fixed size representations of
sentences. These representations are tuned to a specific task such as sequence tagging,
sentence classification, machine translation or the aforementioned language modeling.


In Chapters \ref{ch:COLI}, \ref{ch:IJCNLP}, \ref{ch:ConLL} we decided to apply
recurrent neural networks as sentence encoders.
Recursive neural networks provide a principled approach to
compute representations along the nodes of constituency
\citep{socher2013recursive} or dependency \citep{socher2014grounded} parse trees.
In practice, however, these architectures require parse trees as input,
which makes them impractical for our mission of learning visually grounded
representations for multiple languages.
For each language considered, the training procedure requires finding a good
pipeline from text pre-processing to parsing to generate the input representations
for the networks. Furthermore, tree structures by nature do not
afford straightforward batched computation directly and tend to run dramatically
slower than recurrent or convolutional models. In terms of performance the jury
is still out, however, so far only modest improvements have been observed
over recurrent models on specific tasks in specific settings
\citep{li2015tree,tai2015improved}. For graph structures neural networks the
same argument holds. Two equally practical alternatives to RNNs
that operate on raw sequences are convolutional neural networks
\citep{bai2018empirical} and transformers \citep{vaswani2017attention}.


Contrary to word-embeddings the representations
and the composition function these models learn are task specific and not \emph{universal}.
For learning transferable and general distributed sentence representations the first notable approach is the
skip-thought vectors model \citep{kiros2015skip}.
This approach extends distributional intuition of the skip-gram approach to the sentence level.
They train their a sentence encoder on contiguous sentence sequences
-- such as books -- which learns representations predictive of the sentences around it.
More specifically
they train a recurrent encoder to encode sentence $s_i$ and use two separate recurrent decoders to
generate the pre- $s_{i-1}$ and post-contexts $s_{i+1}$.
This method was confirmed to be a successful self-supervised method for transfer
learning in a number of sentence
classifications tasks beating simpler approaches such as bag-of-words approaches based on skip-gram
or CBOW embeddings, tf-idf vectors and auto-encoders \citep{hill2016learning}. A convolutional
variant of the encoder was introduced in \cite{gan2016unsupervised} and several other works
train simple sum/average pre-trained word-embedding encoders using the same context prediction-style
loss \citep{kenter2016siamese,hill2016learning}. The larger context of paragraphs are
explored in \cite{jernite2017discourse} where the task is to predict discourse coherence
labels in a self-supervised fashion.

Later approaches have also focused on moving away from distributional clues
and identifying supervised tasks that lead to representations that transfer
well to a wide variety of other tasks. Natural language inference as an objective
was identified to learn good sentence embeddings \citep{conneau-EtAl:2017:EMNLP2017}
and \cite{subramanian2018learning} combines a number of other supervised tasks
with self-supervised training through multi-task learning.

The state-of-the-art in learning universal sentence representations %\todo{This paragraph is a bit broken}
at the time of writing the present thesis is
represented by neural language models with an extreme number of parameters
trained over huge corpora \citep{peters2018deep,devlin2018bert}.
These approaches go back to
exploiting only distributional cues and train a stack convolutional, recurrent layers
and/or transfomer layers on large-scale bidirectional language modeling.
For each task the forward and backward representations are extracted from the models
for each word position and then fed to a task specific recurrent network.
Such contextualized word-representations have been explored also using
the LSTM states of machine translation encoders \citep{mccann2017learned}.

\section{Visually grounded sentence representations}
\label{sec:visualsentences}

When aiming to learn general sentence representations text-only tasks
are usually considered by researchers leading to the grounding problem
as discussed in Section~\ref{sec:visualwords}. Given that visual information
has been shown to contain useful information for word-representations it
is a natural question to ask whether this observation generalizes to sentence
embeddings. We have seen in Section~\ref{sec:sentence} that sentence embedding
models exploit the distributional hypothesis by treating sentences as units and
predict their context. This intuition can be adopted to visual grounding i.e:
train sentence embeddings to be predictive of their visual context. The
structure of the data sets required for such a setup has the same form
as the visual-textual MixLDA approach of \cite{feng2010visual} or our
cross-situational word learning model \citep{kadar2015learning}:
pairs of sentences $s$ and images $i$ $<s, i>$.

The architecture we chose in
Chapters \ref{ch:COLI}, \ref{ch:IJCNLP}, \ref{ch:ConLL} trains a recurrent neural network to
represent variable length sentences and extract image features using pre-trained
convolutional neural networks. In Chapter \ref{ch:COLI} our loss function
is quite straightforward: the objective is to minimize the cosine distance
between the learned sentence and image representations. In later chapters we
apply a loss function similar to the negative sampling
in the skip-gram model \citep{mikolov2013distributed}: it minimizes the cosine
distance between true sentence-image pairs $<s, i>$ and maximizes the distance
between random contrastive pairs $<\hat{s}, i>$ and $<s, \hat{i}>$.
Representations learned by such a model has been shown \citep{kiela2017learning}
to improve performance when combined with skip-thought embeddings
on a large number of semantic sentence classifications tasks compared to
skip-thought only. Their findings were confirmed and improved upon using a
self-attention mechanism \citep{yoo2017improving}.

\paragraph{Note on data sets}


The data sets used for this purpose in all chapters were originally
introduced for image-captioning.
These data sets annotate images found in online resources with descriptions
through crowd-sourcing.
These descriptions are largely \emph{conceptual}, \emph{concrete}
and \emph{generic}.
This means that descriptions do not focus too much on \emph{perceptual}
information such as colors, contrast or picture quality; they do not mention
many \emph{abstract} notions about images such as mood and finally the descriptions
are not \emph{specific} meaning that they do not mention the names of cities,
people or brands of objects.
What they do end up mentioning are entities depicted in the images
(frisbee, dog, boy) their attributes (yellow, fluffy, young) and the
relations between them.
The images depict common real-life scenes such as bus turning left or people
playing soccer in the park. As such annotation collected independently from
different workers end up focusing on different aspects of these scenes\footnote{For a comprehensive overview on image-description data sets
please consult \cite{bernardi2016automatic}}.
.
%For our multi-lingual experiments we use the Multi30K data set \cite{elliott2016multi30k,elliott2017findings}; a multilingual extension of Flickr30K. It consists of a {\it translation} and a {\it comparable} portions both containing all images from the original Flickr30K. The \emph{translation} portion annotates a single English description with a German, French and Czech caption, while the \emph{comparable} the original 5 English and additional 5 German descriptions collected independently using the same crowd-sourcing process.

\paragraph{Our contribution}


In \citep{chrupala2015learning} we train a multi-task architecture consisting
of a shared word-embedding matrix and two pathways: 1.) common recurrent language
model trained to maximize the probability of a word given the history,
2.) grounded representation learning model predicting the image representation paired with the sentences.
This model combines the distributional hypothesis for learning the meaning of words
and the grounded learning on the sentence level.
We show that the word-representations learned by the two path architecture
predict human word-similarity judgements with higher accuracy then the one-path
architectures. Furthermore, the combined model is more sensitive to word-order than
the visual-only model.  Lastly we show that images act as an
anchor and the architecture performs paraphrase retrieval well above chance
level.

Chapter~\ref{ch:COLI} is dedicated to develop techniques to interpret the
opaque continuous representations of this model. It provides an in-depth
comparative analysis of the linguistic knowledge represented by the
language-model and visual-pathways of this architecture.

In Chapter~\ref{ch:IJCNLP} we show that in the domain of image-captions
grounded learning improves translation quality and that learning multi-modal
representations provides gains on top of learning from larger bilingual corpora.
This lead to the hypothesis that much of the observed improvements of
multi-modal translation models over text-only baselines is due to grounded
learning and not to the effective use of visual context. This was later confirmed
in CITE DESMOND where only one of the considered three state-of-the-art multi-modals
shown sensitivity to the input image.


%\section{Multilingual representations of words and sentences}\todo{this will be challenging, not sure yet how to build it up.}
%Multilingual or cross-lingual word embedding approaches map words of different languages in a shared space. These representations allow to transfer knowledge between languages and
%perform cross-lingual tasks such as cross-lingual document retrieval. The bulk of such approaches apply the techniques developed for monolingual word-embedding induction as a component
%of their full pipeline. One of the early approaches trains two separate word spaces with using the negative-sampling implementation of the skip-gram algorithm and then maps these spaces onto eachother by minimizing the squared loss between the 5000 top most frequent word-vectors \cite{mikolov2013exploiting}. This main approach went through improvents
%such as by switching mean-squared error to ranking loss \cite{lazaridou2015hubness} or enforcing orthogonality constraint on the mapping matrix
%\cite{xing2015normalized}   (WE DID THE SAME OVER TIME). Another prominent technique to learn a mapping between word (and other types of) spaces is Canonical Correlation Analysis, first
%used by \cite{faruqui2014improving}, which was later improved by the application of Deep CCA \cite{lu2015deep}. Another approach is to create a pseudo-bilingual corpora by replacing words
%in a source language by their translation and train an embedding model on the resulting corpus
%The uber multilingual word-embedding \cite{ammar2016massively}.

\section{Visually grounded multilingual representations}
\label{sec:vismulti}

On top of the visual modality anchoring linguistic representations to perceptual reality it
also provides a universal representation to bridge languages of varying typological distance.
The basic intuition being that words or sentences with similar meanings appear
within similar perceptual contexts.

\subsection{Bilingual lexicon induction}
\label{sec:bilinglex}

On word level images as a common ground have been applied to induce bilingual
lexicons without parallel corpora. The lack of bi-text in this setting have
been traditionally solved by methods relying on textual features
such as orographic similarity \citep{haghighi2008learning} or similar diachronic distributional
statistical trends between words \citep{schafer2002inducing}.
However, images tagged with various labels in a multitude of languages are
available on the internet and a model of image-similarity can be used to exploit
images as pivots to find translations between such tags. Alternatively visual representations
for words are computed through summarizing the image-features co-occurring with them
-- as discussed in Section~\ref{sec:visualwords}. This leads to representing
words in multiple languages within the same image-feature space.

The first approach to construct a dictionary based on image similarity was
developed by \cite{bergsma2011learning}. They use Google image search to find
relevant images for the names of physical objects in multiple languages.
These images are represented by BoVW vectors based
on SIFT and color features. To find word-translations given a word in the
source language and a list of possible translations in the target language
for each word Google is queried to find $n$ images per word. For each image the
feature vectors are extracted. This is followed by computing the similarity
between the feature vectors of the images corresponding to the source word and
all target words. Finally the word in the target vocabulary with the highest
similarity is chosen.

This method is vastly improved upon by a later approach applying
pre-trained convolutional neural networks to represent words
in the visual space \citep{kiela2015visual}. Their approach closely follows
the monolingual visual-word representations of \cite{kiela2014improving}:
each word is represented as the summary of CNN representations of images they
co-occur with. Given a word in the source word the candidate translation is found
simply by performing nearest neighbor search on the target vocabulary.

Exploring the limitations of image-pivoting for bilingual lexicon induction
\cite{hartmann2017limitations} present a negative result
showing that such techniques scale poorly to non-noun words such as
adjectives or verbs. However, combining image-pivot based bilingual
word-representations with more traditional multi-lingual word-embedding
techniques lead to superior performance compared to their uni-modal
counterparts \cite{vulic2016multi}. At the time of writing this thesis
the first large-scale vision based bilingual dictionary induction data set
was put forward by \citep{hewitt2018learning}. They create a data set
of \~200K words and 100 images per each using Google Image Search and perform
experiments with 32 languages.
They confirm the finding of \cite{hartmann2017limitations} that image-pivoting
is most effective for nouns, however, find that using their larger data set
adjectives can also be translated reliably. Similarly to \cite{vulic2016multi}
they also show through combining
visual and textual word-representations through a re-ranking
experiment that visual representations improve the state-of-the-art
text-only methods in the case of \emph{concrete} concepts.

\subsection{Image pivots for translation}
\label{sec:imgpivot}

Images have also been used as a pivot to aid translation of visually
descriptive sentences. In machine translation a pivot based approach is applied
when there are parallel corpora available between language pairs $A\rightarrow C$
and $C \rightarrow B$, but there is no data for $A\rightarrow B$. The problem is
solved by first translating $A$ to $C$ and then $C$ to $B$.
%A common technique in statistical machine translation
%is to apply a heuristic re-ranking algorithm
%to a sampled list of candidate translations from a translation model given the source sentence.
%\cite{hitschler2016multimodal} consider the setting of image-caption translation where an
%image-caption pair is given on the source side and on top of the parallel source-target sentence corpus
%additional target-side image-caption corpus is available. The show that using the images in the re-ranking
%pipeline improves translation performance upon using solely textual-similarity.
\cite{nakayama2017zero} apply image-pivoting in such a zero-resource machine
translation setting. The task is to translate from English to German without
aligned parallel corpora, however, separate image-description data sets are
available in both languages. They solve the problem by
training two components: 1.) visual-sentence encoder that matches images and their descriptions,
2.) image-description generator maximizing the likelihood of gold standard captions given the images.
At test time the visual-sentence encoder representation of the source sentence is fed to the
image-description generator to produce the translation.
There results were improved later by modeling
the image-pivot based zero-resource translation setup as a
multi-agent communication game between encoder and decoder
\citep{chen2018zero,lee2017emergent}.


\subsection{Multi-view perspective}
\label{sec:multiview}

Images and their descriptions in multiple languages can be taken as different views of the
same underlying semantic concepts. From this multi-view perspective learning common representations
of multiple languages and perceptual stimuli can potentially exploit the complementary information
between views to learn better representations. This also leads to practical applications such as
cross-modal and cross-lingual retrieval or similarity.
One of the angles taken by recent multi-view learning approaches to the problem
is to maximize the correlation between the representations of different views.
The deep partial canonical correlation
analysis approach learns multilingual English-German sentence embeddings conditioned on the
representation of the images they are aligned to \citep{rotman2018bridging}.
They show that their model using the visual modality as an extra view finds
better multilingual sentence and word representations as demonstrated by
cross-lingual paraphrasing and word-similarity results.
The bridge correlational neural network approach \citep{rajendran2015bridge}
learns common representations even when the different views only need to be
aligned with one pivot view. In the vision and language domain they preform
image-sentence retrieval experiments in French or German where
the image-caption data set is only available for English, however,
there is parallel corpora between German or French and English. In other words English acts as a pivot.
\cite{gella2017image} considers images as a pivot bridging English and German and train a multilingual
image-sentence ranking model -- recurrent-convolutional architecture as in Chapters~\ref{ch:COLI}, \ref{ch:IJCNLP} and \ref{ch:ConLL}.
There results suggest that the multilingual models outperform the monolingual ones on image-sentence
ranking, however, they do not show consistent gains across languages and model settings.

\paragraph{Our contribution}

In Chapter \ref{ch:ConLL} we implement a similar setup to \cite{gella2017image}
and do show that both English and German image-sentence
ranking performance is reliably improved by bilingual joint training.
We expand the results further and provide evidence that more gains can be
achieved by adding more views: on top of English and German, we
add French and Czech captions. Revisiting the issue of alignment we find that
the unlike in previous experiments \citep{gella2017image,calixto2017multilingual,rotman2018bridging}
image-pivoting leads to better visual-sentence embeddings even when the images
are not matched between languages. Finally the performance on the lower resource
French and Czech languages is improved when we add the larger
English and German image-caption sets, showing successful multilingual
transfer in the vision-language domain.

Chapter~\ref{ch:IJCNLP}, where we show that grounded learning improves the translation
of image descriptions is also related to the multi-view learning topic: we use
images as an additional view that helps us learn better sentence representations.


%\section{Grounding and other modalities}
%Grounding words in auditory signals \cite{kiela2015multi,lopopolo2015sound} and
% olfactory perception \cite{kiela2015grounding}r. Growing body of literature in learning joint representations
% of speech and images \cite{harwath2016unsupervised,chrupala2017representations,harwath2018jointly}


%Mention semantic parsing and the WebNLG paper i did with Thiago. Mention the TextWorld thing. Grzegorz
%and Afra´s work speech, plus the fail paper that im on too.

\section{Interpreting continuous representations}
\label{sec:interpret}
The word, phrase and sentence representations learned through neural architectures are notoriously opaque.
Contrary to count-based methods the features extracted by deep networks from text input
appear as arbitrary dense vectors to the human eye.
Empirical evidence shows that incorporating various views such as multiple languages and vision to
learn representations improves performance on downstream tasks.
But where does the improvement come from? What are the linguistic
regularities represented in the recurrent states that lead to the final performance metrics?
Did the model learn to exploit trivial artifacts in the training data or did it learn to generalize?
What characterizes the individual features recurrent networks extract from the input strings?
The main topic of my thesis is learning visually grounded representations for linguistic units.
For a complete picture it is crucial to assess the difference in the representations
between textual only and multimodal representations not only from a quantitative point of view,
but also from a qualitative linguistics angle.
This is what Chapter \ref{ch:COLI} is dedicated to.

Developing techniques for interpreting machine learning models have multiple goals.
From the practical point of view as learning algorithms make their way into critical applications
such as medicine humans and machines need to be able to co-operate to avoid catastrophic
outcomes \citep{caruana2015intelligible}. As such there is a growing interest in deriving methods
to \emph{explain} the decision of such architectures.
These methods assign a real-valued "relevance" score to each unit in the input signal,
which signifies how much impact it had on the final prediction of the model.
One of the first paradigms in generating such relevance scores is gradient based methods:
they take the gradient of the output of the network with respect to the input \cite{simonyan2013deep}.
Deep neural models of language tasks learn distributed representations of input symbols
and as such further operations have to applied to reduce the resulting gradient vectors to
single scalars e.g.: using $\ell_2$ norm \citep{bansal2016ask}.

Another prominent and well studied
approach still based on gradient information is layerwise
relevance propagation (LRP) \citep{bach2015pixel}. The output of the final layer
is written as the sum of the relevance-scores from the input and similarly to the back-propagation
algorithm the relevance of each neuron
recursively depends on the lower-layer all the way down to the input signal.
Different versions of LRP run the backward pass with different rules taking as
input gradient information and activation values. It was later theoretically analyzed and generalized
into the deep Taylor decomposition method \citep{binder2016layer}.
This has been shown to be equivalent to taking the
gradients with respect to the input as in \cite{simonyan2013deep} and multiplying it elementwise
with the input itself \citep{shrikumar2017learning}.
LRP was also later derived for recurrent architectures \citep{arras2017explaining} to describe
sentiment classifiers. Another backpropagation based algorithm is DeepLIFT, which instead of
using the gradients for computing the relevance scores it uses the difference between the activations
that result from a specific value compared to a baseline input.

Perturbation based methods are also gradient free: these involve generating
pseudo-samples according to some procedure and comparing measuring how the models
react differently to these samples versus the real one.
LIME \citep{ribeiro2016should} and its NLP specific extension LIMSSE
\citep{poerner2018evaluating} perturbs
the input creating a local neighborhood around it and fits interpretable linear models to explain
the predictions of any complex black box classifier.
Even simpler perturbation based techniques apply perturbations to the input and measure the
difference between the original input and the various perturbed candidates such as the erasure
\citep{li2016understanding} and our omission \citep{Kadar2016} method.

Apart from practical considerations training a complex opaque models
from close to raw input then can shed help discover patterns in the
input data that are crucial in solving the task.
Deep neural networks learn to solve tasks from close to raw input, similar to
what humans receive. As such
uncovering the regularities they learn can also shed light into the patterns
humans might extract from data to cope with certain tasks.
Recent methodology in probing the learned representations of LSTM language models,
in fact, resemble psycholinguistic studies.
A number of experiments using the agreement prediction paradigm
\citep{bock1991broken} suggest that LSTM language models successfully learn
syntactic regularities as opposed to memorizing surface patterns
\citep{linzen2016assessing,enguehard2017exploring,bernardy2017using,gulordava2018colorless}.

\paragraph{Our contribution}
The \emph{omission} method we develop in Chapter~\ref{ch:COLI} removes words
from the input while the \emph{occlusion} \citep{li2016understanding}
method replaces these inputs with a baseline.
However, our aim in Chapter~\ref{ch:COLI} was not to develop a method for explanation, but to
shed light on the linguistic characteristics of the input grounded learning
models learn in contrast to text-only language models.

\section{Published work}

\subsection{Chapters}

Each of the following Chapters was presented before as a published long-paper. These are included with
the only modification of re-aligning and re-sizing a few figures.

\begin{description}
	\item[Chapter~\ref{ch:TAL}] \bibentry{kadar2015learning}
	\item[Chapter~\ref{ch:COLI}] \bibentry{kadar2017representation}
	\item[Chapter~\ref{ch:IJCNLP}] \bibentry{elliott2017imagination}
	\item[Chapter~\ref{ch:ConLL}] \bibentry{kadar2018lessons}
	\item[Chapter ] 
\end{description}

\subsection{Other publications completed during my PhD}

Worked on some of the experiments for the visually grounded sentence representation learning
model \texttt{IMAGINET}, which formed the basis in the investigations of Chapter~\ref{ch:COLI}:

\begin{itemize}
\item \bibentry{chrupala2015learning}
\end{itemize}

Other than methods I also contributed to two corpora within the vision and language domain.
During my internship at Microsoft Research Montreal my side-project was a synthetically generated visual-reasoning
dataset named FigureQA:

\begin{itemize}
\item \bibentry{kahou2017figureqa}
\end{itemize}

The second corpus I worked on was with our sister group in the communications department and a collaborations with 
Vrije Universiteit Amsterdam:  a data set of native Dutch spoken image-description with eye-tracking recordings.
The goal of this project was to provide a large data set for the community interested in referring-expression research
with rich annotations already existing in the Visual Genome data set:

\begin{itemize}
\item \bibentry{van2018didec}
\end{itemize}

Outside of the vision and language domain I worked in a couple of other fields.
Within our research group, we published on learning unsupervised sentence represenations
from speech signals:

\item \bibentry{chrupala2018difficulty}

Furthermore, we published a paper about attemtping reproduce a 
recurrent architecture for language modeling:

\begin{itemize}
\item \bibentry{kadar2018revisiting}
\end{itemize}

I had the opportunity to work on implementing recurrent networks for
generation in the context of referring-expression generation:

\begin{itemize}
\item \bibentry{ferreira2018neuralreg}
\end{itemize}

Finally, the largest project other than my main PhD project I worked
on was during my main project during my Microsoft Research Montreal internship.
It is linear-logic programming based system for 
generating text-based adventure games to test
generalization in (deep) reinforcement learning:

\begin{itemize}
\item \bibentry{cote2018textworld}
\end{itemize}

%\section{Vision and Language applications}
%Mention the bunch of work thats here. Do image-captioning and image-sentence ranking and then metion that we do the latter always. Say the importance of attention in this field and mention the DIDEC corpus. Mention VQA and fine-grained reasoning and the FigureQA paper.



%\section{Methods}

%\section{Data sets}
%For image-feature extraction all approaches presented in the thesis use some
%CNN architecture pre-trained on a version of the ImageNet data set \cite{deng2009imagenet} prepared for a large-scale image classification challenge \cite{russakovsky2015imagenet}. ImageNet annotates the noun synsets from WordNet \cite{miller1995wordnet} with images collected from the internet. At the time of writing this thesis on average the data set consists of 500 images per node. The section used in the pre-trained networks is the ILSVRC2012 containing 1.2 million images annotated with 1000 classes. For learning grounded language representation we use data sets introduced for image-captioning. These data sets annotate images found in online resources with descriptions through crowd-sourcing. The smallest data set we use is Flickr8K \cite{hodosh2013framing} with 8000 images, which was later extended to contain 30K images resulting in the Flickr30K benchmark \cite{young2014image} and finally the largest collection we use is the COCO image-caption benchmark \cite{chen2015microsoft} with 123K images. All data sets contain 5 captions per image.
%The descriptions collected for these data sets are largely \emph{conceptual}, \emph{concrete} and \emph{generic}. This means that descriptions do not focus too much on \emph{perceptual} information such as colors, contrast or picture quality; they do not mention many \emph{abstract} notions about images such as mood and finally the descriptions are not \emph{specific} meaning that they do not mention the names of cities, people or brands of objects. What they do end up mentioning are entities depicted in the images (frizbee, dog, boy) their attributes (yellow, fluffy, young) and their relations between each other. The images depict common real-life scenes such as bus turning left or people playing soccer in the park. As such annotation collected independently from different workers end up focusing on different aspects of these scenes. For a comprehensive overview on image-description data sets please consult \cite{bernardi2016automatic}.
%For our multi-lingual experiments we use the Multi30K data set \cite{elliott2016multi30k,elliott2017findings}; a multilingual extension of Flickr30K. It consists of a {\it translation} and a {\it comparable} portions both containing all images from the original Flickr30K. The \emph{translation} portion annotates a single English description with a German, French and Czech caption, while the \emph{comparable} the original 5 English and additional 5 German descriptions collected independently using the same crowd-sourcing process.


%\section{Architectures}
%Just do the usual GRU + CNN thing. cite Jamie (heart).

%\section{Recurrent network}
%Do the GRU mention that the LSTM is strictly more powerful according to Yoav, but it doesnt seem to matter at all according to Chung.

%\section{Convolutional network}
%Just super quick run-down on VGG and ResNet.

%\section{Optimization}
%Just mention that all this shit is optimized with some version of SGD and we use Adam all the time. Mention that Adam sucks but its just some common thing to do. Say this thing about the MSE or cosine loss that we start with that but i results in hubness actually as Angeliki points it out. So we move to the contrastive kinda losses.

%\section{Transfer learning}
%Don't really worry about it too much just say something about how awesome is that the CNNs trained on image net actually end up being so useful for us.

%\section{Multi-task learning}
%Multi-task learning has been an important paradigm since the earliest works on training neural architectures for natural-language processing \cite{collobert2008unified}. Multi-task learning involves optimizing multiple loss function jointly usually with the goal of exploiting commonalities between tasks. As described in the seminal work of \cite{caruana1997multitask}:

%\begin{quote}
%Multitask Learning is an approach to inductive transfer that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias. It does this by learning tasks in parallel while using a shared representation; what is learned for each task can help other tasks be learned better.
%\end{quote}

%Introducing inductive bias using different tasks means that the learning algorithm prefers a specific set of hypotheses over others just as in the case of other forms of regularization such as $\ell_1$ or $\ell_2$ penalties. Here I only discuss the specific instantiation of the mutli-task learning paradigm that are applicable to the chapters presented in this thesis and for a more comprehensive overview please consult \cite{ruder2017overview}. The technique used in the thesis
%is hard-parameter sharing \cite{caruana1997multitask}
