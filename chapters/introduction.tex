%!TEX root = ../dissertation.tex

%EK: edits November 28

\chapter{Introduction}
\label{ch:introduction}

The ability to understand natural language plays a central role in humans' conception of intelligent machines.
Alan Turing already in his now famous Imitation Game \citep{machinery1950computing} 
gives natural language understanding a
key role in tricking humans into thinking that they are interacting with their fellow specimen rather
than a machine. One of the goals of Natural Language Processing (NLP) is to develop algorithms
and build systems to help machines understand what humans are talking about; to
understand the \emph{meaning} of natural language utterances.

In the first part of the thesis I explore computational techniques to
\emph{learn the meaning of words and  sentences} 
considering the \emph{visual world} as a naturally occurring
\emph{meaning representation}. Furthermore, I consider images
as a means to \emph{bridge} between languages and 
present methods seeking to find relationships between \emph{images} and
\emph{natural utterances} in \emph{multiple languages}.


The  Chapters of the thesis follow a progression
starting with a single language at the word-level and arriving to multilingual visually grounded sentence
representations:

\begin{description}
\item[Chapter~\ref{ch:introduction}] introduces the topic and contributions of
the thesis.

\item[Chapter~\ref{ch:background}] discusses the related work and technical background in detail.

\item[Chapter~\ref{ch:TAL}] presents
a cognitive model of language learning that learns \emph{visually grounded word representations}.

\item[Chapter~\ref{ch:COLI} ] focuses on \emph{visually grounded sentence representations} and their
interpretations from a linguistic perspective using the architecture that is the basis
for the chapters to follow: combination of a Convolutional Neural Network to extract visual features and
a Recurrent Neural Network to learn sentence embeddings.

\item[Chapter~\ref{ch:IJCNLP}] applies visually grounded representation learning approach that forms the
basis of Chapter~\ref{ch:COLI} to \emph{improve machine translation} in the domain of visually descriptive language.

\item[Chapter~\ref{ch:ConLL}] shows the clear benefits of learning visually grounded representations
for multiple languages jointly.

\item[Chapter~\ref{ch:EMNLP}] extends the investigations of Chapter~\ref{ch:ConLL} to the
cross-domain setup, removing the assumption that for each language the same images are annotated
with different languages.
\end{description}



\section{Learning representations}
The foundational methodology applied in all chapters is \emph{statistical learning}.
The early days of NLP were characterized by rule-based systems building on such
foundations as Chomskyan theories of grammar \citep{chomsky2002syntactic} or 
Montague Semantics \citep{montague1970english}.
Since the 1980s partly due to such theories falling out of fashion, 
but also due to the increase in the amount
of available computational power Machine Learning (ML) approaches revolutionized
the field.  \emph{Learning} in general proved to be a crucial component to Artificial Intelligence and also
specifically in NLP. Machine Learning algorithms are designed with the goal that given an increasing
number of examples a system improves its performance according to some measure of success.
Reflecting the structure of ML itself and the popularity of ML within the field, NLP research follows
a task-oriented methodology: researchers borrow or collect data sets, define measures of success and develop or
apply learning algorithms. Chapters~\ref{ch:IJCNLP}, \ref{ch:ConLL} and
\ref{ch:EMNLP}  closely follow this blueprint.

From the rule based times of ``engineering grammars" researchers moved onto ``engineering features"
to \emph{represent} the textual data as input to general-purpose pattern recognition algorithms such as
decision trees, support-vector machines or conditional random fields.
A large set of these feature templates are still based on various formal-linguistic theories requiring various
linguistic taggers and parsers to assign structure to raw texts.
Intuitively, different applications such as machine translation or goal-oriented dialogue systems
require different input representations. Furthermore, one would assume that various languages require
different feature-extraction pipelines reflecting the typological differences across languages.

\emph{Linguistic representation learning} challenges this intuition and is interested in
discovering general principles that allows machines to  \emph{learn}
linguistic representations from \emph{raw data}, which are more or less
generally applicable. This line of work, as well as the approaches presented in the thesis,
fit in the general \emph{representation learning}  framework consisting of
machine learning approaches that learn useful representations
for various tasks from (close to) raw input.

The expression ``representation learning" is somewhat synonymous with ``deep learning" at the time
of writing this thesis \citep{bengio2013representation}.
When mentioning representation learning in the deep learning context
it is usually meant that the goal is to learn a function from raw input to target labels. In the
context of this thesis, however, the emphasis is on learning representations of words, phrases and
sentences that are potentially \emph{generally useful}, meaning
that they can be used as input to many tasks. This is sometimes referred to as
\emph{transfer learning} \citep{pratt1993discriminability} 
where we seek to identify unsupervised learning objectives,
supervised tasks, self-supervision schemes or the combinations of these to  learn representations that
perform well on a large variety of problems.

\section{Learning representations of words}

%\begin{quote}
%Is there a way, however, to represent text in a more general way?
%\end{quote}

Most attempts to build  general representations for words are based on the
\emph{distributional hypothesis} of word meaning. It states that the degree to which words are similar
is a function of the similarity of the linguistic context they appear in. In other words, similar
words appear in similar contexts. Computational models of distributional
semantics implement this intuition and generate real-valued word vectors based on co-occurrence
statistics in large text corpora. 
To aid the reader with technical and historical context we introduce distributional
semantics models using the \emph{count-based}/\emph{prediction-based} distinction
borrowed from \cite{baroni2014don}.
Section~\ref{sec:count} introduces earlier \emph{count-based}
methods building word-context co-occurrence vector-space, while
Section~\ref{sec:pred} presents the  \emph{prediction-based} framework in a more
detailed fashion as the techniques discussed here are closely related
to the approaches presented in this thesis.
Section~\ref{sec:w2v} details efficient linear
models for predictive word-learning for two main reasons: 1.) linear word-learning methods
had a tremendous impact
on shaping the current landscape of continuous linguistic representation learning and are still
widely used at the time of writing this thesis, 2.) our main point of comparison for
our word learning model in Chapter~\ref{ch:TAL} is one of such models detailed in the section.

Word-representations within the prediction-based framework 
are an instance of \emph{representation learning}:
word representations -- usually referred to as \emph{word embeddings} -- are learned through
optimizing model parameters to predict context from words or words from context.
Such learned word-representations have proven successful in many applications especially
in recent years, however, they are not \emph{realistic} in a certain sense.
While they capture many aspects of syntax and semantics of natural language they 
are not connected to the real world outside of the large collections of texts.
This leads us to the main topic of the thesis:  \emph{visual grounding} introduced in the
following section.

\section{Visually grounded word representations}

%\begin{quote}
%Can we learn useful linguistic representations just by reading texts?
%\end{quote}

Many theories of human cognition supported by empirical
evidence state that human language and concept representation and acquisition is \emph{grounded}
in perceptual and sensori-motor experiences. Cross-situational word learning,
an influential cognitive account of human word learning, supposes that humans learn the
meanings of words exploiting repeated exposure to linguistic contexts paired with perceptual reality.
Learning representations for linguistic units in a visually grounded manner brings
computational language learning systems closer to human-like learning.
Such theoretical considerations are detailed in Section~\ref{sec:langperc}.

Let us also consider the practical applicability of distributional language representations in the larger scope of Artificial
Intelligence. One of the dreams of AI is to develop technology to power intelligent embodied agents
taking the form of office assistants or emergency aid robots. These machines cannot implement natural
language as an arbitrary symbol manipulation system akin to a calculator's understanding of
magnitudes or slopes.
Similarly to humans they need to link linguistic knowledge to the extra-linguistic world.

Furthermore, while certain aspects of meaning  such as \emph{encyclopedic}
knowledge are abudant textual data, \emph{perceptual} information
can provide complementary valuable insights into physical properties rarely mentioned 
in texts such as \emph{size}, \emph{shape} and \emph{color}. 
In practice harnessing the visual modality to learn language
representations that link linguistic knowledge
to the external world  has been empirically shown to improve performance on several semantic tasks as
detailed in Section~\ref{sec:distvis}.

%Together the aforementioned cognitive
%and practical considerations drove many works, including the Chapters of the present thesis,
%to explore grounded linguistic representations.

In terms of computational modeling the jump from
distributional to grounded models is conceptually simple: one needs to collect data where
the \emph{contexts} of linguistic units are \emph{extra-linguistic} and represent these contexts such
that they can be provided as input to representation learning algorithms.
More concretely in terms of extra-linguistic context the present thesis focuses on the \emph{visual modality}.

Linguistic-visual multi-modal representations on the word level have a well established albeit
somewhat brief history (Section~\ref{sec:distvis}).
Such methods were developed both within the \emph{count-based}
and \emph{prediction-based} frameworks using computer vision techniques to represent the
\emph{visual modality} and NLP methods to represent texts.
These separate spaces are then combined into a single multi-modal representation.

As the \textbf{first contribution} of the thesis in Chapter~\ref{ch:TAL} we present an
incremental cross-situational model of word learning introducing modern computer-vision
techniques to computational cross-situational modeling of  human language learning.
Through our experiments we show that our presented model is competitive with state-of-the-art
\emph{prediction-based} distributional models and that our model can name
relevant concepts given images.

\section{Visually grounded sentence representations}

When moving from \emph{atomic} words to the \emph{compositional} world of sentences we need flexible
models that can represent word-order and hierarchical relationships.  In Chapters~\ref{ch:COLI}, \ref{ch:IJCNLP},  \ref{ch:ConLL} and \ref{ch:EMNLP}
we use Recurrent Neural Networks, which form a powerful class of  sequence models to represent sentences.
Section~\ref{sec:sentences} provides the reader with
historical and technical background to the considerations behind this choice.

The study of general sentence representation learning has a much briefer history than word-representations
and Section~\ref{sec:trans-sentence} situates the reader in the area. 
Most approaches to learn useful sentence representations to date are
based also on the distributional hypothesis and formulate general purpose representation learning as a sort of
linguistic context prediction, but on the sentence level.

Section~\ref{sec:visualsentences} describes the general framework of learning
visually-grounded sentence representations and their utility.
The basic idea is still context prediction, however, we learn associations between
sentences and their \emph{visual} context i.e.\ model parameters are optimized such
that related image sentence pairs get pushed close together and unrelated pairs far
from each other in a learned joint space.

As the \textbf{second contribution} of this thesis in Chapter~\ref{ch:COLI}
we train such an architecture and explore the learned representations.
Our main interest and contribution here is the development of general techniques to
\emph{interpret} linguistic representations learned by
Recurrent Neural Networks and use these techniques to contrast text-only language
models with their grounded counterparts trained on the same sentences.

\section{Visual modality bridging between languages}

One of the intriguing aspects of using the visual modality as a naturally occurring
meaning representation is that it is also naturally \emph{universal} across languages.
The visual modality anchors linguistic representations to perceptual reality, but also
provides a natural bridge between various languages. Linguistic utterances that are similar
to each other, intuitively, appear in the context of perceptually similar scenes across languages.

Utterances in multiple languages and corresponding perceptual stimuli
can be conceptualized as \emph{multiple views} of the same underlying abstract object.
Learning to map these multiple views to the same feature space can lead to better representations as
they have to be more \emph{general} due to the model having to solve multiple tasks at the same time.
This \emph{multi-view learning} perspective is explained in more detail in Section~\ref{sec:multiview}
focusing on the specific case of multi-modal and multi-lingual representations we explore in
Chapter~\ref{ch:ConLL} and \ref{ch:EMNLP}.

The visual modality as pivot on the word-level can be used to find possible translations
for words when no dictionary is available. Extending this idea from word to sentence level gives rise to techniques that
use the visual modality as a pivot to translate full sentences. 
Approaches in this direction are discussed in Section~\ref{sec:imgpivot}.

The \textbf{third contribution} in the thesis combines visually grounded sentence representation learning
with machine translation. More specifically in Chapter~\ref{ch:IJCNLP} we present a \emph{multi-task}
learning architecture that jointly learns to associate English sentences with images and to translate from
English to German. We show that visually grounded learning improves translation quality in our domain and that
it provides orthogonal improvements to having a large additional English-German parallel corpus.

The \textbf{fourth contribution} of this thesis is exploring visually grounded 
sentence representations learned for multiple languages jointly.  
In Chapter~\ref{ch:ConLL} we show that better grounded
representations can be learned by training on multiple languages. We find a consistent pattern
of improvement whereby multilingual visually grounded sentence representations 
outperform bilingual ones, which outperform monolingual representations. 
Furthermore, we provide empirical evidence that the quality of
visually grounded sentence embeddings on lower-resource languages can be improved by jointly training
together with data sets from higher-resource languages.

Lastly, our \textbf{fifth contribution} in Chapter~\ref{ch:EMNLP} is exploring the benefit of multilinguality
in visually grounded representation learning as in Chapter~\ref{ch:ConLL}, but in the cross-domain setting.
Here we consider a \emph{disjoint} scenario where the image-sentence data sets for different languages do
not share images. We assess how the method applied in Chapter~\ref{ch:ConLL} performs under
domain-shift. Furthermore, we introduce a technique we call \emph{pseudopairs}, whereby we generate
new image--caption data sets by creating pairs across data sets using the sentence similarities under
the learned representations. We find that even though this technique does not require any additional
external data source, models or other pipeline elements, 
it consistently improves image sentence ranking performance.


\section{Published work}

\subsection{Chapters}

Each of the following Chapters has been previously published. 
They are included with the only modification of re-aligning and re-sizing a few figures.

\begin{description}
	\item[Chapter~\ref{ch:TAL}] \bibentry{kadar2015learning}
	\item[Chapter~\ref{ch:COLI}] \bibentry{kadar2017representation}
	\item[Chapter~\ref{ch:IJCNLP}] \bibentry{elliott2017imagination}
	\item[Chapter~\ref{ch:ConLL}] \bibentry{kadar2018lessons}
\end{description}

At the time of completing the thesis Chapter~\ref{ch:EMNLP} has been submitted to 
the 2019 Conference on Empirical Methods in Natural Language Processing without modifications.

\subsection{Publications completed during the PhD}
These publications were completed during my PhD work, but have not been included in the thesis.

\subsubsection{Publications on Vision and Language}

%Worked on some of the experiments for the visually grounded sentence representation learning
%model \texttt{IMAGINET}, which formed the basis in the investigations of Chapter~\ref{ch:COLI}:

\begin{itemize}
\item \bibentry{chrupala2015learning}
\item \bibentry{kadar2015lingusitic}
\item \bibentry{kahou2017figureqa}
\item \bibentry{van2018didec}
\end{itemize}

%Other than methods I also contributed to two corpora within the vision and language domain.
%During my internship at Microsoft Research Montreal my side-project was a synthetically generated visual-reasoning
%dataset named FigureQA:


%The second corpus I worked on was with our sister group in the communications department and a collaboration with
%Vrije Universiteit Amsterdam:  a data set of native Dutch spoken image-description with eye-tracking recordings.
%The goal of this project was to provide a large data set for the community interested in referring-expression research with rich annotations already existing in the Visual Genome data set:


\subsubsection{Publications on other topics}


%Outside of the vision and language domain I had the chance to explore other related topics.
%Within our research group, we published on learning unsupervised sentence represenations
%from speech:


%Furthermore, we published a paper about attemtping reproduce a
%recurrent architecture for language modeling:

\begin{itemize}
\item \bibentry{chrupala2018difficulty}
\item \bibentry{kadar2018revisiting}
\item \bibentry{ferreira2018neuralreg}
\item \bibentry{cote2018textworld}
\item \bibentry{manjavacas2019improving}
\end{itemize}

%I had the opportunity to work on implementing recurrent networks for
%generation in the context of referring-expression generation:



%Finally, the largest project other than my main PhD project I worked
%on was during my main project during my Microsoft Research Montreal internship.
%It is linear-logic programming based system for
%generating text-based adventure games to test
%generalization in (deep) reinforcement learning:



\begin{comment}
\section{Background: Distributed linguistic representations}
\label{background}
As opposed to the continuous nature of the visual world, linguistic units
are inherently discrete. This is reflected in the techniques used in the
computational processing of natural language.
Information in linguistic utterances is traditionally encoded
in sparse high-dimensional count vectors,  each
dimension corresponding to the number of times a specific feature occurs in a
sample. For each word in a sentence common features to consider take the form of:
``this token is a noun'', ``the preceding word is cat'',
``the following token has dependency label direct object''.
More often then not these vectors do not even hold counts, but 0-1
indicator features signaling the presence or absence of a particular property.

In such a setting the relationships between features are not handled.
As a result machine learning algorithms are presented with an input representation where
``the preceding word is cat'' and ``the preceding word is dog''
are completely independent properties.
Furthermore, interactions between features has to be
explicitly represented as a separate entry in the vector, which once again has
no clear relationship to its parts as far as the learning algorithms are concerned.

Let us consider for a simple example the problem of language modeling.
Our  aim is to find a probabilistic model
that assigns high probability to likely sequences and low probability to gibberish.
The representation of words and phrases in common n-gram language models is on the
very extreme of discrete: they are represented by their \emph{identity}.
Imagine that in the training set there were a number of sentences expressing the concept
of ``someone is walking a dog'', a little less mentions of ``someone walking a cat''
and no occurrence of ``someone walking a serval'', but the word ``serval'' did occur.
I have never seen anyone walking a serval, however, as a human I do know that it is a dog-sized
feline creature and as a consequence I can imagine someone taking a walk with one.
However, a language model trained with discrete symbolic representation cannot
\emph{generalize} in such a way as the
relationship between the words ``dog'', ``cat'' and ``serval'' is not represented.

Consider now the case of walking a dolphin.
Both ``walking a serval'' and ``walking a dolphin'' would be impossible
by the hypothesized language model, however, us humans would say that walking the serval is
kind of possible, but walking the dolphin is just surreal.
We know it from reading about both animals that they can be domesticated
, however, by their physical properties while one affords to be walked
the other does not. Dogs, cats, servals and dolphins are similar in some
dimensions and different in others.
What the present thesis is concerned about is
\emph{learning continuous representations} of words, phrases up to sentences
from multiple sources of information to encode such useful properties.
Such representations allow machine learning algorithms to represent smooth
functions over linguistic inputs where the local neighborhoods of linguistic
units correspond to syntactic and semantic regularities.

\section{Grounded and multilingual representations}
The well established tradition of continuous word representations consider
features extracted from large text-corpora in a single language focusing on
co-occurrence statistics between words.
This distributional approach to word meaning hypothesizes that semantically related words
tend to appear in similar contexts. This idea goes back in linguistics tradition to the
earlier days of American structuralism \citep{nevin2002legacy}. In the seminal paper
"Distributional structure" \citep{harris1954distributional} back in 1954 Harris already claims
that distributional properties of a word should be taken as
evidences for its meaning and that similarity classes
can be constructed based on co-occurrence statistics.
\cite{cruse1986lexical} writes that ``It is  assumed  that  the  semantic properties  of
a lexical  item  are  fully  reflected  in  the  appropriate  aspects  of  the  relations
it  contracts  with  actual  and  potential  contexts'' and that ``the  meaning  of  a word
is constituted  by  its  contextual  relations''. Computational models of distributional
semantics implement this intuition and learn real-valued word vectors based on co-occurrence
statistics in large text corpora. Section~\ref{sec:words} presents
such representations based on the \emph{count-based} in Section~\ref{sec:count}
versus \emph{prediction-based} in Section~\ref{sec:pred}
distinction borrowed from \cite{baroni2014don}. Most architectures presented
in this thesis are based on the developements in Neural Language Models detailed
in Section~\ref{sec:NNLM}. Here we present the methods focusing on the historical context,
but combined with some standard notation
providing background for later chapters. Section~\ref{sec:w2v} introduces efficient-linear
models for predictive word-learning for two main reasons: 1.) they a had a tremendous impact
on shaping the current landscape of continuous linguistic representation learning and are still
heavily used at the time of writing this thesis, 2.) our main point of comparison between
our word learning model in Chapter~\ref{ch:TAL} is one of such models detailed in the section.


My thesis focuses integrating the \emph{visual modality} and jointly learning representations
of words up to sentences in \emph{multiple languages}. Furthermore, I study the beneficial interaction of
visually grounded and multilingual representation learning.
Grounding linguistic representattions in visual modality is largely motivated by evidence from
perceptual grounding in human concept acquisition and representation \citep{barsalou2003grounding}.
As such it brings computational language learning systems closer to human-like learning.
Furthermore, as pointed out by the dolphin versus serval example in the Preface
certain aspects of meaning are better captured by \emph{encyclopedic}
knowledge such as domestication abundant in textual data, however, perceptual information
can provide complementary valuable insights into physical properties rarely mentioned in texts such as size,
shape and color. In practice harnessing the visual modality to learn language
representations that link linguistic knowledge
to the external world \citep{kiela2014improving,baroni2016grounding,elliott2017imagination,kiela2017learning,yoo2017improving}
has been empirically shown to improve performance on several semantic tasks.

\paragraph{Images bridging languages}

Traditionally, given a task for each language separate corpora are created and
separate sets of parameters are learned.
Learning shared cross-lingual representations, however, allows researchers
and practitioners to train a single model allowing to transfer knowledge
between languages leading to practical consequences
such as mitigating the low-resource problem, cross-lingual applications and
processing data with code-mixing.
Perceptual grounding and multilingual representations have their separate benefits,
however, they can be beneficial to each other as well.
In my work I view images as universal meaning representations, a natural common
ground bridging between languages. From another perspective images and multiple
languages can be seen as multiple latent views of the same underlying semantic
concepts. This approach allows us to take advantage of complementary information
and potential transfer between these views.

\paragraph{Contributions}
In Chapter~\ref{ch:TAL} we start by learning visual representations for words using a
novel computational cognitive model of cross-situational word learning that
takes words and high-level continuous image representations as input. Our approach
integrates recent advances of computer vision into incremental cognitive models
of language learning. We show that on the data sets that we consider our model
is competitive with state-of-the-art distributional semantics models (word2vec)
on word-similarity benchmarks. Furthermore we show that our model is able to
name relevant concepts given images.

Chapter~\ref{ch:COLI} introduces a recurrent and convolutional neural network
based model that learns from both visual-grounding signals and
word-word co-occurrences. We develop techniques to interpret the learned
representations of such an architecture and investigate if certain linguistic
phenomena is encoded in the learned model.

Chapter~\ref{ch:IJCNLP} provides empirical evidence that grounded learning
can improve machine translation quality.
In Chapter~\ref{ch:ConLL} we show under what conditions multilinguality an help improve grounded
representations.
%Multilingual systems also offer a fertile ground computational typology as shared models are forced to exploit cross-lingual regularities.

%My work focuses on learning grounded distributed sentence representations.
%The first half of the thesis is about learning the representations of words and followed-up by learning sentence embeddings for a single language. Later we move on to learning multilingual visually grounded representations. In other works of mine I contributed to generating referring expressions grounded in Wikipedia entities and learning language through interacting with text-games grounded in a knowledge-base.

\end{comment}

\chapter{Background}
\label{ch:background}

\section{Distributed word-representations}
\label{sec:words}
The distributional approach to word meaning hypothesizes that semantically related words
tend to appear in similar contexts. This idea goes back in linguistics tradition to the
earlier days of American structuralism \citep{nevin2002legacy}.
As early as in 1954 in his seminal paper "Distributional structure" Harris claims
that distribution should be taken as an explanation for word meaning and that similarity classes
can be constructed based on co-occurrence statistics \citep{harris1954distributional}.
\cite{cruse1986lexical} write that "It is  assumed  that  the  semantic properties  of
a lexical  item  are  fully  reflected  in  the  appropriate  aspects  of  the  relations
it  contracts  with  actual  and  potential  contexts" and that "the  meaning  of  a word
is constituted  by  its  contextual  relations". Computational models of distributional
semantics implement this intuition and generate real-valued word vectors based on co-occurrence
statistics in large text corpora. Here we present such representations using the \emph{count-based} 
and \emph{prediction-based} distinction borrowed from \cite{baroni2014don}.
%One of the first computational verification
%attempts of  this distributional hypothesis was put forward in \cite{miller1991contextual} \todo{say something about the paper man}.

\subsection{Count-based approaches}
\label{sec:count}

Early computational linguistics models of distributional semantics fall in the category of
count-based approaches: they store the number of times target words appear in different
contexts. In the resulting co-occurrence matrix
each row corresponds to a
word and each column to a context. Each cell in the matrix is the number of times a word 
appears in a particular context.
The size of the co-occurrence matrix is then vocabulary size by the number of contexts.

Contexts are typically words appearing within a certain window size or text documents.
To the counts in the co-occurrence matrix various re-weighting schemes are applied followed 
by matrix factorization, resulting in a lower dimensional dense representation.
%Both re-weighting and low-rank approximation reduces the dimensionality of the sparse matrix and leads
%to faster compute and better generalization.

The earliest approaches include Hyperspace Analogue to Language \citep{lund1996producing},
which constructs a term-term co-occurrence matrix and Latent Semantic Analysis \citep{dumais2004latent},
which applies the tf-idf re-weighting scheme on a term-document matrix
followed by singular value decomposition.
More recent approaches apply different re-weighting schemes such as
point-wise mutual information \citep{bullinaria2007extracting}  and local mutual
information \citep{evert2005statistics} or different matrix factorization algorithms such as non-negative
matrix factorization \citep{baroni2014don}. For a comprehensive set of empirical experiments on count-based
approaches consult \cite{bullinaria2007extracting} and \cite{bullinaria2012extracting}.

\subsection{Prediction-based approaches}
\label{sec:pred}
In more recent years
%-- and more related to the present thesis --
various deep learning methods have been applied to learn distributed word representations usually referred to
as \emph{word embeddings} in the literature.
Contrary to count-based methods prediction-based approaches fit into the
standard machine learning pipeline: they optimize a set of parameters to maximize the probability of words
given contexts or contexts given words, where the word-embeddings 
themselves form a subset of the parameters of the full model.

\subsubsection{Neural language models}
\label{sec:NNLM}
Laying down the framework for recent developments the first modern approach to learn
distributed word-representations from realistic data
was the neural language model introduced by \cite{bengio2003neural}.
They present a feed-forward multilayer perceptron with continuous word-embeddings,
a single hidden layer and a softmax output layer.
More precisely the model is parametrized by a 1.) word-embedding matrix, 
whose rows correspond to word-vectors
and columns to a learned feature, 2.)
hidden and output weight-matrices. 
The network takes as input the concatenation of $n$ word-vectors preceding the target word as context
representation and outputs the probability distribution over the current word.
The model is trained to maximize the probability of
the target word given the previous fixed number of words as context over a training corpus
-- $n$-gram language model -- trained with stochastic gradient descent \citep{cauchy1847methode}
through the backpropagation algorithm \citep{rumelhart1985learning}.
Shortly after its publication the neural probabilistic feed-forward neural language model of  \cite{bengio2003neural} 
was shown to improve performance in speech recognition \citep{schwenk2005training}.

%where tanh is the hyperbolic tangent function applied elementwise

%\begin{equation}
%\label{eq:tanh}
%\text{tanh(x)} = \frac{e^{2x} - 1 }{e^{2x} + 1 }
%\end{equation}

%and softmax is a commonly used function to map a vector of unnormalized scores to a
%categorical probability distribution:

%\begin{equation}
%softmax(\mathbf{x}) = \frac{e^{x_{i}}}{\sum_{k=1} e^{x_k}}
%\end{equation}
Following a similar recipe, the convolutional architecture of \cite{collobert2008unified}
based on the time-delay neural network model \citep{waibel1990phoneme} took several steps towards the
by now standard practices in deep NLP.
Contrary to the feed-forward network the convolution over-time structure can handle
sequences of variable length. This is essential for NLP applications where typically sentences
are composed of a varying number of words.
\cite{collobert2008unified} introduce the idea of jointly learning many linguistic tasks at the same time such as part-of-speech
tagging, chunking, named entity recognition and semantic role labeling through \emph{multi-task learning} 
\citep{caruana1997multitask}.
Their architecture was later refined in \cite{collobert2011natural} and the pre-trained
full model was made available alongside the standalone word-embeddings.
Finally, \cite{collobert2008unified} were the first to show the utility of pre-trained word-embeddings
in other tasks through transfer learning.

In this thesis we make extensive use of the  multi-task learning strategy: 
In Chapters~\ref{ch:COLI} we apply it to language modeling and image-sentence ranking, 
in Chapter~\ref{ch:IJCNLP} for machine translation
and image-sentence ranking, while in Chapters~\ref{ch:ConLL} and \ref{ch:EMNLP} for image-sentence
and sentence-sentence ranking in multiple languages.

The arguably most popular architecture in NLP, however, which we also use in
Chapters~\ref{ch:COLI}, \ref{ch:IJCNLP}, \ref{ch:ConLL} and \ref{ch:EMNLP}
is the recurrent neural network (RNN). It was
first introduced by the late Jeffrey L. Elman in his seminal paper  ``Finding Structure in Time''
as a model of human language learning and processing \citep{elman1990finding}. Recurrent neural networks
take a variable length sequence as input and at every time-step they compute their next state
based on the previous state and the current input. Recurrent networks 
essentially ``read'' the input left-to-right and keeps track of the context when encoutering a new input.
Equation~\ref{eq:rec} provides the recursive definition of the computation in a RNN language model:

\begin{align}
\label{eq:rec}
P(w_t|w_{<t}) &= \text{softmax}(\mathbf{U} \mathbf{h_t} + \mathbf{b_o}) \\
\mathbf{h_t} &= \text{tanh}(\mathbf{W_h}\mathbf{h_{t-1}} + \mathbf{W_i}\mathbf{w_t} + \mathbf{b_h})
\end{align}

In case of language modeling at each time-step $t$ the network takes an input word vector $\mathbf{w_t}$
and its previous hidden state $\mathbf{h_{t-1}}$ maintained through previous
time-steps. These are used to compute the current
state $\mathbf{h_t}$ and to predict the probability distribution over the following word $P(w_t|w_{<t})$.
It is parametrized by a word-embedding matrix $\mathbf{W} \in \mathbb{R}^{|V| \times d}$,
an input-to-hidden weight matrix $\mathbf{W_i}$, a hidden-to-hidden weight
matrix $\mathbf{W_h}$ and finally the hidden-to-output
weight matrix $\mathbf{U}$ to predict the unnormalized probabilities over the vocabulary entries and additional
hidden and output bias terms $ \mathbf{b_o}$ and $ \mathbf{b_h}$.
This model is trained to maximize the probability of the training sequences, 
with the backpropagation through time algorithm (BPTT)
\citep{robinson1987utility,werbos1988generalization,williams1995gradient}.

\cite{elman1991distributed} shows when trained on simple natural language-like input
hidden states of the network $\mathbf{h_t}$ encode grammatical relations and hierarchical
constituent structure. In Chapter~\ref{ch:COLI} we also train a recurrent language model
and compare it to its grounded counterpart
on real-world data and explore similar questions about the learned opaque representations
as \cite{elman1991distributed}.

Despite the early successes, however, it turned out to be difficult to train
Elman networks on practical applications with longer sequences
due to the vanishing and exploding gradient phenomena \citep{bengio1994learning}.
The RNNLM implementation of \cite{mikolov2010recurrent}, however,
established a new state-of-the-art on language modeling and RNNs regained their popularity
in language processing.  At the time of writing this thesis RNNs remain a widely used in language processing
and are still trained with BPTT with various versions of stochastic gradient decent and smart initialization
strategies. 

While more complex training algorithms such as hessian-free optimizers
\citep{martens2011learning} remain out of fashion,
an overwhelming amount of empirical evidence
shows that the more complex long short-term
memory (LSTM) \citep{hochreiter1997long,gers1999learning}
and gated recurrent unit networks (GRU) \citep{Cho2014}  recurrent network variants
vastly outperform the simple Elman network in practice. We opted for GRUs in
Chapters~\ref{ch:COLI}, \ref{ch:IJCNLP}, \ref{ch:ConLL} and \ref{ch:EMNLP}.

%Interestingly, the successful
%application the standard BPTT algorithm
%to simple RNNs on real world language modeling \cite{mikolov2012statistical}
%seems  only to have required more computational resources and
%the now widely used gradient clipping trick. 



%Gated recurrent unit networks use a particular \emph{memory structure}, which
%adds an inductive bias to the network to make it easier to learn to \emph{carry information}
%over time by keeping a portion of the components of the hidden state $\mathbf{h_t}$
%approximately constant at each step.
%Intuitively GRUs can be understood as a sort of
%sequential computer with soft/continuous read-write memory operations: the reset-gate $\mathbf{r_t}$
%decides how much of each component of the previous state is relevant to be mixed in with the
%current input, resulting in the current candidate memory state $\mathbf{\tilde{h_t}}$. The output-gate
%then trades off the previous state with the current candidate.


%\begin{align}
%\tag{update-gate}
%\mathbf{z_t} &= \sigma(\mathbf{W_z} \mathbf{w_t} + \mathbf{U_z} \mathbf{h_{t-1}} + \mathbf{b_z}) \\
%\tag{reset-gate}
%\mathbf{r_t} &= \sigma(\mathbf{W_r} \mathbf{w_t} + \mathbf{U_r} \mathbf{h_{t-1}} + \mathbf{b_r})  \\
%\tag{memory content}
%\mathbf{\tilde{h_t}} &= \text{tanh}(\mathbf{W_h} \mathbf{w_t} + \mathbf{r_t} \odot  \mathbf{U_h} \mathbf{h_{t-1}}) \\
%\tag{hidden state}
%\mathbf{h_t} &= (1-\mathbf{z_t}) \odot \mathbf{h_{t-1}}  \mathbf{z_t} \odot \mathbf{\tilde{h_t}}
%\end{align}

%where $\sigma$ is the sigmoid function:

%\begin{equation}
%\sigma(x) = \frac{e^x}{e^x + 1}
%\end{equation}



\subsubsection{Efficient linear models}
\label{sec:w2v}
Despite the success of deep learning architectures in NLP it wasn't until the introduction
of the much simpler and fast continuous bag-of-words
and skip-gram with negative sampling (SGNS) algorithms \cite{mikolov2013efficient}
packaged into the easy-to-use \texttt{word2vec}
toolkit that
word-embeddings became ubiquitous in computational linguistics and NLP research.
These algorithms rely on simple log-linear models as opposed to the more expensive
neural networks leading to faster training on larger corpora. The more successful SGNS
model has two learnable word embedding matrices one for the target words
and a separate one for the contexts.
The model is trained to maximize the dot product
between true word-context pairs appearing in corpus and minimize the
dot product between randomly sampled contrastive examples. 

%\begin{align}
%J(\theta) &= \prod_{(w,c) \in D} p(D=1|w,c;\theta)  \prod_{(w,c) \in D'}  p(D=0|w,c;\theta) \\
%&= \sum_{(w,c) \in D} \log  \sigma(\mathbf{c}^T\mathbf{w}) \sum_{(w,c) \in D'}  \log \sigma(-\mathbf{c}^T\mathbf{w})
%\end{align}

%where  $p(D=1|w,c)$ is the probability that $(w,c)$ is from the true corpus, $p(D=0|w,c)$ is the
%probability that it did not and $D'$ is the artificial corpus of negative pairs.
Similarly to the negative sampling algorithm of  \cite{mikolov2013efficient} the ranking objectives
implemented in Chapters~\ref{ch:IJCNLP}, \ref{ch:ConLL} and ~\ref{ch:EMNLP}
force the models to push images and corresponding sentences close and contrastive examples
far from each other in the learned multi-modal space.

The GloVe approach \citep{pennington2014glove} is another popular simple linear model
with word  and context embeddings, which
represents a hybrid between count- and prediction-based techniques:
it optimizes word-embeddings to predict the re-weighted
$\log$ co-occurrence counts collected from large text corpora.

%\begin{equation}
%\label{eq:glove}
%\mathbf{w}^T\mathbf{c} + b_w + b_c = \log(\#(w,c))\;\;\; \forall (w,c \in D)
%\end{equation}

%where $b_w$ and $b_c$ are context specific bias terms.
%Now let us express the Glove model in the traditional re-weighted
%co-occurrence matrix factorization form.
%Let the matrix $M \in \mathbb{R}^{|V| \times |C|}$ store the log-weighted
%counts and $\mathbf{b_w}$ and $\mathbf{b_c}$ be the vectors of all
%$b_w$ and $b_c$ scalars, then

%\begin{equation}
%\label{eq:glove2}
%M \approx \mathbf{W} \cdot \mathbf{C^T} + \mathbf{b_w} + \mathbf{b_c}
%\end{equation}

There has been work on finding relationships between
count- and prediction-based methods \citep{levy2014neural} and
using insights from both to develop novel improved
variants \citep{levy2015improving}.

%\paragraph{Transferring word representations}

%Later the word-embeddings $\mathbf{W}$ learned by recurrent language models were
%shown to represent intriguing syntactic and semantic regularities \cite{mikolov2013linguistic}.

%In earlier work where pre-trained word-embeddings were also shown for the first time through a
%larger-scale empirical investigation by \cite{turian2010word} to transfer to sequence prediction tasks
%when used as features in state-of-the-art systems.


\section{Visually grounded representations of words}
\label{sec:visualwords}
% \todo{ADD MORE FROM THIS https://arxiv.org/pdf/1806.06371.pdf}

\subsection{Language and perception}
\label{sec:langperc}

The discussion of learning distributed representations so far has focused on
implementations of the distributional hypothesis and extracting information exclusively
from text corpora. To human language learners, however, a plethora of
perceptual information is available to aid the
learning process and to enrich their mental representations.
The link between human word and concept representation
and acquisition and the perceptual-motor systems has been well established through behavioral
neuroscientific experiments \citep{pulvermuller2005brain}.
The earliest words children learn tend to be names of concrete perceptual phenomena
such as objects, colors and simple actions \citep{bornstein2004cross}. Furthermore, children generalize to
the names of novel objects based on perceptual cues such as shape or color \citep{landau1998object}.
In general, the \emph{embodiment}-based theories of concept representation and acquisition in the
cognitive scientific literature put forward the view that a wide variety of cognitive processes
are grounded in perception and action \citep{meteyard2008role}. The precise role of
sensori-motor information in language acquisition and representatio, however, is a highly
debated topic \citep{meteyard2012coming}.

Motivated by such cognitive theories and experimental data,
various computational cognitive models of child
language acquisition investigate the issue
of learning word meanings from small scale or synthetic multi-modal data. The model presented by
\cite{yu2005emergence} uses visual information to learn the meanings of object
names whereas the architecture of \cite{roy2002learning} learns to associate word sequences
with simple shapes in a synthetically generated data setting.

Interestingly even the articles introducing Latent Semantic Analysis
\citep{landauer1997solution} and Hyperspace Analogue to Language \citep{lund1996producing} mention
that a possible limitation of the presented distributional semantic models is the lack of
grounding in extra-linguistic reality. \cite{landauer1997solution} puts it as "But still, to be more than
an abstract system like mathematics words must touch reality at least occasionally."
The lack of relationship between symbols and the external reality is usually referred
to as the \emph{grounding problem} in the literature \citep{harnad1990symbol,perfetti1998limits}.
On the defense of purely textual models \cite{louwerse2011symbol} argues that the corpora
used to train distributional semantic models are generated by humans and as such reflect the perceptual world.
For a counter argument consider the few pieces of text that would state obvious perceptual
facts such as ``bananas are yellow'' or how often objects with the property ``yellow''
would appear in similar textual contexts \citep{bruni2014multimodal}.

In practice much work on multi-modal distributional semantics have found that text-only spaces tend to
represent more encyclopedic knowledge, whereas multi-modal representations capture more concrete aspects
\citep{andrews2009integrating,baroni2008concepts}. In Chapter~\ref{ch:TAL}, where we develop
a cross-situational cognitive model of word-learning, we also find that the word-representations learned by
our model correlate better with human similarity judgements on more concrete than abstract words.
In contrast, word-embeddings learned by the SGNS algorithm trained on the same sentences
perform better on more abstract words.

One does not necessarily need to reach a conclusion on whether grounded or distributional
models are superior;  combining their merits in a
pragmatic way is an attractive alternative \citep{riordan2011redundancy}.

%Learning representations
%from both linguistic and visual input is a step towards realistic models of language learners.
%Note however, it is as close as assuming children learn language by sitting still and watching TV.

\subsection{Combined distributional and visual spaces}
\label{sec:distvis}

When learning multimodal word representations we wish to construct a matrix,
where each row 
corresponds to a word and each feature column
represents a distributional feature, a perceptual feature or a mixture of the two.

The first approach to learn visual word representations from realistic data sets
was introduced by \cite{feng2010visual}. They develop a multi-modal topic model trained on a
BBC News data set containing text articles with image illustrations.
Each document considered by their model is a pair of a text document  and  an image generated by
a mixture of latent multi-modal topics. The documents are represented as bag-of-words vectors (BoW), while 
images are representedas bag-of-visual-words (BoVW)  \citep{csurka2004visual} using a
difference-of-Gaussians point detector for image segmentations
SIFT features for local region descriptors \citep{lowe1999object}.
Each document is then represented as the concatenation of BoW textual an BoVW visual features.
A Latent Dirichlet Allocation (LDA) \citep{blei2003latent} topic model is trained
on joint representations and after convergence each word is represented by a vector,
where each component corresponds to the conditional probability of that word given
a particular multi-modal topic. \cite{feng2010visual}
shows that the multi-modal LDA model outperforms the text-only LDA representations by a
large margin on word association and word similarity experiments.
%Finally, these extracted features are clustered with K-means resulting in the
%bag-of-visual-words  representation i.e.
%a vector of counts, where each entry is the number of regions on the image that
%corresponds to a certain SIFT-cluster. 

The perceptually grounded word representations of \cite{bruni2012distributional}
also combine distributional semantic models and BoVW pipelines.
Rather than having a collection of documents they consider a set of words for which
both distributional and image features are available. For visual word representations
they use images labeled with tags by annotators. Similarly to \cite{feng2010visual} they
apply a BoVW pipeline to represent images. Contrary to \cite{feng2010visual}, however,
a visual-only representation is created for each tag-word by summing over the features
for all images corresponding to the tag.
For text-only models \cite{bruni2012distributional} construct several types of distributional semantic spaces
from text-only corpora unrelated to the images.
As in \cite{feng2010visual} they wish to create a multimodal
word representation and apply separate pipelines to extract textual and visual
features. Finally they create the multi-modal space through concatenation.

On word-similarity benchmarks \cite{bruni2012distributional} show that the text-only model performs better than visual-only
and that the combination of the two surpasses both.
They also find that distributional semantics
models perform poorly on finding the typical colors of concrete nouns,
whereas the visual and multi-modal models perform perfectly. In these experiments
distributional semantics models fail to capture the obvious fact that ``the grass is green''
providing evidence against the theoretical argument that perceptual information is available in
large collections of texts and so grounded representations are superfluous \citep{louwerse2011symbol}.

Combining BoVW and count-based distributional word-representations remained the standard methodology
in many other works on multi-modal word representations at the time
\citep{bruni2011distributional,leong2011going,leong2011measuring}.
\cite{bruni2014multimodal} frame multi-modal distributional semantics under a general framework:
create separate textual and visual features for words followed by
re-weighting and matrix factorization. Researchers can apply distributional methods
models to create a semantic space and computer vision techniques to create
visual feature representations for the same words. The separate feature spaces
are mixed together  first  by concatenation, which is
optionally followed by singular value decomposition
to combine the two modalities.\footnote{We remark that discovering latent factors through the application
of singular value decomposition on the concatenated textual and visual spaces is similar in spirit
to applying topic models such as LDA.}

For example \cite{kiela2014learning} run the SGNS algorithm
on large text corpora and take the word-embedding parameters
as the distributional space and
apply pre-trained convolutional neural networks (CNN) as black-box image
feature extractors. We apply CNNs as image feature extractors in all chapters.

%This framing allows researchers to take advantage of evolving NLP and computer vision techniques
%and experiment by plugging in various distributional semantics methods as textual feature extractors
%and image processing techniques.

Convolutional neural networks learn a hierarchy of blocks of image filters followed by
pre-defined pooling operations optimized for a particular task.
It has been observed in the computer vision community that the lower layers of
deep CNNs trained across various data sets and tasks tend to learn filter maps
that resemble Gabor filters and color blobs. Intuitively these low-level features appear to be general
and as such afford \emph{transfer}. There is an extensive body of work on exploring the transferability
of CNN features to various computer vision tasks through fine-tuning
 \citep{donahue2014decaf,oquab2014learning} or simply taking the last layer representation of CNNs
as high-level features as inputs to linear classifiers \citep{girshick2014rich,sharif2014cnn}.
Given their success in transfer learning in computer vision it is natural to apply CNNs in the
visually grounded language learning community as black-box image feature extractors.
Similarly to \cite{bruni2014multimodal} in
\cite{kiela2014learning} the visual features of words are computed as the summary the feature vectors
extracted from all images they co-occur with.
Applying the simple concatenation operation to the two spaces
they show that CNN features outperform BoVW features on word similarity
experiments with the multi-modal word representations.

All approaches described so far require both textual and visual information for the same concepts and
representations for these modalities are learned separately and are fused later.
The multi-modal skip-gram \citep{lazaridou2015combining} model was developed to alleviate such limitation:
it is a multi-task extension of the skip-gram algorithm predicting both the context of words, but also
the visual representations of concrete nouns. This optimizes word representations to be predictive of
both their visual and textual contexts jointly. Ground truth visual representations are constructed by averaging
the CNN representations \citep{krizhevsky2012imagenet} of 100 pictures sampled from ImageNet \citep{deng2009imagenet}.
This architecture was later proposed as a model of child language learning and
was applied to the CHILDES corpus \citep{macwhinney2014childes} with modifications to model referential
uncertainty and social cues \cite{lazaridou2016multimodal} present in language learning.
Later it was compared to human performance in terms of learning the meaning of novel words from minimal 
exposure  \citep{lazaridou2017multimodal}.


Chapter~\ref{ch:TAL} presents a computational cognitive model of word learning using only
visual information developed at the same time as the multi-modal skip-gram approach.
Similarly to \cite{feng2010visual} we assume pairs of text and images. 
However, in our dataset images represent everyday scenes and are paired with descriptive
sentences to mimic the language environment of children on a high level.
Our model is inspired by the cross-situational account of language
learning  and assumes that the representations of words are learned exclusively through the co-occurrences
between visual features and words. Rather than building a
matrix of image-features per word and then summarizing as in \cite{kiela2014learning},
we apply an online expectation-maximization-based 
algorithm \citep{dempster1977maximum} to align words with image features mimicking 
child language learning.
In essence, our approach combines the cross-situational incremental word-learning model
of \cite{fazly.etal.10csj} with the larger realistic data sets, modern convolutional image representations
and extends it to operate on real-valued scene representations.
The result of the learning process is a word embedding matrix,
where each row corresponds to a word, each column 
to a CNN feature and each entry to the strength
of the relationship between the word and an image-feature.

We show through word-similarity experiments that, while our approach performs on
par with the SGNS trained on the same text data, there is a qualitative difference between
the learned embeddings: the correlation between our visual word-embeddings 
and human similarity judgements is significantly higher for concrete than abstract nouns. 
As each word embedding  is represented in the
image-feature space as in \cite{kiela2014learning}, we show that our model can label images 
with related nouns through simple nearest-neighbor search.

\section{From words to sentences}
\label{sec:sentences}
Applying the distributional intuition to model the meaning of sentences is not as straightforward
as it is for words. In a sentence-embedding matrix each row
would correspond to a possible sentence and each column to a feature.
Intuitively however,  the number of words in a corpus is much
lower than the number of sentences: one can assume a large, but finite set
of existing words and an infinite set of
potential sentences to \emph{compose} from them.
Words can be thought of as \emph{atomic units} and
in downstream applications one can use a lookup operation on a word embedding matrix
to represent units in the input. However, it is infeasible to look up full sentences.
In fact, from a method that represents sentences in a continuous space one would expect
to also represent and generalize to unseen sentences at test time.
Furthermore, given two sentences $\mathit{John \; loves \; Mary}$ and
$\mathit{Mary \; loves \; John}$ we wish our sentence encoding function
$\phi$ to represent the meaningful difference stemming from the underlying syntactic structure
$\phi(\mathit{John \; loves \; Mary}) \neq \phi(\mathit{Mary \; loves \; John})$.
As such we seek to learn a sentence encoder that is sensitive to
syntactic structure and semantic compositionality i.e.:
the notion that the meaning of an expression is
a function of its parts and the rules combining them \citep{montague1970english}.\footnote{Similarly one could argue that \emph{morphemes} are atomic units from which words are composed. In fact, sub-word
representations form an important field of research \citep{bojanowski2017enriching}.}

The compositional distributional semantics  framework  produces
continuous representations for phrases up to sentences using additive and multiplicative interactions
of count-based distributed word-representations \citep{mitchell2008vector} or combine symbolic and
continuous representations with tensor-products \citep{clark2007combining}.
The latter line of work culminated in a number of unified theories of
distributional semantics and formal type logical and categorical grammars
\citep{coecke2010mathematical,clarke2012context,baroni2014frege}.
These approaches assume that words are represented by distributional word embeddings
and define compositional operators on top of them motivated by particular formal
semantic considerations. From the point of view of theoretical linguistics arguably
one of the most intriguing aspects of such theories of meaning is that
they provide an elegant data-driven solution to deal with the representation of the lexical entries of
content words -- nouns, verbs and adjectives.
% This is  otherwise a major challenge in formal semantics
%requiring sophisticated formalisms such as typed $\lambda$-calculus with intricate and vast
%type-systems \citep{asher2011lexical}. 
Within applied NLP, however, this line of work has not resulted in
practical machine learning approaches to solve natural language tasks
on real world data sets. This is likely due to the different scope and
the computationally expensive high-order
tensor operations involved \citep{bowman2016modeling}.

Lastly, before going forward with the more recent neural models in the next section,
it is only fair to mention that bag-of-words
based representations bypassing the issue of compositionality form a set of very strong baselines
for a number of sentence-level tasks \citep{hill2016learning}.
These simple baselines include using multinomial naive Bayes uni- and bigram
log-count features within support vector machines \citep{wang2012baselines} or
feeding the average of the word-embeddings in a sentence into a softmax classifier \citep{joulin2016bag}.


\section{Neural sentence representations}
\label{sec:trans-sentence}

In Section \ref{sec:words} we have discussed the feed-forward \citep{bengio2003neural}
and recurrent network \citep{mikolov2010recurrent} language models from the perspective
of learning word-representations. However, both architectures learn embeddings of not only single words,
but also learn to represent sequences of multiple words such as sentences.
An intriguing property of such approaches is that
they represent various linguistic objects in the same space as activations of the neural models.\footnote{This property is not unique to neural models e.g. the pregroup-algebra-ased compositional distributional
semantics framework of \cite{coecke2010mathematical}
sentences of any grammatical structure live in the same inner-product space.}
When learning transferable
sentence representations there are two main considerations we will discuss:
1.) which architecture to choose and 2.) what objective to optimize.
Various neural network architectures have been proposed that
handle variable-sized data structures useful for language processing: recurrent networks
take the input sequentially one word or character at a time,
convolutional neural networks
\citep{kalchbrenner2014convolutional,zhang2015character,conneau2016very,chen2013learning}
process sequences in fixed-sized n-gram patterns up to a large window,
recursive neural networks \citep{goller1996learning,socher2011parsing,kai2015treelstm}
take a tree data structure as input such as a sentence according to the traversal of a
constituency and graph neural networks operate on graphs \citep{marcheggiani2017encoding} such as
syntactic/semantic dependency or abstract meaning representations.
All the aforementioned architectures take word-embeddings as input
and compute fixed vectors for sentences.
These representations are tuned to a specific task such as sequence tagging,
sentence classification, machine translation, parsing or language modeling.


In Chapters \ref{ch:COLI}, \ref{ch:IJCNLP}, \ref{ch:ConLL} and \ref{ch:EMNLP} we decided to apply
recurrent neural networks as sentence encoders.
Recursive neural networks provide a principled approach to
compute representations along the nodes of constituency
or dependency parse trees \citep{socher2013recursive,socher2014grounded,le2015compositional,kai2015treelstm}.
In practice, however, these architectures require parse trees as input,
which makes them impractical for our mission of learning visually grounded
representations for multiple languages.
For each language considered, the training procedure requires finding a good
pipeline from text pre-processing to parsing to generate the input representations
for the networks. Furthermore, tree structures by nature do not
afford straightforward batched computation directly and tend to run dramatically
slower than recurrent or convolutional models. In terms of performance the jury
is still out, however, so far only modest improvements have been observed
over recurrent models on specific tasks in specific settings
\citep{li2015tree,kai2015treelstm}. For graph structured neural networks the
same argument holds. Two equally practical alternatives to RNNs
that operate on raw sequences are convolutional neural networks
\citep{bai2018empirical} and transformers \citep{vaswani2017attention} and both could
replace the RNNs in Chapters~\ref{ch:IJCNLP}, \ref{ch:ConLL} and \ref{ch:EMNLP}.


As mentioned earlier contrary to word-embeddings the representations
and the composition function these models learn are task-specific and not \emph{universal}.
For learning transferable and general distributed sentence representations the
first notable approach is the
skip-thought vectors model \citep{kiros2015skip}.
It extends distributional semantics intuition to the sentence level:
they train a sentence encoder on contiguous sentence sequences
-- such as books -- which learns sentence representations predictive of the sentences around it.
More specifically
they train a recurrent encoder to encode a target sentence and use two separate recurrent decoders to
generate its pre-context and post-context.
This method was confirmed to be a successful self-supervised method for transfer
learning in a number of sentence
classification tasks beating simpler methodssuch as bag-of-words approaches based on skip-gram
or CBOW embeddings, tf-idf vectors and auto-encoders \citep{hill2016learning}. A convolutional
variant of the encoder was introduced in \cite{gan2016unsupervised} and several other works
train simple sum/average pre-trained word-embedding encoders using the same sentential
context prediction objective \citep{kenter2016siamese,hill2016learning}. The larger context of paragraphs is
explored in \cite{jernite2017discourse} where the task is to predict discourse coherence
labels in a self-supervised fashion.

Some later approaches have  moved away from distributional cues
and identifyied supervised tasks that lead to representations that transfer
well to a wide variety of other tasks. The task of Natural Language Inference \citep{bowman2015large,williams2017broad} as an objective
was identified to learn good sentence embeddings \citep{conneau-EtAl:2017:EMNLP2017,kiros2018inferlite}
and \cite{subramanian2018learning} combine a number of other supervised tasks
with self-supervised training through multi-task learning.


The state-of-the-art in learning universal sentence representations
at the time of writing is represented by neural language models
with a large number of parameters
trained over huge corpora \citep{peters2018deep,devlin2018bert}.
These approaches go back to
exploiting only distributional cues and train a stack of convolutional and/or recurrent
and/or transfomer layers on large-scale language modeling.
When transferring the networks to novel tasks they are either fine-tuned, a
separate smaller network is trained on top of their representations or they are used as
fixed feature extractors \citep{howard2018universal,peters2019tune}.

\section{Visually grounded sentence representations}
\label{sec:visualsentences}

Universal sentence representations are in general learned from text-only corpora. The most
successful current trend is large-scale language modeling based on
the distributional semantics intuition of the general usefulness of linguistic context prediction.
However, this leaves the resulting sentence representations blind to the language-external
reality leading to the grounding problem
as discussed in Section~\ref{sec:visualwords}. Given that visual information
has been shown to contain useful information for word-representations it
is a natural question to ask whether this observation generalizes to sentence
embeddings. The idea of context prediction coming from the distributional
hypothesis can be adopted to visual grounding in a conceptually straightforward manner:
train sentence embeddings to be predictive of their visual context.

The larger family of techniques that our visually grounded sentence learning
approach technically belongs to is \emph{learning to rank} \citep{li2011learning}.
Some of the earlier attempts at multi-modal ranking did not consider full sentences rather,
images paired with (mostly) noun tags such as \cite{weston2010large}.
%Another influential framework is the multimodal deep
%boltzmann machine \citep{srivastava2012multimodal};
%a generartive model that can infer joint representations of images and tags or generate
%tags given images.

Framing the learning of multi-modal vision and language spaces as a ranking problem
from images to descriptions and conversely from descriptions to images was
put forward by \cite{hodosh2013framing}.
They argue that evaluating grounded learning through image--description generation
is plagued by the lack of straightforward performance measures.
This issue was later discussed in \cite{elliott2014comparing} who demonstrate low to moderate correlation
between automatic measures such as BLUE, METEOR and ROUGE with human judgements.

From the image--sentence ranking perspective  a joint space between
language and vision reflects accurately
the underlying semantics if given one modality as \emph{query} the other modality
can be accurately \emph{retrieved}.  This leads to a straightforward evaluation protocol
adopting metrics from information retrieval literature such as Recall@N, Precision@N, Mean Reciprocal
Rank, Median Rank or Mean Rank. From the practical point of view it also unifies image--annotation
and image--search based on language queries.

The standard benchmark data sets  we used for this purpose
annotate images found in online resources with descriptions through crowd-sourcing.
These descriptions are largely \emph{conceptual}, \emph{concrete}
and \emph{generic}.
This means that descriptions do not focus too much on \emph{perceptual}
information such as colors, contrast or picture quality; they do not mention
many \emph{abstract} notions about images such as mood and finally the descriptions
are not \emph{specific} meaning that they do not mention the names of cities,
people or brands of objects.
What they do end up mentioning are entities depicted in the images
(frisbee, dog, boy) their attributes (yellow, fluffy, young) and the
relations between them.
The images depict common real-life scenes such as a \emph{bus turning left} or \emph{people
playing soccer in the park}. As such, annotation collected independently from
different crowd-source workers end up focusing on different aspects of
these scenes. For a comprehensive overview on image-description data sets
consult \cite{bernardi2016automatic}.

Following the ranking formulation of grounded learning by \cite{hodosh2013framing}
the earliest works applying a combination of sentence and image encoder neural networks
to image--sentence ranking were put forward by \cite{kiros2014unifying} and \cite{socher2014grounded}:
they both apply pre-trained convolutional neural networks as image encoders and while
\cite{socher2014grounded} use recursive neural network variants to encode sentences
\cite{kiros2014unifying} apply recurrent networks. Rather than matching whole images with full
sentences the alternative approach of learning latent alignments between image--regions and
sentence fragments have also been explored concurrently \citep{karpathy2014deep,karpathy2015deep}.


To predict images from the sentences -- and conversely sentences from the images --
the architecture we chose in
Chapters \ref{ch:COLI}, \ref{ch:IJCNLP}, \ref{ch:ConLL} and \ref{ch:EMNLP} follows \cite{kiros2014unifying}: 
we combine a recurrent neural network to represent sentences and a pre-trained convolutional neural network
followed by an affine transformation we train for the task
to extract features from images.
The image-context prediction from sentences in
Chapter \ref{ch:COLI} is formulated  as minimizing the cosine distance
between the learned sentence  and image representations
in a training set of image--caption pairs.


Later we follow the formulations of \cite{vendrov2016order} and \cite{faghri2017vse++} and apply
the sum-of-hinges loss in Chapter~\ref{ch:IJCNLP} and max-of-hinges ranking objective
in Chapters~\ref{ch:ConLL} and \ref{ch:EMNLP}. These loss functions
push relevant image--sentence pairs close, while contrastive pairs 
far from each other in a joint embedding space. Given a mini-batch of e.g. 100 samples, for each sample
the contrastive pairs are generated by taking the wrong pairings from that batch leading to
99 contrastive pairs per sample. The ranking losses are minimized in both image $\rightarrow$ sentence
and sentence $\rightarrow$ image directions.

We empirically found both ranking losses to perform better than minimizing the cosine distance alone 
and max-of-hinges to perform consistently better in our experiments than sum-of-hinges. 
The use of these various objectives across
chapters  reflects the evolving common practices in the field at the time.

%Similarly in Chapter~\ref{ch:COLI} as the pre-trained CNN image-feature extractor we apply the VGG-16
%architecture \citep{conneau2016very},  in Chapter~\ref{ch:IJCNLP} the Inception-V3 \citep{Szegedy2015},
%while in ~\ref{ch:ConLL} and ~\ref{ch:EMNLP} the ResNet-50 \citep{he2016deep}. These choices are
%again somewhat arbitrary. Chapter~\ref{ch:COLI} is based our Imaginet model \citep{chrupala2015learning}
%and VGG features were the standard choice at that time.  For Chapter~\ref{ch:IJCNLP} we empirically
%determined Inception-V3 to give the best performance, while for Chapters~\ref{ch:ConLL} and \ref{ch:EMNLP}
%we used ResNet-50 as the features extracted from this architecture were given as part of
%a Shared Task based on the data set we used.

%Other than image--sentence context prediction we also apply the $J_{max}$
%objective for sentence--sentence ranking in Chapters~\ref{ch:ConLL} and \ref{ch:EMNLP}.
%The data for this objective are sentences that belong to the same image.

%Note the similarity between the ranking losses and the negative sampling
%in the skip-gram model \citep{mikolov2013distributed}.

The representations learned by such image--sentence  ranking models have
been shown
to improve performance when combined with skip-thought embeddings
on a large number of semantic sentence classifications tasks compared to
skip-thought only \citep{kiela2017learning}. These findings were confirmed and improved upon using a
self-attention mechanism on top of the RNN encoder \citep{yoo2017improving}.

To investigate the difference between the representations learned by (text-only) language
models and image--sentence ranking models in Chapter~\ref{ch:COLI} we develop 
novel visualization and analysis methods.
Expading on the findings of \citep{kiela2017learning} and \citep{yoo2017improving} 
in Chapter~\ref{ch:IJCNLP} we show that
visually grounded learning through an image--sentence ranking objective 
leads to better translations in the visually descriptive domain. Furthermore,
we show that learning multi-modal representations provides gains on top of learning
from larger bilingual corpora.
Finally in Chapters~\ref{ch:ConLL} and \ref{ch:EMNLP} we apply the image--sentence ranking
framework \citep{vendrov2016order,faghri2017vse++} in the multi-lingual setting and demonstrate
that better visually grounded representations can be learned when training on multiple
languages jointly.

%For our multi-lingual experiments we use the Multi30K data set \cite{elliott2016multi30k,elliott2017findings}; a multilingual extension of Flickr30K. It consists of a {\it translation} and a {\it comparable} portions both containing all images from the original Flickr30K. The \emph{translation} portion annotates a single English description with a German, French and Czech caption, while the \emph{comparable} the original 5 English and additional 5 German descriptions collected independently using the same crowd-sourcing process.

%\paragraph{Our contribution}


%In \citep{chrupala2015learning} we train a multi-task architecture consisting
%of a shared word-embedding matrix and two pathways: 1.) common recurrent language
%model trained to maximize the probability of a word given the history,
%2.) grounded representation learning model predicting the image representation paired with the sentences.
%This model combines the distributional hypothesis for learning the meaning of words
%and the grounded learning on the sentence level.
%We show that the word-representations learned by the two path architecture
%predict human word-similarity judgements with higher accuracy then the one-path
%architectures. Furthermore, the combined model is more sensitive to word-order than
%the visual-only model.  Lastly we show that images act as an
%anchor and the architecture performs paraphrase retrieval well above chance
%level.

%Chapter~\ref{ch:COLI} is dedicated to develop techniques to interpret the
%opaque continuous representations of this model. It provides an in-depth
%comparative analysis of the linguistic knowledge represented by the
%language-model and visual-pathways of this architecture.




%\section{Multilingual representations of words and sentences}\todo{this will be challenging, not sure yet how to build it up.}
%Multilingual or cross-lingual word embedding approaches map words of different languages in a shared space. These representations allow to transfer knowledge between languages and
%perform cross-lingual tasks such as cross-lingual document retrieval. The bulk of such approaches apply the techniques developed for monolingual word-embedding induction as a component
%of their full pipeline. One of the early approaches trains two separate word spaces with using the negative-sampling implementation of the skip-gram algorithm and then maps these spaces onto eachother by minimizing the squared loss between the 5000 top most frequent word-vectors \cite{mikolov2013exploiting}. This main approach went through improvents
%such as by switching mean-squared error to ranking loss \cite{lazaridou2015hubness} or enforcing orthogonality constraint on the mapping matrix
%\cite{xing2015normalized}   (WE DID THE SAME OVER TIME). Another prominent technique to learn a mapping between word (and other types of) spaces is Canonical Correlation Analysis, first
%used by \cite{faruqui2014improving}, which was later improved by the application of Deep CCA \cite{lu2015deep}. Another approach is to create a pseudo-bilingual corpora by replacing words
%in a source language by their translation and train an embedding model on the resulting corpus
%The uber multilingual word-embedding \cite{ammar2016massively}.

\section{Visually grounded multilingual representations}
\label{sec:vismulti}

On top of the visual modality anchoring linguistic representations to perceptual reality it
also provides a universal meaning representation bridging between languages.
The  intuition being that words or sentences with similar meanings appear
within similar perceptual contexts independently of the language.
First in Section~\ref{sec:multiview} we discuss
the multi-view representation learning perspective of considering images annotated
with multiple descriptions in different languages as multiple views of the
same underlying semantic concepts.
Our aim in Chapters~\ref{ch:ConLL} and \ref{ch:EMNLP} is to learn
better visually grounded sentence representations by learning from these multiple views
simultaneously. Furthermore, in Chapter~\ref{ch:IJCNLP} we show that we can improve
translation performance by learning better sentence representations through  adding the visual
modality as an additional view.
Next we discuss how images can be used as pivots in practice for translation on word-level 
and on sentence-level in Section~\ref{sec:imgpivot}.

\subsection{Multi-view representation learning perspective}
\label{sec:multiview}


Images and their descriptions in multiple languages can be taken as different views of the
same underlying semantic concepts. From this multi-view perspective learning common representations
of multiple languages and perceptual stimuli can potentially exploit the complementary information
between views to learn better representations. Being able to extract
a shared representation from only a single view also leads to practical applications such as
cross-modal and cross-lingual retrieval or similarity calculation.

Multi-view learning traditionally considers paired samples
where each pair of rows in two feature matrices
are two measurements of the same
underlying phenomena. In other words we assume two sets of variables representing the
same data point. The two main multi-view learning paradigms put forward in recent literature are based on
autoencoders and canonical correlation analysis (CCA) \citep{wang2015deep}.

\cite{ngiam2011multimodal} introduced the idea
of multi-modal autoencoders to learn multi-modal joint representations of audio and video. Their
architecture has a shared encoder neural network extracting features from both modalities and two
modality specific decoders.  
Their approach learns shared representations such that one view can be reconstructed from another and
the activations of the shared encoder learn a multi-modal joint space.
Autoencoder approaches  remain one of the standard family of models
to study the learning of visual-linguistic multi-modal spaces \citep{silberer2014learning,silberer2017visually,wang2018associative}.

For the discussion of CCA-based approaches let us consider the deep canonical correlation analysis (DCCA)
put forward by \cite{andrew2013deep}.  In this approach view
specific networks are applied to extract non-linear features
and the canonical correlation between these representations is maximized.
This optimization process amounts to maximizing
the correlation between the projections of the two data views subject to the constraint that
the projected dimensions are uncorrelated.


%\begin{align}
%\max_ {\theta_{\phi}, \theta_{\psi}, \mathbf{U}, \mathbf{V}} \; \; &  \frac{1}{N} \text{tr}(\mathbf{U}^T \phi(\mathbf{X}) \psi(\mathbf{Y})  \mathbf{V}^T) \\
%\text{subject to} \; \; & \mathbf{U}^T \left(\frac{1}{N} \phi(\mathbf{X}) \phi(\mathbf{X})^T + r_x \mathbf{I} \right) = \mathbf{I} \\
%& \mathbf{V}^T \left(\frac{1}{N} \psi(\mathbf{Y}) \psi(\mathbf{Y})^T + r_y \mathbf{I} \right) = \mathbf{I} \\
%& \mathbf{u}_i^T  \phi(\mathbf{X}) \psi(\mathbf{Y})  \mathbf{v}_j = 0, \; \; \forall_{i,j} \;  i \neq j
%\end{align}

%Here $\theta_{\phi}, \theta_{\psi}$ are neural network parameters, $r_x, r_y$
%are regularization parameteres \citep{de2003regularization} and $\mathbf{U}, \mathbf{V}$
%are projection matrices or \emph{directions} of the CCA. 

The \emph{workhorse} of the image--sentence ranking experiments in the foundational
work on the topic from \cite{hodosh2013framing}
was in fact the Kernel CCA method \citep{akaho2006kernel}. Other CCA
based approaches were also applied to the image--sentence data sets we consider in our work \citep{ ,gong2014improving,Klein2015AssociatingNW,eisenschtat2017linking}.

A third direction that is also explored in the literature is combining the reconstruction 
objective of autoencoders with an additional correlation loss, but without the 
whitening constraints of CCA \citep{ap2014autoencoder,chandar2016correlational}.

In the multi-lingual multi-modal setting \cite{funaki2015image} apply Generalized CCA \citep{horst1961generalized} -- a variant of CCA
generalized to multiple views as opposed to only two -- to
learn correlated representations of images and multiple languages.
The deep partial canonical correlation analysis approach
 -- a deep learning extension of the partial canonical correlation  \citep{rao1969partial} --
learns multilingual English-German sentence embeddings conditioned on the
representation of the images they are aligned to \citep{rotman2018bridging}.
They show that their model using the visual modality as an extra view finds
better multilingual sentence and word representations as demonstrated by
cross-lingual paraphrasing and word-similarity results.

The bridge correlational neural network approach \citep{rajendran2015bridge}
combines autoencoders and correlation objectives to
learn common representations in a setting when the different views only need to be
aligned with one pivot view. They preform
image-sentence retrieval experiments in French or German where
the image-caption data set is only available for English, however,
there are parallel corpora between German or French and English. In other words English acts as a pivot.
A similar combination of autencoder and correlational training was applied to \emph{bridge}
image--captioning \citep{Saha2016}.

Our formulation of the problem does not fall in the set of approaches generally
considered as multi-view learning.  However, it is similar to the CCA-based techniques in that we
train two sub-networks -- one for the linguistic and another for the image modality --
and do not rely on decoder networks to compute a reconstruction  loss as in the autoencoder approaches.
Another connections is that in Chapter~\ref{ch:COLI} we minimize the cosine distance between the learned representations
which is related to the CCA objective: when the feature matrices are not centered
the CCA objective corresponds to maximizing the cosine similarity instead of the correlation.

One of the main benefits of the learning to rank approach we opted for is
its flexibility: 1.) in Chapter~\ref{ch:COLI} we train an image--sentence ranking model in a single
language, 2.) we apply the same building blocks to train on multiple languages where
the same images are shared between languages in Chapter~\ref{ch:ConLL}, 3.) in
Chapter~\ref{ch:EMNLP} we explore the setup without such an alignment and finally 4.)
in Chapter~\ref{ch:IJCNLP} we improve the automatic translation  performance
by adding the image--sentence ranking objective, incorporating an additional view
to help us learn better sentence representations.

In our setup additional views are incorporated in the sentence representations
by full parameter sharing through multi-task learning:
given multiple image--caption data sets at each iteration we sample
a batch from one of them and perform an update to the encoders
using the ranking loss function.
\cite{gella2017image} apply the image--sentence ranking framework in the multilingual setup
considering images as pivots bridging English and German and train a multilingual
image--sentence ranking model.
Their results suggest that the multilingual models outperform the monolingual ones on image-sentence
ranking, however, they do not show consistent gains across languages and model settings.
In Chapter \ref{ch:ConLL} we implement a similar setup to \cite{gella2017image}
and  show that both English and German image-sentence
ranking performance is reliably improved by bilingual joint training using our setup.
We expand on the results further and provide evidence that more gains can be
achieved by adding more views: on top of English and German, we
add French and Czech captions and show the monolingual model is consistently
outperformed by the bilingual and the latter by the multilingual.
%Revisiting the issue of alignment we find that
%the unlike in previous experiments \citep{gella2017image,calixto2017multilingual,rotman2018bridging}
%image-pivoting leads to better visual-sentence embeddings even when the images
%are not matched between languages.
We apply the same approach to improve the performance on the lower resource
French and Czech languages by adding the larger
English and German image-caption sets; showing successful multilingual
transfer in the vision-language domain.


\subsection{Images as pivots for translation}
\label{sec:imgpivot}

On word level images have been used to link languages and induce bilingual
lexicons without parallel corpora. The lack of bi-text in this setting has
been traditionally solved by methods relying on textual features
such as ortographic similarity \citep{haghighi2008learning} or similar diachronic distributional
statistical trends between words \citep{schafer2002inducing}.
However, images tagged with various labels in a multitude of languages are
available on the internet and a model of image-similarity can be used to exploit
images as pivots to find translations between such tags. Alternatively representing words in multiple
languages in a joint space with images
-- as discussed in Section~\ref{sec:visualwords} for the monolingual case --
allows word translation to be simply performed through nearest neighbour search.

The first approach to construct a dictionary based on image similarity \citep{bergsma2011learning} 
used Google image search to find
relevant images for the names of physical objects in multiple languages.
These images are represented by BoVW vectors based
on SIFT and color features. Given a source word and a list of possible translations
Google is queried to find $n$ images per word. For each image the
feature vectors are extracted, which is followed by computing the similarity
between the feature vectors of the images corresponding to the source word and
all target words. Finally the word in the target vocabulary with the highest
similarity is chosen.

This method is vastly improved upon by a later approach applying
pre-trained convolutional neural networks to represent words
in the visual space \citep{kiela2015visual}. Their approach closely follows
the monolingual visual-word representations of \cite{kiela2014improving}:
each word is represented as the summary of CNN representations of images they
co-occur with. Given a word in the source language the candidate translation is found
simply by performing nearest neighbor search on the target vocabulary.

Exploring the limitations of image-pivoting for bilingual lexicon induction
\cite{hartmann2017limitations} present a negative result
showing that such techniques scale poorly to non-noun words such as
adjectives or verbs. However, combining image-pivot based bilingual
word-representations with more traditional multi-lingual word-embedding
techniques leads to superior performance compared to their uni-modal
counterparts \citep{vulic2016multi}.

The first large-scale vision-based bilingual dictionary induction data set
was put forward by \cite{hewitt2018learning}. They create a data set
of \~200K words and 100 images per word using Google Image Search and perform
experiments with 32 languages.
They confirm the finding of \cite{hartmann2017limitations} that image-pivoting
is most effective for nouns, however, find that using their larger dataset
adjectives can also be translated reliably. 

%Similarly to \cite{vulic2016multi}
%they also show that combining
%visual and textual word-representations through a re-ranking
%experiment that visual representations improve the state-of-the-art
%text-only methods in the case of \emph{concrete} concepts.

Images have also been used as pivots for translating full sentences.
In automatic machine translation a pivot-based approach is applied
when there are parallel corpora available between language pairs $A\rightarrow C$
and $C \rightarrow B$, but there is no data for $A\rightarrow B$. The problem is
solved by first translating $A$ to $C$ and then $C$ to $B$.
Image--pivoting refers to a setup where we assume the existence of a dataset
of images paired with words or sentences in different languages $A \leftrightarrow I_A$ and
$B \leftrightarrow I_B$ and translation is done through the image space
going  through $A \rightarrow I$ to $I \rightarrow C$.

\cite{nakayama2017zero} apply image-pivoting in such a zero-resource machine
translation setting. The task is to translate from English to German without
aligned parallel corpora, however, separate image--description data sets are
available in both languages. They solve the problem by
training two components: 1.) visual-sentence encoder that matches images and their descriptions,
2.) image-description generator maximizing the likelihood of gold standard captions given the images.
At test time the visual-sentence encoder representation of the source sentence is fed to the
image--description generator to produce the translation.
Their results were improved later by modeling
the image-pivot-ased zero-resource translation setup as a
multi-agent communication game between encoder and decoder
\citep{chen2018zero,lee2017emergent}.


\section{Interpreting continuous representations}
\label{sec:interpret}
The linguistic representations learned through neural architectures are notoriously opaque.
Contrary to count-based methods the features extracted by deep networks from text input
appear as arbitrary dense vectors to the human eye.
In experiments with grounded learning throughout this thesis we find that visual grounding
improves translation peformance and that multilingual representations outperform monolingual
ones in image--sentence ranking.
But where do these improvement come from? What are the linguistic
regularities represented in the recurrent states that lead to the final performance metrics?
Did the model learn to exploit trivial artifacts in the training data or did it learn to meaningfully generalize?
What characterizes the individual features recurrent networks extract from the input sentences?

The main topic of this thesis is learning visually grounded representations for linguistic units.
For a complete picture, however, it is crucial to assess the difference in the representations
between textual only and multimodal representations not only from a quantitative point of view,
but also from a qualitative linguistics angle.
This is what Chapter \ref{ch:COLI} is dedicated to.

Developing techniques for interpreting machine learning models have multiple goals.
From the practical point of view as learning algorithms make their way into critical applications
such as medicine, humans and machines need to be able to co-operate to avoid catastrophic
outcomes \citep{caruana2015intelligible}. As such there is a growing interest in deriving methods
to \emph{explain} the decision of such architectures.

One of the approaches is to assign a real-valued "relevance" score to each unit in the input signal,
signifying how much impact it had on the final prediction of the model.
One of the first paradigms in generating such relevance scores is gradient based methods:
they take the gradient of the output of the network with respect to the input \citep{simonyan2013deep}.
Deep neural models of language tasks learn distributed representations of input symbols
and as such further operations have be applied to reduce the resulting gradient vectors to
single scalars e.g.: using $\ell_2$ norm \citep{bansal2016ask}.

Another prominent and well studied
approach still based on gradient information is layerwise
relevance propagation (LRP) \citep{bach2015pixel}. The output of the final layer
is written as the sum of the relevance-scores from the input and similarly to the back-propagation
algorithm the relevance of each neuron
recursively depends on the lower-layer all the way down to the input signal.
Different versions of LRP run the backward pass with different rules taking as
input gradient information and activation values. It was later theoretically analyzed and generalized
into the deep Taylor decomposition method \citep{binder2016layer}.
%This has been shown to be equivalent to taking the
%gradients with respect to the input as in \cite{simonyan2013deep} and multiplying it elementwise
%with the input itself \citep{shrikumar2017learning}.
%LRP was also later derived for recurrent architectures \citep{arras2017explaining} to describe
%sentiment classifiers.

%Another backpropagation based algorithm is DeepLIFT, which instead of
%using the gradients for computing the relevance scores it uses the difference between the activations
%that result from a specific value compared to a baseline input.

Perturbation based methods are gradient free and are algorithmically very simple: they involve generating
pseudo-samples according to some procedure and measuring how the models' response changes
between each pseudo-sample and the original.
LIME \citep{ribeiro2016should} and its NLP specific LIMSSE extension
\citep{poerner2018evaluating} perturbs
the input creating a local neighborhood around it and fits interpretable linear models to explain
the predictions of any complex black box classifier.
Even simpler perturbation based techniques measure the
difference between the original input and the various perturbed candidates such as the erasure
\citep{li2016understanding} and our omission \citep{kadar2017representation} method in Chapter~\ref{ch:COLI}.

Apart from practical considerations of model interpretation training complex and opaque models
from close to raw input can help us discover interesting patterns in the input data that
are crucial in solving the task.
Deep neural networks learn to solve tasks from close to raw input, similar to
what humans receive. As such the regularities they learn can also shed light on the patterns
humans might extract from data to cope with certain tasks.
Recent methodology in probing the learned representations of LSTM language models,
in fact, resemble psycholinguistic studies.
A number of experiments using the agreement prediction paradigm
\citep{bock1991broken} suggest that LSTM language models successfully learn
syntactic regularities as opposed to memorizing surface patterns
\citep{linzen2016assessing,enguehard2017exploring,bernardy2017using,gulordava2018colorless}.

For our purpuses in Chapter~\ref{ch:COLI} we develop our explanation method to
shed light on the linguistic characteristics of the input grounded learning
models learn in contrast to text-only language models.


%\section{Vision and Language applications}
%Mention the bunch of work thats here. Do image-captioning and image-sentence ranking and then metion that we do the latter always. Say the importance of attention in this field and mention the DIDEC corpus. Mention VQA and fine-grained reasoning and the FigureQA paper.



%\section{Methods}

%\section{Data sets}
%For image-feature extraction all approaches presented in the thesis use some
%CNN architecture pre-trained on a version of the ImageNet data set \cite{deng2009imagenet} prepared for a large-scale image classification challenge \cite{russakovsky2015imagenet}. ImageNet annotates the noun synsets from WordNet \cite{miller1995wordnet} with images collected from the internet. At the time of writing this thesis on average the data set consists of 500 images per node. The section used in the pre-trained networks is the ILSVRC2012 containing 1.2 million images annotated with 1000 classes. For learning grounded language representation we use data sets introduced for image-captioning. These data sets annotate images found in online resources with descriptions through crowd-sourcing. The smallest data set we use is Flickr8K \cite{hodosh2013framing} with 8000 images, which was later extended to contain 30K images resulting in the Flickr30K benchmark \cite{young2014image} and finally the largest collection we use is the COCO image-caption benchmark \cite{chen2015microsoft} with 123K images. All data sets contain 5 captions per image.
%The descriptions collected for these data sets are largely \emph{conceptual}, \emph{concrete} and \emph{generic}. This means that descriptions do not focus too much on \emph{perceptual} information such as colors, contrast or picture quality; they do not mention many \emph{abstract} notions about images such as mood and finally the descriptions are not \emph{specific} meaning that they do not mention the names of cities, people or brands of objects. What they do end up mentioning are entities depicted in the images (frizbee, dog, boy) their attributes (yellow, fluffy, young) and their relations between each other. The images depict common real-life scenes such as bus turning left or people playing soccer in the park. As such annotation collected independently from different workers end up focusing on different aspects of these scenes. For a comprehensive overview on image-description data sets please consult \cite{bernardi2016automatic}.
%For our multi-lingual experiments we use the Multi30K data set \cite{elliott2016multi30k,elliott2017findings}; a multilingual extension of Flickr30K. It consists of a {\it translation} and a {\it comparable} portions both containing all images from the original Flickr30K. The \emph{translation} portion annotates a single English description with a German, French and Czech caption, while the \emph{comparable} the original 5 English and additional 5 German descriptions collected independently using the same crowd-sourcing process.


%\section{Architectures}
%Just do the usual GRU + CNN thing. cite Jamie (heart).

%\section{Recurrent network}
%Do the GRU mention that the LSTM is strictly more powerful according to Yoav, but it doesnt seem to matter at all according to Chung.

%\section{Convolutional network}
%Just super quick run-down on VGG and ResNet.

%\section{Optimization}
%Just mention that all this shit is optimized with some version of SGD and we use Adam all the time. Mention that Adam sucks but its just some common thing to do. Say this thing about the MSE or cosine loss that we start with that but i results in hubness actually as Angeliki points it out. So we move to the contrastive kinda losses.

%\section{Transfer learning}
%Don't really worry about it too much just say something about how awesome is that the CNNs trained on image net actually end up being so useful for us.

%\section{Multi-task learning}
%Multi-task learning has been an important paradigm since the earliest works on training neural architectures for natural-language processing \cite{collobert2008unified}. Multi-task learning involves optimizing multiple loss function jointly usually with the goal of exploiting commonalities between tasks. As described in the seminal work of \cite{caruana1997multitask}:

%\begin{quote}
%Multitask Learning is an approach to inductive transfer that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias. It does this by learning tasks in parallel while using a shared representation; what is learned for each task can help other tasks be learned better.
%\end{quote}

%Introducing inductive bias using different tasks means that the learning algorithm prefers a specific set of hypotheses over others just as in the case of other forms of regularization such as $\ell_1$ or $\ell_2$ penalties. Here I only discuss the specific instantiation of the mutli-task learning paradigm that are applicable to the chapters presented in this thesis and for a more comprehensive overview please consult \cite{ruder2017overview}. The technique used in the thesis
%is hard-parameter sharing \cite{caruana1997multitask}
