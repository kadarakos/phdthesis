%!TEX root = ../dissertation.tex

%EK: edits November 28

\chapter{Introduction}
\label{ch:introduction}

The ability to understand natural language plays a central role in humans' conception of intelligent machines.
Alan Turing already in the 1950s in his now famous Turing Test(s) gives natural language understanding a 
key role in tricking humans into thinking that they are interracting with their fellow specimen rather 
than a machine. One of the goals of Natural Language Processing (NLP) is to develop algorithms
and build systems to help machines understand what humans are talking about; to 
understand the \emph{meaning} of natural language utterrances. 
In this thesis I explore computational techniques to 
\textbf{learn the meaning of words and  sentences}
considering the \textbf{visual world} as a naturally occuring 
\textbf{meaning representation briding between languages}. 
The methods presented in this thesis seek to find relationships between \textbf{images} and 
\textbf{natural utterrances} in \emph{multiple languages}. 


The  Chapters of the thesis follow a progression 
starting with a single language at the word-level and arriving to multilingual visually grounded sentence
representations:

\begin{description}
\item[Chapter~\ref{ch:introduction}] the present chapter introduces the topic and contributions of 
the thesis.

\item[Chapter~\ref{ch:background}] exposes the related work and technical background in detail. 

\item[Chapter~\ref{ch:TAL}] presents
a cognitive model of language learning that learns \emph{visually grounded word representations}.

\item[Chapter~\ref{ch:CL} ] focuses on \emph{visually grounded sentence representations} and their 
intepretations from a linguistic perspective using the black-box architecture that is the basis 
for the chapters to follow: combination of a Convolutional Neural Network to extract visual features and
a Recurrent Neural Network to learn sentence embeddings.

\item[Chapter~\ref{ch:IJCNLP}] applies visually grounded representation learning approach that forms the
basis of Chapter~\ref{ch:CL} to \emph{improve machine translation} in the domain of visually descriptive language. 

\item[Chapter~\ref{ch:ConLL}] shows the clear benefints of learning visually grounded representations
for multiple languages jointly.

\item[Chapter~\ref{ch:EMNLP}] extends the investigations of Chapter~\ref{ch:ConLL} to the 
cross-domain setup breaking the assumption that for each language the same images are annotated
with different languages.
\end{description}
 


\section{Learning representations}
The foundational methodology applied in all chapters is \emph{statistical learning}. 
The earlier days of NLP was characterized by rule-based systems building on such 
foundations as Chomskyan theories of grammar or Montegue Semantics. 
Since the 1980s partly due to such theories falling out of fashion, but also to the increase in the amount 
of available computational power Machine Learning (ML) approahces revolutionized
the field.  \textbf{Learning} in general proved to be a crucial component to Artifical Intelligence and also
specifically in NLP. Machine Learning algorithms are designed with the goal that given an increasing 
number of examples a system improves its performance according to some measure of success.
Reflecting the structure of ML itself and the popularity of ML within the field, NLP research follows
a task-oriented methodology: researchers borrow or collect data sets, define measures of success and develop or
apply learning algorithms. In Chapters~\ref{ch:IJCNLP}, \ref{ch:ConLL} and 
\ref{ch:EMNLP}  closely follow this blueprint.

From the rule based times of "engineering grammars" researchers moved onto "engineering features"
to \textbf{represent} the textual data as input to general purpose pattern-recognition algorithms such as
decision-trees, support-vector machines or hidden Markov models. 
A large set of these features templates are still based on various formal-linguistic theories requiring various 
linguistic taggers and parsers to assign structure to raw texts.
Intuitively, different applications such as machine translation or goal-oriented dialogue systems 
require different input representations. Furthermore, one would assume that various languages require 
different feature-extraction pipelines reflecting the typological differences across languages. 

\emph{Linguistic representation learning} challenges this intuition and is interested in 
discovering general principles that allows machines to  \emph{learn} 
linguistic representaitons from \emph{raw data}, which are more-or-less
generally applicable. This line of work, as well as the approaches presented in the thesis,
fit in the general \textbf{representation learning}  framework consisting of 
machine learning approaches that learn useful representations 
for various tasks from (close to) raw input.

The expression "representation learning" is somewhat synonymous with "deep learning" at the time
of writing this thesis. When mentioning representation learning in the deep learning context
it is usually meant that the goal is to learn a function from raw input to target labels. In the 
context of this thesis, however, the emphasis is on learning representations of words, phrases and 
sentences that are potentially \emph{generally useful}, meaning 
that they can be used as input to in many tasks. This is sometimes referred to as 
\emph{transfer-learning} where we seek to indentify unsupervised learning objectives, 
supervised tasks, self-supervision schemes or the combinations of these to  learn representations that
perform well on a large variety of problems. 

\section{Learning representations of words}

%\begin{quote}
%Is there a way, however, to represent text in a more general way?
%\end{quote}

Most attempts to build  general representations for words are based on the 
\emph{distributional hypothesis} of word-meaning. It states that the degree to which words are similar 
is a function of the similarity of the linguistic contexts they appear in. In other words, similar 
words appear in similar contexts. Computational models of distributional
semantics implement this intuition and learn real-valued word vectors based on co-occurrence
statistics in large text corpora.
To aid the reader with techincal and historical context 
on the topic Section~\ref{sec:count} introduces earlier \emph{count-based} 
methods building word-context co-occurrence vectors-space. 
Section~\ref{sec:pred} presents the  \emph{prediction-based} framework in a more 
detailed fashion as the techniques discussed here are closely related
to the approaches presented in this thesis. 
Section~\ref{sec:w2v} details efficient-linear 
models for predictive word-learning for two main reasons: 1.) they a had a tremendous impact
on shaping the current landscape of continuous linguistic representation learning and are still
widely used at the time of writing this thesis, 2.) our main point of comparison between
our word learning model in Chapter~\ref{ch:TAL} is one of such models detailed in the section.

Word-representations within the latter framework are an instance on \emph{representation learning}:
word representations are learned through
optimizing model parameters to predict contexts from words or words from contexts.
Such learned word-representations have proven successful in many applications especially 
in recent-years, however, they are not \emph{realistic} in a certain sense. While they capture many
aspects of syntax and semantics of natural language they bare no relationship to the real-world
outside of large amounts of text-documents.
This leads us to the main topic of the thesis:  \emph{visual grounding} introduced in the
following section.

\section{Visually grounded word representations}

%\begin{quote}
%Can we learn useful linguistic representations just by reading texts?
%\end{quote}

Many theories of human cognition supported by empirical
evidence state that human language and concept representation and acquistion is \emph{grounded}
in perceptual and sensori-motor experiences. Cross-situational word learning, 
an influential cognitive account to human word-learning, supposes that humans learn the
meanings of words exploiting repeated exposure to linguistic contexts paired with preceptual reality.
Learning representations for linguistic units in a visually grounded manner brings
computational language learning systems closer to human-like learning. 
Such theoretical considerations are detailed in Section~\ref{sec:langperc}.

Furthermore, consider the applicability of distributional language represenations in the larger scope of Artificial 
Intelligence. One of the dreams of AI is to develop technology to power intelligent embodied agents
taking the form of office assistants or emergency aid robots. These machines cannot implement natural
language as an arbitrary symbol manipulation system akin to a calculator's understanding of 
magnitudes or slopes. 
Similarly to humans they need to link linguistic knowledge to the extra-linguistic world. 

Furthermore, certain aspects of meaning are better captured by \emph{encyclopedic}
knowledge such as domestication is abundant in textual data, however, \emph{perceptual} information
can provide complementary valuable insights into physical properties rarely mentioned in texts such as size,
shape and color. In practice harnessing the visual modality to learn language
representations that linking linguistic knowledge
to the external world  has been empirically shown to improve performance on several semantic tasks as
detailed in Section~\ref{sec:distvis}.

%Together the aforementioned cognitive 
%and practical considerations drove many works, including the Chapters of the present thesis, 
%to explore grounded linguistic representations.

In terms of computional modeling the jump from 
distributional to grounded models is conceptually simple: one needs to collect data where
the \emph{contexts} of linguistic units are \emph{extra-linguistic} and represent these contexts such
that they can be provided as input to representation learning algorithms.
More concretely in terms of extra-linguistic context the present thesis focuses on the \emph{visual-modality}.

Linguistic-visual multi-modal representations on the word-level have a well established albeit
somewhat breif history (Section~\ref{sec:distvis}). 
Such methods fall in both the \emph{count-based}
and \emph{prediction-based} framework using computer-vision techniques to represent the 
\emph{visual modality} and NLP methods to represent texts. 
These separate spaces are then combined into a single multi-modal representation.

As the \textbf{first contribtuion} of the thesis in Chapter~\ref{ch:TAL} we present an 
incremental cross-situational model of word-learning introducing modern computer-vision 
techniques to computational cross-situational modeling of  human language learning. 
Through our experiments we show that our presented model is competitive with state-of-the-art
\emph{prediction-based} distributional models and that our model can name 
relevant concepts given images.

\section{Visually grounded sentence representations}

When moving from \emph{atomic} words to the \emph{compositional} world of sentences we need flexible
models that can represent word-order and hierarchical relationships.  In Chapters~\ref{ch:COLI}, \ref{ch:IJCNLP}  \ref{ch:ConLL} and \ref{ch:EMNLP} 
we use Recurrent Neural Networks to represent sentences; a powerful class of  sequence models.
Section~\ref{sec:sentences} provides the reader with 
historical and technical background to the considerations behind this choice.

The study of general sentence representation learning has a much briefer history then word-representations
and Section~\ref{sec:trans-sentence} situates the reader in the area. Most of these approahces to date are 
based also on the distributional hypohtesis and formulate general purpose representation learning as a sort of
linguistic context prediction, but on the sentence level. 

Section~\ref{sec:visualsentences} describes the general framework of learning 
visually-grounded sentence representations and their utility. 
The basic idea is still context-prediction, however, we learn associations between 
sentences and their \emph{visual} contexts i.e model parameters are optimized such 
that realted image--sentence pairs get pushed close together and unrelated pairs far 
from each other in a learned joint space.

As the \textbf{second contribution} of this thesis in Chapter~\ref{ch:COLI} 
we train such an architecture and explore the learned representations.
Our main interest and contribution here is the development general techniques to 
\emph{interpret} linguistic representations learned by
Recurrent Neural Networks and use these tools to contrast text-only language 
models with their grounded counter-parts trained on the same sentences.

\section{Visual modality briding between languages}

One of the intriguing aspects of using the visual modality as a naturally occuring 
meaning representation is that it is also naturally \emph{universal} across languages. 
The visual modality anchors linguistic representations to perceptual reality, but also
also provides a natural bridge between various languages. Linguistic utterrances that are similar 
to each other, intuitively, appear in the context of perceptually similar scenes across languages. 

Utterrances in multiple languages and corresponding perceptual stimuli
can be conceptualized as \emph{mutliple-views} of the same underlying \emph{ideal datapoint}.
Learning to map these multiple views in the same feature-space can lead to better representations as
they have to be more \emph{general} due to the model having to solve multiple tasks at the same time. 
This \emph{multi-view learning} perspective is explained in more detail in Section~\ref{sec:multiview} 
focusing on the specific case of multi-modal and multi-lingual representations we explore in 
Chapter~\ref{ch:ConLL} and \ref{ch:EMNLP}.

The visual modality as pivot on the word-level can be used to find possible translations 
for words when no dictionary is available. Section~\ref{sec:bilinglex} provides the reader with more
detail on bilingual lexicon induction using the visual modality. 
Extending this idea to sentence level gives rise to techniques that
use the visual modality as a pivot to translate full sentences. Approahces in this direction are discussed
in Section~\ref{sec:imgpivot}. 

The \textbf{third contribution} in the thesis combines visually grounded sentence representation learning
with machine translation. More specifically in Chapter~\ref{ch:IJCNLP} we present a \emph{multi-task} 
learning architecture that jointly learns to associate English sentences with images and to translate from
English to German. We show that visually grounded learning improves translation quality in our domain and that 
it provides orthogonal improvements to having extra parallel English-German parellel corpus.

The \textbf{fourth contribution} of this thesis learns visually grounded sentence representations for 
multiple languages jointly.  In Chapter~\ref{ch:ConLL} we show that better grounded 
representations can be learned by training on multiple languages. We find a consistent pattern
of imrpovement whereby multilingual outperform bilingual, which outperform monolingual grounded
sentence representations. Furthermore, we provide empirical evidence that the quality of 
visually grounded sentence embeddings on lower -resource languages can be improved by jointly training
together with data sets from higher-resource languages.

Lastly, our \textbf{fifth contribution} in Chapter~\ref{ch:EMNLP} is exploring the benefit of multilinguality
in visually grounded representation learning as in Chapter~\ref{ch:ConLL}, but in the cross-domain setting.
Here we consider a \emph{disjoint} scenario where the image--sentetnce data sets for different languages do
not share images. We assess how the method applied in Chapter~\ref{ch:ConLL} performs under 
domain-shift. Furthermore, we introduce a technique we call \emph{pseudopairs}, whereby we generate
new image--caption data sets by creating pairs across data sets using the sentence similarities under
the learned representations. We find that even though this technique does not require any additional 
external data source, models or other pipeline elements it improves image--sentence ranking performance.


\begin{comment}
\section{Background: Distributed linguistic representations}
\label{background}
As opposed to the continuous nature of the visual world, linguistic units
are inherently discrete. This is reflected in the techniques used in the
computational processing of natural language.
Information in linguistic utterances is traditionally encoded
in sparse high-dimensional count vectors,  each
dimension corresponding to the number of times a specific feature occurs in a
sample. For each word in a sentence common features to consider take the form of:
``this token is a noun'', ``the preceding word is cat'',
``the following token has dependency label direct object''.
More often then not these vectors do not even hold counts, but 0-1
indicator features signaling the presence or absence of a particular property.

In such a setting the relationships between features are not handled.
As a result machine learning algorithms are presented with an input representation where
``the preceding word is cat'' and ``the preceding word is dog''
are completely independent properties.
Furthermore, interactions between features has to be
explicitly represented as a separate entry in the vector, which once again has
no clear relationship to its parts as far as the learning algorithms are concerned.

Let us consider for a simple example the problem of language modeling.
Our  aim is to find a probabilistic model
that assigns high probability to likely sequences and low probability to gibberish.
The representation of words and phrases in common n-gram language models is on the
very extreme of discrete: they are represented by their \emph{identity}.
Imagine that in the training set there were a number of sentences expressing the concept
of ``someone is walking a dog'', a little less mentions of ``someone walking a cat''
and no occurrence of ``someone walking a serval'', but the word ``serval'' did occur.
I have never seen anyone walking a serval, however, as a human I do know that it is a dog-sized
feline creature and as a consequence I can imagine someone taking a walk with one.
However, a language model trained with discrete symbolic representation cannot
\emph{generalize} in such a way as the
relationship between the words ``dog'', ``cat'' and ``serval'' is not represented.

Consider now the case of walking a dolphin.
Both ``walking a serval'' and ``walking a dolphin'' would be impossible
by the hypothesized language model, however, us humans would say that walking the serval is
kind of possible, but walking the dolphin is just surreal.
We know it from reading about both animals that they can be domesticated
, however, by their physical properties while one affords to be walked
the other does not. Dogs, cats, servals and dolphins are similar in some
dimensions and different in others.
What the present thesis is concerned about is
\emph{learning continuous representations} of words, phrases up to sentences
from multiple sources of information to encode such useful properties.
Such representations allow machine learning algorithms to represent smooth
functions over linguistic inputs where the local neighborhoods of linguistic
units correspond to syntactic and semantic regularities.

\section{Grounded and multilingual representations}
The well established tradition of continuous word representations consider
features extracted from large text-corpora in a single language focusing on
co-occurrence statistics between words. 
This distributional approach to word meaning hypothesizes that semantically related words
tend to appear in similar contexts. This idea goes back in linguistics tradition to the
earlier days of American structuralism \citep{nevin2002legacy}. In the seminal paper
"Distributional structure" \citep{harris1954distributional} back in 1954 Harris already claims
that distribution should be taken as an explanation for word meaning and that similarity classes
can be constructed based on co-occurrence statistics.
\cite{cruse1986lexical} writes that 'It is  assumed  that  the  semantic properties  of
a lexical  item  are  fully  reflected  in  the  appropriate  aspects  of  the  relations
it  contracts  with  actual  and  potential  contexts' and that 'the  meaning  of  a word
is constituted  by  its  contextual  relations'. Computational models of distributional
semantics implement this intuition and learn real-valued word vectors based on co-occurrence
statistics in large text corpora. Section~\ref{sec:words} presents 
such representations based on the \emph{count-based} in Section~\ref{sec:count}
versus \emph{prediction-based} in Section~\ref{sec:pred}
distinction borrowed from \cite{baroni2014don}. Most architectures presented  
in this thesis are based on the developements in Neural Language Models detailed
in Section~\ref{sec:NNLM}. Here we present the methods focusing on the historical context,
but combined with some standard notation
providing background for later chapters. Section~\ref{sec:w2v} introduces efficient-linear 
models for predictive word-learning for two main reasons: 1.) they a had a tremendous impact
on shaping the current landscape of continuous linguistic representation learning and are still
heavily used at the time of writing this thesis, 2.) our main point of comparison between
our word learning model in Chapter~\ref{ch:TAL} is one of such models detailed in the section.


My thesis focuses integrating the \emph{visual modality} and jointly learning representations 
of words up to sentences in \emph{multiple languages}. Furthermore, I study the beneficial interaction of
visually grounded and multilingual representation learning.
Grounding linguistic representattions in visual modality is largely motivated by evidence from
perceptual grounding in human concept acquisition and representation \citep{barsalou2003grounding}.
As such it brings computational language learning systems closer to human-like learning.
Furthermore, as pointed out by the dolphin versus serval example in the Preface
certain aspects of meaning are better captured by \emph{encyclopedic}
knowledge such as domestication abundant in textual data, however, perceptual information
can provide complementary valuable insights into physical properties rarely mentioned in texts such as size,
shape and color. In practice harnessing the visual modality to learn language
representations that link linguistic knowledge
to the external world \citep{kiela2014improving,baroni2016grounding,elliott2017imagination,kiela2017learning,yoo2017improving}
has been empirically shown to improve performance on several semantic tasks.

\paragraph{Images bridging languages}

Traditionally, given a task for each language separate corpora are created and
separate sets of parameters are learned.
Learning shared cross-lingual representations, however, allows researchers
and practitioners to train a single model allowing to transfer knowledge
between languages leading to practical consequences
such as mitigating the low-resource problem, cross-lingual applications and
processing data with code-mixing.
Perceptual grounding and multilingual representations have their separate benefits,
however, they can be beneficial to each other as well.
In my work I view images as universal meaning representations, a natural common
ground bridging between languages. From another perspective images and multiple
languages can be seen as multiple latent views of the same underlying semantic
concepts. This approach allows us to take advantage of complementary information
and potential transfer between these views.

\paragraph{Contributions}
In Chapter~\ref{ch:TAL} we start by learning visual representations for words using a
novel computational cognitive model of cross-situational word learning that
takes words and high-level continuous image representations as input. Our approach
integrates recent advances of computer vision into incremental cognitive models
of language learning. We show that on the data sets that we consider our model
is competitive with state-of-the-art distributional semantics models (word2vec)
on word-similarity benchmarks. Furthermore we show that our model is able to
name relevant concepts given images.

Chapter~\ref{ch:COLI} introduces a recurrent and convolutional neural network
based model that learns from both visual-grounding signals and
word-word co-occurrences. We develop techniques to interpret the learned
representations of such an architecture and investigate if certain linguistic
phenomena is encoded in the learned model.

Chapter~\ref{ch:IJCNLP} provides empirical evidence that grounded learning
can improve machine translation quality.
In Chapter~\ref{ch:ConLL} we show under what conditions multilinguality an help improve grounded
representations.
%Multilingual systems also offer a fertile ground computational typology as shared models are forced to exploit cross-lingual regularities.

%My work focuses on learning grounded distributed sentence representations.
%The first half of the thesis is about learning the representations of words and followed-up by learning sentence embeddings for a single language. Later we move on to learning multilingual visually grounded representations. In other works of mine I contributed to generating referring expressions grounded in Wikipedia entities and learning language through interacting with text-games grounded in a knowledge-base.

\end{comment}

\chapter{Background}
\label{ch:background}

\section{Distributed word-representations}
\label{sec:words}
The distributional approach to word meaning hypothesizes that semantically related words
tend to appear in similar contexts. This idea goes back in linguistics tradition to the
earlier days of American structuralism \citep{nevin2002legacy}.  
As early as in 1954 in his seminal paper "Distributional structure" Harris claims
that distribution should be taken as an explanation for word meaning and that similarity classes
can be constructed based on co-occurrence statistics \citep{harris1954distributional}.
\cite{cruse1986lexical} writes that "It is  assumed  that  the  semantic properties  of
a lexical  item  are  fully  reflected  in  the  appropriate  aspects  of  the  relations
it  contracts  with  actual  and  potential  contexts" and that "the  meaning  of  a word
is constituted  by  its  contextual  relations". Computational models of distributional
semantics implement this intuition and learn real-valued word vectors based on co-occurrence
statistics in large text corpora. Here we present such representations using the \emph{count-based}--\emph{prediction-based} distinction borrowed from \cite{baroni2014don}.
%One of the first computational verification
%attempts of  this distributional hypothesis was put forward in \cite{miller1991contextual} \todo{say something about the paper man}.

\subsection{Count-based approaches}
\label{sec:count}

Early computational linguistics models of distributional semantics fall in the category of
count-based approaches: they store counts of the number of times target words appear in different
contexts. In the  resuilting co-occurrence matrix
$\mathbf{W}$ each row $\mathbf{W_i}$ represents a 
word $i$ and each column $\mathbf{W_,j}$ a context. It has size
is a $\mathbf{W} \in \mathbb{N}^{|V| \times |C|}$ where $V$ is the set of words and $C$ is
the set of contexts.
The entry $\mathbf{W}_{i,j}$ is the number of times word $i$ appears in context $j$.
Contexts are typically words appearing within a certain window size or text documents.
To the counts in $W$ various re-weighting schemes are applied followed by some matrix factorization algorithm
resulting in a lower dimensional dense representation $\mathbf{W} \in \mathbb{R}^{|V| \times d}$, 
where $d < |C|$.
%Both re-weighting and low-rank approximation reduces the dimensionality of the sparse matrix and leads
%to faster compute and better generalization.

The earliest approaches include Hyperspace Analogue to Language \citep{lund1996producing},
which constructs a term-term co-occurrence matrix and Latent Semantic Analysis \citep{dumais2004latent},
which applies the tf-idf re-weighting scheme on a term-document matrix 
followed by singular-value decomposition.
More recent approaches apply different re-weighting schemes such as 
point wise mutual information \cite{bullinaria2007extracting}  and local mutual
information \citep{evert2005statistics} or different matrix factorization algorithms such as non-negative
matrix factorization \citep{baroni2014don}.\footnote{For a comprehensive set of empirical experiments on count-based
approaches please consult \cite{bullinaria2007extracting} and \cite{bullinaria2012extracting}.} 
\subsection{Prediction-based approaches}
\label{sec:pred}
In more recent years
%-- and more related to the present thesis --
various deep learning methods have been applied to learn distributed word-representations usually referred to
as \emph{word-embeddings} in the literature. 
Contrary to count-based methods prediction-based approaches fit into the
standard machine learning pipeline: they optimize a set of parameters to maximize the probability of words
given contexts or contexts given words, where the word-embeddings $\mathbf{W}$
themselves form a subset of the parameters of the full model. 

\subsubsection{Neural language models}
\label{sec:NNLM}
Laying down the framework for recent developments the first modern approach to learn 
distributed word-representations from realistic data
was the neural language model introduced by \cite{bengio2003neural}.
They present a feed-forward multilayer perceptron with continuous word-embeddings,
a single hidden layer and a softmax output layer.
More precisely the model is parametrized by a 1.) word-embedding matrix
$\mathbf{W} \in \mathbb{R}^{|V| \times d}$, where each row corresponds to a word-vector
$\mathbf{W_i}$ of size $d$ and each column $\mathbf{W_{,j}}$ to a learned feature, 2.)
hidden and output weight-matrices $\mathbf{H}$ and $\mathbf{U}$. Similarly
to the case of count-based embeddings $d < |V|$.
The network takes as input the concatenation of $n$ word-vectors preceding the target word as context
representation $\hat{\mathbf{w}} = [\mathbf{w_1}; \ldots; \mathbf{w_{n-1}}]$
and outputs the probability distribution over the current word $w_n$.
The computation of the network is shown in Equation~\ref{eq:bengio}:

\begin{equation}
\label{eq:bengio}
P(w_n|w_{<n}) \approx \text{softmax}(\mathbf{U} \text{tanh}(\mathbf{H} \mathbf{\hat{w}} + b_h ) + b_u)
\end{equation}

where tanh is the hyperbolic tangent function applied elemetwise

\begin{equation}
\label{eq:tanh}
\text{tanh(x)} = \frac{e^{2x} - 1 }{e^{2x} + 1 }
\end{equation}

and softmax is a commonly used function to map a vector of unnormalized scores to a
categorical probability distribution:

\begin{equation}
softmax(\mathbf{x}) = \frac{e^{x_{i}}}{\sum_{k=1} e^{x_k}}
\end{equation}

The model is trained to maximize the probability of
the target word given the previous fixed number of words as context over a training corpus
-- $n$-gram language model -- trained with stochastic gradient descent \citep{cauchy1847methode}
through the backpropagation algorithm \citep{rumelhart1985learning}.
The superior performance of the feed-forward neural language model has soon been shown to
improve performance in speech recognition \citep{schwenk2005training}.

Following a similar recipe the convolutional architecture of \cite{collobert2008unified}
based on the time-delay neural network model \citep{waibel1990phoneme} took several steps towards the
by now standard practices in deep NLP.
Contrary to the feed-forward network the convolution over-time structure can handle
sequences of variable length. This is essential for NLP applications where typically sentences
are composed from a varying number of words.
They introduces the idea of jointly learning many linguistic tasks at the same time such as part-of-speech
tagging, chunking, named entity recognition and semantic role labeling through \emph{multi-task learning}.
Their architecture was later refined in \cite{collobert2011natural} and the pre-trained
full model was made available alongside the standalone word-embeddings in the SENNA toolkit.\footnote{\url{https://ronan.collobert.com/senna/}}
Finally, \cite{collobert2008unified} were the first to show the utility of pre-trained word-embeddings
in other tasks through transfer learning.

We also make extensive use of the multi-task learning strategy:  Chapters~\ref{ch:COLI} for
a language modeling and image--sentence ranking, in Chapter~\ref{ch:IJCNLP} for machine translation
and image--sentence ranking and in Chapters~\ref{ch:ConLL} and \ref{ch:EMNLP} for image--sentence
and sentence--sentence ranking in multiple languages.


The architecture, however, that arguably became most popular in NLP -- which we also use in
Chapters~\ref{ch:COLI}, \ref{ch:IJCNLP}, \ref{ch:ConLL} and \ref{ch:EMNLP} --
is the recurrent neural network (RNN). It was 
first introduced by the late Jeffrey L. Elman in his seminal paper  'Finding Strucutre in Time'
as a model of human language learning and processing \citep{elman1990finding}. Recurrent neural networks
take a variable length sequence as input and perform a stateful computation over it's elements.
essentially "reading" the input from left-to-right.
In case of language modeling at each time-step $t$ the network takes an input word vector $\mathbf{w_t}$
and the it's previous hidden state $\mathbf{h_{t-1}}$ maintained through previous 
time-steps. These are used to compute the current
state $\mathbf{h_t}$ and to predict the probability distribution over the following word $P(w_t|w_{<t})$.
It is parametrized by a word-embedding matrix $\mathbf{W} \in \mathbb{R}^{|V| \times d}$, 
an input to hidden weight matrix $\mathbf{W_i}$, a hidden to hidden weight 
matrix $\mathbf{W_h}$ and finally the hidden to output
weight matrix $\mathbf{U}$ to predict the unnormalized probabilities over the vocabulary entries:

\begin{align}
P(w_n|w_{n<n}) &\approx \text{softmax}(\mathbf{U} \mathbf{h_t} + \mathbf{b_o}) \\
\mathbf{h_t} &= \text{tanh}(\mathbf{W_h}\mathbf{h_{t-1}} + \mathbf{W_i}\mathbf{w_t} + \mathbf{b_h})
\end{align}

where $ \mathbf{b_o}$ and $ \mathbf{b_h}$ are bias terms.

This model is also trained to maximize the probability of the traning sequences, but rather than standard
backpropagation it is optimized with the backpropagation through time algorithm (BPTT)
\citep{robinson1987utility,werbos1988generalization,williams1995gradient} developed for RNNs.

\cite{elman1991distributed} shows when trained on simple natural language like input the
hidden states of the network $\mathbf{h_t}$ encode grammatical relations and hierarchical
constituent structure. In Chapter~\ref{ch:COLI} we also train a recurrent language model
and compare it to a its grounded counterpart 
on real-world data and explore similar questions about the learned opaque representations 
as \cite{elman1991distributed} .

Despite the early successes, however, it turned out to be difficult to train
Elman networks on practical applications with longer sequences
due to the vanishing and exploding gradient phenomena \citep{bengio1994learning}.
It  wasn't until the RNNLM implementation of \cite{mikolov2010recurrent}\footnote{\url{http://www.fit.vutbr.cz/~imikolov/rnnlm/}}
established a new state-of-the-art on language modeling that RNNs regained their popularity
in language processing.  Interestingly the successful
application the standard BPTT algorithm
to simple RNNs on real world language modeling \cite{mikolov2012statistical}
seems  only to have required more computational resources and 
the now widely used gradient clipping trick. Still at the time of writing this thesis RNNs are
trained with BPTT with various versions of stochastic gradient decent and smart initialization
strategies, while more complex training algorithms such as hessian-free optimizers 
\citep{martens2011learning} remain out of fashion.

At the time of writing this thesis an overwhelming amount of empirical evidence
shows that new RNN variants the long short-term 
memory (LSTM) \citep{hochreiter1997long,gers1999learning}
and gated recurrent unit networks (GRU) \citep{cho2014learning}
vastly outperform the simple Elman network in practice. We use GRUs in
Chapters~\ref{ch:COLI}, \ref{ch:IJCNLP}, \ref{ch:ConLL} and \ref{ch:EMNLP}.

Gated recurrent unit networks use a particular \emph{memory structure}, which
adds an inductive bias to the network to make it easier to learn to \emph{carry information}
over time by keeping a portion of the components of the hidden state $\mathbf{h_t}$
approximately constant at each step:

\begin{align}
\tag{update-gate}
\mathbf{z_t} &= \sigma(\mathbf{W_z} \mathbf{w_t} + \mathbf{U_z} \mathbf{h_{t-1}} + \mathbf{b_z}) \\
\tag{reset-gate}
\mathbf{r_t} &= \sigma(\mathbf{W_r} \mathbf{w_t} + \mathbf{U_r} \mathbf{h_{t-1}} + \mathbf{b_r})  \\
\tag{memory content}
\mathbf{\tilde{h_t}} &= \text{tanh}(\mathbf{W_h} \mathbf{w_t} + \mathbf{r_t} \odot  \mathbf{U_h} \mathbf{h_{t-1}}) \\
\tag{hidden state}
\mathbf{h_t} &= (1-\mathbf{z_t}) \odot \mathbf{h_{t-1}}  \mathbf{z_t} \odot \mathbf{\tilde{h_t}}
\end{align}

where $\sigma$ is the sigmoid function:

\begin{equation}
\sigma(x) = \frac{e^x}{e^x + 1}
\end{equation}

As the sigmoid function has a range of $[0,1]$ it is intuitive to undestand the GRU as a sort of
sequential computer with soft/continuous read-write memory operations: the reset-gate $\mathbf{r_t}$
decides how much of each component of the previous state is relevant to be mixed in with the
current input, resulting in the current candidate memory state $\mathbf{\tilde{h_t}}$. The output-gate
then trades of the previous state with the current candidate.



\subsubsection{Efficient linear models}
\label{sec:w2v}
Despite the success of deep learning architectures in NLP it wasn't until the introduction 
of the much simpler and fast continuous bag-of-words
and skip-gram with negative sampling (SGNS) algorithms \cite{mikolov2013efficient}
packaged into the easy to use \texttt{word2vec} 
toolkit\footnote{\url{https://code.google.com/archive/p/word2vec/}} that
word-embeddings became ubiquitous in computational linguistics and NLP research.
These algorithms rely on simple log-linear models as opposed to the more expensive
neural networks leading to faster training on larger corpora. The more successful SGNS
model has two learnable word embeddings matrices one for the target words
$\mathbf{W_w}  \in \mathbb{R}^{|V| \times d}$
and a separate one for the contexts $\mathbf{W_c} \in \mathbb{R}^{|V| \times d}$.
The model is trained to maximize the dot product $\mathbf{c}^T\mathbf{w}$
between true word-context pairs $(w,c)$ appearing in corpus and minimize the
dot product $\mathbf{\hat{c}^T\mathbf{w}}$ between randomly
samples contrastive examples $(w,\hat{c})$ through  maximizing the following loss function

\begin{align}
J(\theta) &= \prod_{(w,c) \in D} p(D=1|w,c;\theta)  \prod_{(w,c) \in D'}  p(D=0|w,c;\theta) \\
&= \sum_{(w,c) \in D} \log  \sigma(\mathbf{c}^T\mathbf{w}) \sum_{(w,c) \in D'}  \log \sigma(-\mathbf{c}^T\mathbf{w})
\end{align}

where  $p(D=1|w,c)$ is the probability that $(w,c)$ is from the true corpus, $p(D=0|w,c)$ is the
probability that it did not and $D'$ is the articial corpus of negative pairs.
Similarly to this negative sampling loss function the ranking objectives 
implemented in the approaches in Chapters~\ref{ch:IJCNLP} \ref{ch:ConLL} and ~\ref{ch:EMNLP}
and described in detail in Section~\ref{sec:visualsentences} force the model
to match images $i$ and corresponding sentences $s$ and push contrastive examples $(\hat{i},s)$ 
and $(i,\hat{s})$ far from each other in the learned multi-modal space.

Finally the GloVe approach \citep{pennington2014glove} is another simple linear model
with word $\mathbf{W_w}  \in \mathbb{R}^{|V| \times d}$ and context
$\mathbf{W_c} \in \mathbb{R}^{|V| \times d}$ embeddings, which
represents a hybrid between count- and prediction-based techniques:
it optimizes word-embeddings to predict the re-weighted
$\log$ co-occurrence counts $\log(\#(w,c))$ collected from corpora $D$:

\begin{equation}
\label{eq:glove}
\mathbf{w}^T\mathbf{c} + b_w + b_c = \log(\#(w,c))\;\;\; \forall (w,c \in D)
\end{equation}

where $b_w$ and $b_c$ are context specific bias terms.
Now let us express the Glove model in the traditional re-weighted
co-occurrence matrix factorization form.
Let the matrix $M \in \mathbb{R}^{|V| \times |C|}$ store the log-weighted
weighted counts and $\mathbf{b_w}$ and $\mathbf{b_c}$ be the vectors of all
$b_w$ and $b_c$ scalars, then

\begin{equation}
\label{eq:glove2}
M \approx \mathbf{W} \cdot \mathbf{C^T} + \mathbf{b_w} + \mathbf{b_c}
\end{equation}

In fact there has been work on finding relationships between
count- and prediction-based methods \citep{levy2014neural} and
using insights from both to develop novel improved
variants \citep{levy2015improving}.

%\paragraph{Transferring word representations}

%Later the word-embeddings $\mathbf{W}$ learned by recurrent language models were
%shown to represent intriguing syntactic and semantic regularities \cite{mikolov2013linguistic}.

%In earlier work where pre-trained word-embeddings were also shown for the first time through a
%larger-scale empirical investigation by \cite{turian2010word} to transfer to sequence prediction tasks
%when used as features in state-of-the-art systems.


\section{Visually grounded representations of words}
\label{sec:visualwords}
% \todo{ADD MORE FROM THIS https://arxiv.org/pdf/1806.06371.pdf}

\subsection{Language and perception}
\label{sec:langperc}

The discussion of learning distributed representations so far has focused on 
implementations of the distributional hypothesis and extracting information exclusively
from text corpora. To human language learners, however, a plethora of 
perceptual information is available to aid the
learning process and to enrich their mental representations. 
The link between human word and concept representation
and acquisition and the perceptual-motor systems has been well established through behavioral
neuroscientific experiments \citep{pulvermuller2005brain}.
The earliest words children learn tend to be names of concrete perceptual phenomena
such as objects, colors and simple actions \citep{bornstein2004cross}. Furthermore, children generalize to
the names of novel objects based on perceptual cues such as shape or color \citep{landau1998object}.
In general, the \emph{embodiment} based theories of concept representation and acquisition in the
cognitive scientific literature put forward the view that a wide variety of cognitive processes
are grounded in perception and action \citep{meteyard2008role}. That said, the precise role of
sensori-motor information in language acquisition and representation is a highly
debated topic \citep{meteyard2012coming}.

Motivated by such cognitive theories and experimental data 
various computational cognitive models of child 
language acquisition investigate the issue
of learning word meanings from small scale or synthetic multi-modal data. The model presented by
\cite{yu2005emergence} uses visual information to learn the meanings of object
names whereas the architecture of \cite{roy2002learning} learns to associate word sequences
with simple shapes in a synthetically generated data setting.

Interestingly even the articles introducing Latent Semantic Analysis
\citep{landauer1997solution} and Hyperspace Analogue to Language \citep{lund1996producing} mention
that a possible limitation of the presented distributional semantic models is the lack of
grounding in extra-linguistic reality. \cite{landauer1997solution} puts it as "But still, to be more than
an abstract system like mathematics words must touch reality at least occasionally."
The lack of relationship between symbols and the external reality is usually referred
to as the \emph{grounding problem} in the literature \citep{harnad1990symbol,perfetti1998limits}.
On the defense of purely textual models \cite{louwerse2011symbol} argues that the corpora
used to train distributional semantic models are generated by humans and as such reflect the perceptual world.
For a counter argument consider how many pieces of text state obvious perceptual
facts such as "bananas are yellow" or how often do objects holding the "yellow" property
appear in similar textual contexts \citep{bruni2014multimodal}.

In practice much work on multi-modal distributional semantics have found that text-only spaces tend to
represent more encyclopedic knowledge, whereas multi-modal representations capture more concrete aspects
\citep{andrews2009integrating,baroni2008concepts}. In Chapter~\ref{ch:TAL}, where we develop
a cross-situational cognitive model of word-learning, we also find that the word-representations learned by
our model correlate better with human similarity judgements on more concrete words rather than.
In contrast word-embeddings learned by the SGNS algorithm trained on the same sentences
perform better on more abstract words.

One does not necessarily need to reach a conclusion on whether grounded or distributional 
models are superior;  combining their merits in a
pragmatic way is an attractive alternative \citep{riordan2011redundancy}. 

%Learning representations
%from both linguistic and visual input is a step towards realistic models of language learners.
%Note however, it is as close as assuming children learn language by sitting still and watching TV.

\subsection{Combined distributional and visual spaces}
\label{sec:distvis}

When learning multimodal word representations we wish to construct a matrix $\mathbf{W}$, 
where each row $\mathbf{W_i}$
corresponds to a word and each feature column $\mathbf{W}_{,j}$ 
represents a distributional feature, a perceptual feature or a mixture of the two.

The first approach to learn visual word representations from realistic data sets
was introduced by \cite{feng2010visual}. They develop a multi-modal topic model trained on a
BBC News based data set containing text articles with image illustrations.
Each document considered by their model
is a pair $<D, I>$ with text document $D$ and image $I$.
The documents $D$ are represented as bag-of-words vectors (BoW) -- as common in the
\emph{count-based} distributional semantics framework -- while images are represented
as bag-of-visual-words (BoVW)  \citep{csurka2004visual}.  The BoVW representations are
obtained by first applying the difference-of-Gaussians point detector to segment images
into local regions, which are then represented by SIFT features \citep{lowe1999object}.
Finally, these extracted features are clustered with K-means resulting in the
bag-of-visual-words  representation i.e.
a vector of counts, where each entry is the number of regions on the image that
corresponds to a certain SIFT-cluster.  Given both the BoW  $\psi(D)$ and
BoVW $\phi(I)$ feature functions each pair $<D, I>$ can be represented as concatenation
of the application of the two $[\psi(D), \phi(I)]$.

 This if followed by training multi-modal
 topic model, specifically Latent Dirichlet Allocation \citep{blei2003latent}
on top of the joint representations of the documents and images.
The hierarchical bayesian generative story of multi-modal LDA  starts by sampling multi-modal topics,
which then generate both the English words and visual words.
After convergence each word is represented by a vector,
where each component corresponds to the conditional probability of that word given 
a particular multi-modal topic. \cite{feng2010visual}
shows that the multi-modal LDA model outperforms the text-only LDA representations by a
large-margin on word association and word-similarity experiments.

The perceptually grounded word representations of \cite{bruni2012distributional}
also combines distributional semantic models and BoVW pipelines.
Rather than having a collection of documents $<D, I>$ they consider a set of words for, which
both distributional and image features are available. For visual word representations
they use images labeled with tags by annotators. Similarly to \cite{feng2010visual} they
apply the BoVW feature extraction pipeline with SIFT and color features to
represent images. Contrary to \cite{feng2010visual}, however,
a visual-only representation is created for each tag-word by summing over the features
for all images corresponding to the tag.
For text-only models they construct several types of distributional semantic spaces
from text-only corpora unrelated to the images.
Note that as in the previous example they wish to create a multimodal
word representation and applied separate functions to extract textual $\psi$ and visual
$\phi$ features. Finally they create the multi-modal space through concatenation.

On word-similarity benchmarks they show that the text-only model performs better than visual-only
and that the combination of the two surpasses both.
They also find that distributional semantics
models perform poorly on finding the typical colors of concrete nouns,
whereas the visual and multi-modal models perform perfectly. In these experiments
distributional semantics models fail to capture the obvious fact that ``the grass is green''
providing evidence against the theoretical argument that perceptual information is available in
large collections of texts and so grounded representations are superfluous \citep{louwerse2011symbol}.


Combining BoVW and count-based distributional word-representations remained the standard methodology
in many other works on multi-modal word representations at the time
\citep{bruni2011distributional,leong2011going,leong2011measuring}.
\cite{bruni2014multimodal} frames multi-modal distributional semantics under a general framework: 
create separate textual and visual features for words followed by
re-weighting and matrix factorization. Researchers can apply vector-space
models to create the distributional space $\mathbf{W^d}$ and computer vision techniques to create
visual feature representations for the same words $\mathbf{W^v}$. The separate feature spaces
are mixed together  first  by concatenation $\mathbf{W} = [\mathbf{W^d};\mathbf{W^v}]$, which is
optionally followed by singular value decomposition
to combine the two modalities.\footnote{Note that discovering latent factors through the application
of signgular value decomposition on the concatenated textual and visual spaces is similar in spirit
to applying topic models such as LDA.} 

This framing allowes researchers to take advantage of evolving NLP and computer vision techniques
and experiment by plugging in various distributional semantics methods as textual feature extractors
$\phi$ and image processing techniques as $\psi$.
For example \cite{kiela2014learning} runs the SGNS algorithm
on large text corpora and takes the word-embedding parameters
$\mathbf{W^d} \in \mathbb{R}^{|V| \times d}$ as the distributional space and
they apply pre-trained convolutional neural networks (CNN) as black-box image
feature extractors. We apply CNNs as image feature extractors in all chapters.

These architectures learn a hierarchy of blocks of image filters followed by
pre-defined pooling operations optimized for a particular task.
It has been observed in the computer vision community that the lower layers of
deep CNNs trained across various data sets and tasks tend to learn filter maps
that resemble Gabor-filters and color-blobs. Intuitively these low-level features appear to be general
and as such afford \emph{transfer}. Around the time there was extensive work on exploring the transferability
of CNN features to various computer vision tasks through fine-tuning
 \citep{donahue2014decaf,oquab2014learning} or simply taking the last layer representation of CNNs
as high-level features as inputs to linear classifiers \citep{girshick2014rich,sharif2014cnn}.
Given their success in transfer learning for vision it is natural to apply CNNs in the 
visually grounded language learning community as black-box image feature extractors. 
Similarly to \cite{bruni2014multimodal} in
\cite{kiela2014learning} the visual features of words are computed as the summary the feature vectors
extracted from all images they co-occur with $\mathbf{I} \in \mathbb{R}^{|V| \times i}$, where $i$ is
the size of the CNN image-embedding.
Applying the simple concatenation operation to the two spaces $[\mathbf{W};\mathbf{I}]$
they show that CNN features outperform BoVW features on word-similarity
experiments with the multi-modal word-representations.

All approaches described so far require both visual and textual information for the same concepts.
Furthermore, in \citep{bruni2014multimodal} and \citep{kiela2014learning} textual and visual
representations are learned separately and are fused later.
The multi-modal skip-gram \cite{lazaridou2015combining} model was developed to alleviate such limiation:
it is a multi-task extension of the skip-gram algorithm predicting the both the context of words, but also
the visual representations of concrete nouns. This optimizes word representations to be predictive of
both their visual and textual contexts jointly. Ground truth visual representations are constructed by averaging
the CNN representations \cite{krizhevsky2012imagenet} of 100 pictures sampled from ImageNet \cite{deng2009imagenet}.
This architecture was later proposed as a model of child language learning and
was applied to the CHILDES corpus \cite{macwhinney2014childes} with modifications to model referential
uncertainty and social cues \cite{lazaridou2016multimodal} present in language learning. 
Later it was compared to human learning performance \cite{lazaridou2017multimodal}.


Chapter~\ref{ch:TAL} presents a computational cognitive model of word learning using only
visual information developed at the same time as the multi-modal skip-gram approach.
Similarly to \cite{feng2010visual} we assume pairs of text and images
$<D, I>$. However, in our data set images represent every day scenes and are paired with descriptive
sentences to mimic the language environment of children on a high level.
Our model implements the extreme literal of cross-situational language
learning and assumes that the representations of words are learned exclusively through the co-occurrences
between visual features and words. Rather than building a
matrix of image-features per word and then summarizing as in \cite{kiela2014learning},
we apply an online expectation-maximization based \citep{dempster1977maximum} 
algorithm to align words with image features mimicking child language learning.
In essence, our approach combines the cross-situational incremental word-learning model
of \cite{fazly.etal.10csj} with the larger realistic data sets, modern convolutional image representations
and extenends it to operate on real-valued scene representations.
The result of the learning process is a word embedding matrix $\mathbf{W_i}$,
where each row $\mathbf{W_i}$ corresponds to a word, each column $\mathbf{W_{,j}}$
to a CNN feature and each entry $\mathbf{W_{i,j}}$ to the strength
of the relationship between the word $i$ and image-feature $j$.
We show through word-similarity experiments that, while our approach performs on
par with the SGNS trained on the same data set, there is a qualitative difference between
the learned embeddings: our visual word-embeddings represent concrete nouns closer to human
judgements then abstract nouns. As each word embedding $\mathbf{W_i}$ is represented in the
image-feature space we show that our model can label images with related nouns through
simple nearest neighbor search.

\section{From words to sentences}
\label{sec:sentences}
Applying the distributional intuition to model the meaning of sentences is not as straightforward
as it is for words.
Imagine building a sentence-embedding matrix $\mathbf{W}$ where each row 
$\mathbf{W_i}$ corresponds to
a possible sentence $i$ and each column $\mathbf{W_{,j}}$ to a feature $j$.
How many rows should we include? 

Intuitively, the number of words in a corpus is much
lower than the number sentences: one can assume a large, but finite set
of existing words and an infinite set of
potential sentences to \emph{compose} from them.
Words can be thought of as \emph{atomic units} and
in downstream applications one can use a lookup operation on a word embedding matrix
to represent units in the input. However, it is infeasible to lookup full sentences.
In fact, from a method that represents sentences in a continuous space one would expect
to also represent and generalize to unseen sentences at test time.
Furthermore, given two sentences $\mathit{John \; loves \; Mary}$ and
$\mathit{Mary \; loves \; John}$ we wish our sentence encoding function
$\phi$ to represent the meaningful difference stemming from the underlying syntactic structure
$\phi(\mathit{John \; loves \; Mary}) \neq \phi(\mathit{Mary \; loves \; John})$.
As such we seek to learn a sentence encoder $\phi$ that is sensitive to
syntactic structure or in other words \emph{compositionality} i.e.:
the notion that the meaning of an expression is
a function of its parts and the rules combining them \citep{montague1970english}.\footnote{Similarly one could argue that \emph{morphemes} are atomic units from which words are composed from. Infact, sub-word
representations form an important field of research \citep{bojanowski2017enriching}.}

The compositional distributional semantics  framework  produce
continuous representations for phrases up to sentences using additive and multiplicative interactions
of count-based distributed word-representations \citep{mitchell2008vector} or combine symbolic and
continuous representations with tensor-products \citep{clark2007combining}.
The latter line of work culminated in a number of unified theories of
distributional semantics and formal type logical and categorical grammars
\citep{coecke2010mathematical,clarke2012context,baroni2014frege}.
These approaches assume that words are represented by distributional word embeddings
and define compositional operators on top of them motivated by particular formal
semantic considerations. From the point of view of theoretical linguistics arguably 
one of the most intriguing aspects of such theories of meaning is that 
they provide an elegant data driven solution to deal with the representation of the lexical entries of
content words -- nouns, verbs and adjectives. This is  otherwise a major challenge in formal semantics
requiring sophistitcated formalisms such as typed $\lambda$-calculus with intricate and vast
type-systems \citep{asher2011lexical}. Within applied NLP, however, this line of work have not resulted in
practical machine learning approaches to solve natural language tasks
on real world data sets. Likely this is due to the different scope and
the computationally expensive high-order
tensor operations involved \citep{bowman2016modeling}.

Lastly, before going forward with the more recent neural models in the next section,
it is only fair to mention that bag-of-words
based representations bypassing the issue of compositionality form a set of very strong baselines
for a number of sentence level tasks \citep{hill2016learning}.
These simple baselines include using multinomial-naive bayes uni- and bi-gram 
log-count features within support vector machines \citep{wang2012baselines} or
feeding the average of the word-embeddings in a sentence into a softmax classifier \citep{joulin2016bag}.


\section{Neural sentence representations}
\label{sec:trans-sentence}

In Section \ref{sec:words} we have discussed the feed-forward \citep{bengio2003neural}
and recurrent network \citep{mikolov2010recurrent} language models from the perspective
of learning word-representations. However, both architectures learn embeddings of not only single words,
but also learn to represent of sequences of multiple words such as sentences.
An intriguing property of such approaches is that
they represent various linguistic objects in the same space as activations of the neural models.\footnote{This property is not unique to neural models e.g. the pregroup algebra based compositional distributional
semantics framework of \cite{coecke2010mathematical} 
sentences of any grammatical structure live in the same inner-product space.} 
When learning transferable
sentence representations there are two main considerations we will discuss:
1.) which architecture to choose and 2.) what objective to optimize.

Various neural network architectures have been proposed that
handle variable sized data structures useful for language processing: a.) recurrent networks 
take the input sequentially one word or character at a time
b.) convolutional neural networks
\citep{kalchbrenner2014convolutional,zhang2015character,conneau2016very,chen2013learning}
process sequences in fixed sized n-gram patterns up to a large window,
c.) recursive neural networks \citep{goller1996learning,socher2011parsing,tai2015improved}
take a tree data structure as input such as a sentence according to the traversal of a
constituency
d.) graph neural networks operate on graphs \citep{marcheggiani2017encoding} such as
syntactic/semantic-dependency or abstract meaning representations.
All the aforementioned architectures take word-embeddings as input 
and compute fixed vectors for sentences. 
These representations are tuned to a specific task such as sequence tagging,
sentence classification, machine translation or language modeling.


In Chapters \ref{ch:COLI}, \ref{ch:IJCNLP}, \ref{ch:ConLL} and \ref{EMNLP} we decided to apply
recurrent neural networks as sentence encoders.
Recursive neural networks provide a principled approach to
compute representations along the nodes of constituency
or dependency parse trees \citep{socher2013recursive,socher2014grounded,le2015compositional,tai2015improved}.
In practice, however, these architectures require parse trees as input,
which makes them impractical for our mission of learning visually grounded
representations for multiple languages.
For each language considered, the training procedure requires finding a good
pipeline from text pre-processing to parsing to generate the input representations
for the networks. Furthermore, tree structures by nature do not
afford straightforward batched computation directly and tend to run dramatically
slower than recurrent or convolutional models. In terms of performance the jury
is still out, however, so far only modest improvements have been observed
over recurrent models on specific tasks in specific settings
\citep{li2015tree,tai2015improved}. For graph structured neural networks the
same argument holds. Two equally practical alternatives to RNNs
that operate on raw sequences are convolutional neural networks
\citep{bai2018empirical} and transformers \citep{vaswani2017attention} and both could
replace the RNNs in Chapters~\ref{ch:IJCNLP}, \ref{ch:ConLL} and \ref{ch:EMNLP}.



As mentioned earlier contrary to word-embeddings the representations
and the composition function these models learn are task specific and not \emph{universal}.
For learning transferable and general distributed sentence representations the
first notable approach is the
skip-thought vectors model \citep{kiros2015skip}.
It extends distributional semantics intuition to the sentence level:
they train a sentence encoder on contiguous sentence sequences
-- such as books -- and learns sentence representations predictive of the sentences around it.
More specifically
they train a recurrent encoder to encode sentence $s_i$ and use two separate recurrent decoders to
generate the pre-context $s_{i-1}$ and post-context $s_{i+1}$.
This method was confirmed to be a successful self-supervised method for transfer
learning in a number of sentence
classifications tasks beating simpler approaches such as bag-of-words approaches based on skip-gram
or CBOW embeddings, tf-idf vectors and auto-encoders \citep{hill2016learning}. A convolutional
variant of the encoder was introduced in \cite{gan2016unsupervised} and several other works
train simple sum/average pre-trained word-embedding encoders using the same sentential 
context prediction objective \citep{kenter2016siamese,hill2016learning}. The larger context of paragraphs is
explored in \cite{jernite2017discourse} where the task is to predict discourse coherence
labels in a self-supervised fashion.

Some later approaches have  moved away from distributional cues
and identifyied supervised tasks that lead to representations that transfer
well to a wide variety of other tasks. The task of Natural Language Inference \citep{bowman2015large,williams2017broad} as an objective
was identified to learn good sentence embeddings \citep{conneau-EtAl:2017:EMNLP2017,kiros2018inferlite}
and \cite{subramanian2018learning} combines a number of other supervised tasks
with self-supervised training through multi-task learning.


The state-of-the-art in learning universal sentence representations
at the time of writing is represented by neural language models 
with a large number of parameters
trained over huge corpora \citep{peters2018deep,devlin2018bert}.
These approaches go back to
exploiting only distributional cues and train a stack of convolutional and/or recurrent
and/or transfomer layers on large-scale language modeling.
When trasferring the networks to novel tasks these networks are either fine-tuned, a
separate smaller network is trained on top of their representations or they are used and
fixed feature extractors \citep{howard2018universal,peters2019tune}.

\section{Visually grounded sentence representations}
\label{sec:visualsentences}

Universal sentence representations are in general learned from text-only corpora. The most 
successful current trend being large-scale language modeling based on
the distributional semantics intuition of the general usefulness of linguistic context prediction.
However, this leaves the resulting sentence representations blind to the language-external 
reality leading to the grounding problem
as discussed in Section~\ref{sec:visualwords}. Given that visual information
has been shown to contain useful information for word-representations it
is a natural question to ask whether this observation generalizes to sentence
embeddings. The idea of context prediction coming from the distributional 
hypothesis can be adopted to visual grounding in a conceptualy straightforward manner:
train sentence embeddings to be predictive of their visual context. 

The larger family of techniques that our visually grounded sentence learning
approach technically belongs to is \emph{learning to rank} \citep{li2011learning}. 
Some of the earlier attempts at multi-modal ranking did not consider full sentences rather 
images paired with (mostly) noun tags such as \citep{weston2010large}.
%Another influential framework is the multimodal deep 
%boltzmann machine \citep{srivastava2012multimodal}; 
%a generartive model that can infer joint representations of images and tags or generate 
%tags given images. 
Framing the learning of multi-modal vision and language spaces as a ranking problem 
from images to descriptions and conversely from descriptions to images was 
put forward by \citep{hodosh2013framing}.  They argue that the evaluating visually grounded
image understanding through generation is plagued by the lack of straightforward performance measures.
This issue was later discussed by \citep{elliott2014comparing} showing low to moderate correlation
between automatic measures such as BLUE, METEOR and ROUGE with human judgements.
From the image--sentence ranking perspective  a joint space between 
language and vision reflects accurately 
the underlying semantics if given one modality as \emph{query} the other modality
can be accurately \emph{retrieved}.  This leads to a straighforward evaluation process
adopting metrics from information retrieval literature such as Recall@N, Precision@N, Mean Reciprocal
Rank, Median Rank or Mean Rank. From the practical point of view it also unifies image--annotation
and image--search.

The standard benchmark data sets  we used for this purpose 
annotate images found in online resources with descriptions through crowd-sourcing.
These descriptions are largely \emph{conceptual}, \emph{concrete}
and \emph{generic}.
This means that descriptions do not focus too much on \emph{perceptual}
information such as colors, contrast or picture quality; they do not mention
many \emph{abstract} notions about images such as mood and finally the descriptions
are not \emph{specific} meaning that they do not mention the names of cities,
people or brands of objects.
What they do end up mentioning are entities depicted in the images
(frisbee, dog, boy) their attributes (yellow, fluffy, young) and the
relations between them.
The images depict common real-life scenes such as a \emph{bus turning left} or \emph{people
playing soccer in the park}. As such, annotation collected independently from
different crowd-source workers end up focusing on different aspects of 
these scenes\footnote{For a comprehensive overview on image-description data sets
please consult \cite{bernardi2016automatic}}.

Following the ranking formulation of grounded learning by \citep{hodosh2013framing}
the earliest works applying a combination of sentence and image encoder neural networks
to image--sentence ranking was put forward by \cite{kiros2014unifying} and \cite{socher2014grounded}:
they both apply pre-trained convolutional neural networks as image encoders and while 
\cite{socher2014grounded} use recursive neural network variants to encode sentences 
\cite{kiros2014unifying} apply recurrent networks. Rather than mathcing whole images with full
sentences the alternative approach of learning latent alignments between image--regions and
sentence fragments have also been explored concurrently \citep{karpathy2014deep,karpathy2015deep}.


To predict images from the sentences -- and conversely sentences from the images --
the architecture we chose in
Chapters \ref{ch:COLI}, \ref{ch:IJCNLP}, \ref{ch:ConLL} and \ref{EMNLP}  combines 
a recurrent neural network $\phi(s, \theta)$ to
represent sentences $s$ and a pre-trained convolutional neural networks 
followed by an affine transformation we train for the task  $\psi(i, \theta)$ 
to extract features from images $i$  as in \citep{kiros2014unifying}. 
The image-context prediction from sentences in 
Chapter \ref{ch:COLI} is formulated  as minimizing the cosine distance
between the learned sentence $\mathbf{s} = \phi(s, \theta)$ and image representations 
$\mathbf{i} = \psi(i, \theta)$ in a training set of image--caption pairs $<i,s> \in D$ :

\begin{equation}
J_{cos}(\theta) = \sum_{<i,s> \in D} 1 - \frac{\mathbf{i} \cdot \mathbf{s}}{\| \mathbf{i}\| \| \mathbf{s} \|} \\
\end{equation}

Later we follow the formulations of \cite{vendrov2015order} and \cite{faghri2017vse++} and apply 
the sum-of-hinges loss in Chapter~\ref{ch:IJCNLP} and max-of-hinges ranking objective
in Chapters~\ref{ch:ConLL} and \ref{ch:EMNLP} . These loss functions 
push relevant pairs $<i, s>$ close, while contrastive pairs  $<i, \hat{s}>$ and $<\hat{i}, s>$
far from each other in a joint embedding space. Given a mini-batch of a 100 samples, for each sample
the contrastive pairs are generated by taking the wrong pairings from that batch leading to 
99 contrastive pair per sample. The ranking losses are minimized in both directitons:

\begin{equation}
J_{rank}(\theta) = \sum_{<i,s> \in D} f_{rank}(\mathbf{i}, \mathbf{s}) + f_{rank}(\mathbf{s}, \mathbf{i}) 
\end{equation}

where the $f_{rank}$ is either the sum-of-hinges loss $J_{sum}$

\begin{equation}
\label{eq:sumviol}
J_{sum}(\theta) = \sum_{<\hat{a},b>}[\max(0, \alpha - s(\mathbf{a},\mathbf{b}) + s(\hat{\mathbf{a}}, \mathbf{b}))] 
\end{equation}

or the max-of-hinges objective:

\begin{equation}
\label{eq:maxviol}
J_{sum}(\theta) = \max_{<\hat{a}, b>}[\max(0, \alpha - s(\mathbf{a},\mathbf{b}) + s(\hat{\mathbf{a}}, \mathbf{b}))]
\end{equation}

where $\alpha$ is the margin parameter for both loss functions and $s(a, b)$ is the cosine similarity
function.
We empirically found both ranking losses to perform better than $J_{cos}$ and $J_{max}$ to perform
consistently better in our experiments than $J_{sum}$. The use of these various objetctives across
chapters  reflects the evolving common practices in the field at the time. 

%Similarly in Chapter~\ref{ch:COLI} as the pre-trained CNN image-feature extractor we apply the VGG-16 
%architecture \citep{conneau2016very},  in Chapter~\ref{ch:IJCNLP} the Inception-V3 \citep{Szegedy2015},
%while in ~\ref{ch:ConLL} and ~\ref{ch:EMNLP} the ResNet-50 \citep{he2016deep}. These choices are
%again somewhat arbitrary. Chapter~\ref{ch:COLI} is based our Imaginet model \citep{chrupala2015learning}
%and VGG features were the standard choice at that time.  For Chapter~\ref{ch:IJCNLP} we empirically 
%determined Inception-V3 to give the best performance, while for Chapters~\ref{ch:ConLL} and \ref{ch:EMNLP}
%we used ResNet-50 as the features extracted from this architecture were given as part of  
%a Shared Task based on the data set we used.

%Other than image--sentence context prediction we also apply the $J_{max}$ 
%objective for sentence--sentence ranking in Chapters~\ref{ch:ConLL} and \ref{ch:EMNLP}.
%The data for this objective are sentences that belong to the same image.

%Note the similarity between the ranking losses and the negative sampling
%in the skip-gram model \citep{mikolov2013distributed}.

The representations learned by such image--sentence  ranking models have 
been shown 
to improve performance when combined with skip-thought embeddings
on a large number of semantic sentence classifications tasks compared to
skip-thought only \citep{kiela2017learning}. These findings were confirmed and improved upon using a
self-attention mechanism on top of the RNN encoder \citep{yoo2017improving}.


In Chapter~\ref{ch:COLI} we develop methods to 
to investigate the difference between the representations learned by (text-only) language
models and image--sentence ranking models.
Next in Chapter~\ref{ch:IJCNLP} we show that incorporating
the visually grounded learning through an image--sentence ranking objective 
leads to better translations in the visually descriptive domain. Furthermore, 
we show that learning multi-modal representations provides gains on top of learning 
from larger bilingual corpora.
Finally in Chapters~\ref{ch:ConLL} and \ref{ch:EMNLP} we apply this 
framework in the multi-lingual setting and demonstrate
that better visually grounded representations can be learned when training on multiple
languages jointly. 

%For our multi-lingual experiments we use the Multi30K data set \cite{elliott2016multi30k,elliott2017findings}; a multilingual extension of Flickr30K. It consists of a {\it translation} and a {\it comparable} portions both containing all images from the original Flickr30K. The \emph{translation} portion annotates a single English description with a German, French and Czech caption, while the \emph{comparable} the original 5 English and additional 5 German descriptions collected independently using the same crowd-sourcing process.

%\paragraph{Our contribution}


%In \citep{chrupala2015learning} we train a multi-task architecture consisting
%of a shared word-embedding matrix and two pathways: 1.) common recurrent language
%model trained to maximize the probability of a word given the history,
%2.) grounded representation learning model predicting the image representation paired with the sentences.
%This model combines the distributional hypothesis for learning the meaning of words
%and the grounded learning on the sentence level.
%We show that the word-representations learned by the two path architecture
%predict human word-similarity judgements with higher accuracy then the one-path
%architectures. Furthermore, the combined model is more sensitive to word-order than
%the visual-only model.  Lastly we show that images act as an
%anchor and the architecture performs paraphrase retrieval well above chance
%level.

%Chapter~\ref{ch:COLI} is dedicated to develop techniques to interpret the
%opaque continuous representations of this model. It provides an in-depth
%comparative analysis of the linguistic knowledge represented by the
%language-model and visual-pathways of this architecture.




%\section{Multilingual representations of words and sentences}\todo{this will be challenging, not sure yet how to build it up.}
%Multilingual or cross-lingual word embedding approaches map words of different languages in a shared space. These representations allow to transfer knowledge between languages and
%perform cross-lingual tasks such as cross-lingual document retrieval. The bulk of such approaches apply the techniques developed for monolingual word-embedding induction as a component
%of their full pipeline. One of the early approaches trains two separate word spaces with using the negative-sampling implementation of the skip-gram algorithm and then maps these spaces onto eachother by minimizing the squared loss between the 5000 top most frequent word-vectors \cite{mikolov2013exploiting}. This main approach went through improvents
%such as by switching mean-squared error to ranking loss \cite{lazaridou2015hubness} or enforcing orthogonality constraint on the mapping matrix
%\cite{xing2015normalized}   (WE DID THE SAME OVER TIME). Another prominent technique to learn a mapping between word (and other types of) spaces is Canonical Correlation Analysis, first
%used by \cite{faruqui2014improving}, which was later improved by the application of Deep CCA \cite{lu2015deep}. Another approach is to create a pseudo-bilingual corpora by replacing words
%in a source language by their translation and train an embedding model on the resulting corpus
%The uber multilingual word-embedding \cite{ammar2016massively}.

\section{Visually grounded multilingual representations}
\label{sec:vismulti}

On top of the visual modality anchoring linguistic representations to perceptual reality it
also provides a universal meaning representation bridging between languages.
The  intuition being that words or sentences with similar meanings appear
within similar perceptual contexts independently of the language. 
First in Section~\ref{sec:multiview} we discuss
the multi-view representation learning perspective of considering images annotated 
with multiple descriptions in different langauges as multiple views of the 
same underlying semantic concepts.
Our aim in Chapters~\ref{ch:ConLL} and \ref{ch:EMNLP} is to learn 
better visually grounded sentence representations by learning from these multiple-views 
simultanuously. Furthermore, in Chapter~\ref{ch:IJCNLP} we show that we can improve
translation performance by learning better sentence representations through  adding the visual
modality as an additional view.
Next we discuss how images can be used as pivots in practice for translation on word-level in 
Section~\ref{sec:bilinglex} and on sentence-level in Section~\ref{sec:imgpivot}.  

\subsection{Multi-view representation learning perspective}
\label{sec:multiview}


Images and their descriptions in multiple languages can be taken as different views of the
same underlying semantic concepts. From this multi-view perspective learning common representations
of multiple languages and perceptual stimuli can potentially exploit the complementary information
between views to learn better representations. Being able to extract
a shared representation from only a single view also leads to practical applications such as
cross-modal and cross-lingual retrieval or similarity calculation.

Multi-view learning traditionally considers paired samples $\mathbf{X}$ and $\mathbf{Y}$,
where each pair of rows $<\mathbf{x_i} \in \mathbf{X}, \mathbf{y_i} \in \mathbf{Y}>$ 
are two measurements of the same
underlying phenomena. In other words we assume two sets of variables representing the 
same data point. The two main multi-view learning paradigms put forward in recent literature are based on 
autoencoders and canonical correlation analyisis (CCA) \citep{wang2015deep}.

To illustrate the autoencoder approach let us consider \cite{ngiam2011multimodal} who introduced the idea
of multi-modal autoencoders to learn multi-modal joint representations of audio and video. Let
$\phi$ be an encoder neural network extracting features from both modalities and $\psi$ and $\chi$ be 
modality specific decoders. Their model performs the following optimization:

\begin{equation}
\min_ {\theta_{\phi}, \theta_{\psi}, \theta_{\chi}} \frac{1}{N} \sum^N_i (||X_i - \psi(\phi(X_i))||^2 +||Y_i - \chi(\phi(Y_i))||^2)
\end{equation}

where $\theta_{\phi}, \theta_{\psi}, \theta_{\chi}$ are the encoder and decoder paramters and $N$
is the size of the training set.
This approach learns shared representations such the one view can be reconstructed from another.
Autoencoder approaches  remain one of the standard family of models
to study the learning of visual-linguistic multi-modal spaces \citep{silberer2014learning,silberer2017visually,wang2018associative}.

For the discussion of CCA based approahces let us consider the deep canonical correlation analysis (DCCA) 
put forward by \cite{andrew2013deep}.  In this approach view
specific networks $\psi$ and $\phi$ are applied to extract non-linear features
and the canonical correlation between these representations is maximized:

\begin{align}
\max_ {\theta_{\phi}, \theta_{\psi}, \mathbf{U}, \mathbf{V}} \; \; &  \frac{1}{N} \text{tr}(\mathbf{U}^T \phi(\mathbf{X}) \psi(\mathbf{Y})  \mathbf{V}^T) \\
\text{subject to} \; \; & \mathbf{U}^T \left(\frac{1}{N} \phi(\mathbf{X}) \phi(\mathbf{X})^T + r_x \mathbf{I} \right) = \mathbf{I} \\
& \mathbf{V}^T \left(\frac{1}{N} \psi(\mathbf{Y}) \psi(\mathbf{Y})^T + r_y \mathbf{I} \right) = \mathbf{I} \\
& \mathbf{u}_i^T  \phi(\mathbf{X}) \psi(\mathbf{Y})  \mathbf{v}_j = 0, \; \; \forall_{i,j} \;  i \neq j
\end{align}

Here $\theta_{\phi}, \theta_{\psi}$ are neural network parameters, $r_x, r_y$
are regularization paramteres \citep{de2003regularization} and $\mathbf{U}, \mathbf{V}$
are projection matrices or \emph{directions} of the CCA. This optimization process amounts to maximizing
the correlation between the projections of the two data views subject to the contraint that 
the projected dimensions are uncorrelated. 

The \emph{workhorse} of the image--sentence ranking experiments of  \citep{hodosh2013framing}
was in fact the Kernel CCA method \citep{akaho2006kernel}. Other CCA 
based approaches were also applied to the image--sentence data sets we consider in our work \citep{ ,gong2014improving,Klein2015AssociatingNW,eisenschtat2017linking}.

A third direction that is also explored is combining the reconstruction objective of autoencoders 
with an additional correlation loss, but without the whitening constraints of CCA 
 \citep{ap2014autoencoder,chandar2016correlational}.
 

 In the multi-lingual multi-modal setting \cite{funaki2015image} applies Generalized CCA \citep{horst1961generalized} -- a variant of CCA 
generalized to multiple views as opposed to only two -- to 
learn correlated representations of images and multiple languages. 
The deep partial canonical correlation analysis approach 
 -- a deep learning extension of the partial canonical correlation  \citep{rao1969partial} --
learns multilingual English-German sentence embeddings conditioned on the
representation of the images they are aligned to \citep{rotman2018bridging}.
They show that their model using the visual modality as an extra view finds
better multilingual sentence and word representations as demonstrated by
cross-lingual paraphrasing and word-similarity results.

The bridge correlational neural network approach \citep{rajendran2015bridge}
combines autoencoders and correlation objectives to 
learn common representations in a setting when the different views only need to be
aligned with one pivot view. They preform
image-sentence retrieval experiments in French or German where
the image-caption data set is only available for English, however,
there is parallel corpora between German or French and English. In other words English acts as a pivot.
A similar combination of autencoder and correlational training was applied to \emph{bridge}
image--captioning \citep{saha2016correlational}.

Our formulation of the problem does not fall in the set of approaches generally 
considered as multi-view learning.  However, it is similar to the CCA based techniques in that we
train two sub-networks -- one for the linguistic $\psi$ and another for the image modality $\phi$ --
and do not rely on decoder networks to compute a reconstruction  loss as in the autoencoder approaches.
Furthermore in Chapter~\ref{ch:COLI} we minimize the cosine distance between the learned representations
which is related to the CCA objective: when the matrices $\mathbf{X}$ and  $\mathbf{Y}$ are not centered
the CCA objective corresponds to maximizing the cosine similarity instead of the correlation.

One of the main benefits of the learning to rank approach we opted for is
its flexibility: 1.) in Chapter~\ref{ch:COLI} we train an image--sentence ranking model in a single 
language, 2.) we apply the same building blocks to train on multiple languages where 
the same images are shared between languages in Chapter~\ref{ch:ConLL}, 3.) in 
Chapter~\ref{ch:EMNLP} we explore the setup without such an alignment and finally 4.) 
in Chapter~\ref{ch:IJCNLP} we improve the automatic translation  performance 
by adding the image--sentence ranking objective, incorporating an additional view 
to help us learn better sentence representations. 

In our setup additional views are incorporated in the sentence representations
by full paramter sharing with multi-task learning: 
given multiple image--caption data sets at each iteration we sample 
a batch from one of them and perform an update  to $\theta_{\phi}$ and $\theta_{\psi}$
using the ranking loss function.
\cite{gella2017image} applies the image--sentence ranking framework in the multilingual setup
considering images as a pivots bridging English and German and train a multilingual
image--sentence ranking model.
Their results suggest that the multilingual models outperform the monolingual ones on image-sentence
ranking, however, they do not show consistent gains across languages and model settings.
In Chapter \ref{ch:ConLL} we implement a similar setup to \cite{gella2017image}
and  show that both English and German image-sentence
ranking performance is reliably improved by bilingual joint training using our setup.
We expand on the results further and provide evidence that more gains can be
achieved by adding more views: on top of English and German, we
add French and Czech captions and show the monolingual model is consistently 
outperformed by the bilingual and the latter by the multilingual. 
%Revisiting the issue of alignment we find that
%the unlike in previous experiments \citep{gella2017image,calixto2017multilingual,rotman2018bridging}
%image-pivoting leads to better visual-sentence embeddings even when the images
%are not matched between languages. 
We apply the same approach to improve the performance on the lower resource
French and Czech languages by adding the larger
English and German image-caption sets; showing successful multilingual
transfer in the vision-language domain.


\subsection{Bilingual lexicon induction}
\label{sec:bilinglex}

On word level images have been used to link languages and induce bilingual
lexicons without parallel corpora. The lack of bi-text in this setting have
been traditionally solved by methods relying on textual features
such as orographic similarity \citep{haghighi2008learning} or similar diachronic distributional
statistical trends between words \citep{schafer2002inducing}.
However, images tagged with various labels in a multitude of languages are
available on the internet and a model of image-similarity can be used to exploit
images as pivots to find translations between such tags. Alternatively representing words in multiple
languages in a joint space with images 
-- as discussed in Section~\ref{sec:visualwords} for the monolingual case --
allows word translation to be simply performed through nearest neighbour search.

The first approach to construct a dictionary based on image similarity was
developed by \cite{bergsma2011learning}. They use Google image search to find
relevant images for the names of physical objects in multiple languages.
These images are represented by BoVW vectors based
on SIFT and color features. To find word-translations given a word in the
source language and a list of possible translations in the target language
for each word Google is queried to find $n$ images per word. For each image the
feature vectors are extracted. This is followed by computing the similarity
between the feature vectors of the images corresponding to the source word and
all target words. Finally the word in the target vocabulary with the highest
similarity is chosen.

This method is vastly improved upon by a later approach applying
pre-trained convolutional neural networks to represent words
in the visual space \citep{kiela2015visual}. Their approach closely follows
the monolingual visual-word representations of \cite{kiela2014improving}:
each word is represented as the summary of CNN representations of images they
co-occur with. Given a word in the source word the candidate translation is found
simply by performing nearest neighbor search on the target vocabulary.

Exploring the limitations of image-pivoting for bilingual lexicon induction
\cite{hartmann2017limitations} present a negative result
showing that such techniques scale poorly to non-noun words such as
adjectives or verbs. However, combining image-pivot based bilingual
word-representations with more traditional multi-lingual word-embedding
techniques lead to superior performance compared to their uni-modal
counterparts \cite{vulic2016multi}. 

At the time of writing this thesis
the first large-scale vision based bilingual dictionary induction data set
was put forward by \citep{hewitt2018learning}. They create a data set
of \~200K words and 100 images per each using Google Image Search and perform
experiments with 32 languages.
They confirm the finding of \cite{hartmann2017limitations} that image-pivoting
is most effective for nouns, however, find that using their larger data set
adjectives can also be translated reliably. Similarly to \cite{vulic2016multi}
they also show that combining
visual and textual word-representations through a re-ranking
experiment that visual representations improve the state-of-the-art
text-only methods in the case of \emph{concrete} concepts.

\subsection{Image pivots for translation}
\label{sec:imgpivot}

In automatic machine translation a pivot based approach is applied
when there are parallel corpora available between language pairs $A\rightarrow C$.
and $C \rightarrow B$, but there is no data for $A\rightarrow B$. The problem is
solved by first translating $A$ to $C$ and then $C$ to $B$. 
Image--pivoting the refers to a setup where we assume the exitence of a data set
of images paired with words or sentences in different langauges $A \leftrightarrow I_A$ and 
$B \leftrightarrow I_B$ and translation is done through the image space 
going  through $A \rightarrow I$ to $I \rightarrow C$. 
%A common technique in statistical machine translation
%is to apply a heuristic re-ranking algorithm
%to a sampled list of candidate translations from a translation model given the source sentence.
%\cite{hitschler2016multimodal} consider the setting of image-caption translation where an
%image-caption pair is given on the source side and on top of the parallel source-target sentence corpus
%additional target-side image-caption corpus is available. The show that using the images in the re-ranking
%pipeline improves translation performance upon using solely textual-similarity.
\cite{nakayama2017zero} apply image-pivoting in such a zero-resource machine
translation setting. The task is to translate from English to German without
aligned parallel corpora, however, separate image--description data sets are
available in both languages. They solve the problem by
training two components: 1.) visual-sentence encoder that matches images and their descriptions,
2.) image-description generator maximizing the likelihood of gold standard captions given the images.
At test time the visual-sentence encoder representation of the source sentence is fed to the
image--description generater to produce the translation.
Their results were improved later by modeling
the image-pivot based zero-resource translation setup as a
multi-agent communication game between encoder and decoder
\citep{chen2018zero,lee2017emergent}.






\section{Interpreting continuous representations}
\label{sec:interpret}
The linguistic representations learned through neural architectures are notoriously opaque.
Contrary to count-based methods the features extracted by deep networks from text input
appear as arbitrary dense vectors to the human eye.
In experiments with grounded learning throughout this thesis we find that visual grounding
improves translation peformance and that multilingual representations outperform monolingual 
ones in image--sentence ranking.
But where do these improvement come from? What are the linguistic
regularities represented in the recurrent states that lead to the final performance metrics?
Did the model learn to exploit trivial artifacts in the training data or did it learn to meaningfully generalize?
What characterizes the individual features recurrent networks extract from the input sentences?

The main topic of this thesis is learning visually grounded representations for linguistic units.
For a complete picture, however, it is crucial to assess the difference in the representations
between textual only and multimodal representations not only from a quantitative point of view,
but also from a qualitative linguistics angle.
This is what Chapter \ref{ch:COLI} is dedicated to.

Developing techniques for interpreting machine learning models have multiple goals.
From the practical point of view as learning algorithms make their way into critical applications
such as medicine humans and machines need to be able to co-operate to avoid catastrophic
outcomes \citep{caruana2015intelligible}. As such there is a growing interest in deriving methods
to \emph{explain} the decision of such architectures.

One of the approaches is to assign a real-valued "relevance" score to each unit in the input signal,
signifying how much impact it had on the final prediction of the model.
One of the first paradigms in generating such relevance scores is gradient based methods:
they take the gradient of the output of the network with respect to the input \cite{simonyan2013deep}.
Deep neural models of language tasks learn distributed representations of input symbols
and as such further operations have to applied to reduce the resulting gradient vectors to
single scalars e.g.: using $\ell_2$ norm \citep{bansal2016ask}.

Another prominent and well studied
approach still based on gradient information is layerwise
relevance propagation (LRP) \citep{bach2015pixel}. The output of the final layer
is written as the sum of the relevance-scores from the input and similarly to the back-propagation
algorithm the relevance of each neuron
recursively depends on the lower-layer all the way down to the input signal.
Different versions of LRP run the backward pass with different rules taking as
input gradient information and activation values. It was later theoretically analyzed and generalized
into the deep Taylor decomposition method \citep{binder2016layer}.
%This has been shown to be equivalent to taking the
%gradients with respect to the input as in \cite{simonyan2013deep} and multiplying it elementwise
%with the input itself \citep{shrikumar2017learning}.
%LRP was also later derived for recurrent architectures \citep{arras2017explaining} to describe
%sentiment classifiers. 

%Another backpropagation based algorithm is DeepLIFT, which instead of
%using the gradients for computing the relevance scores it uses the difference between the activations
%that result from a specific value compared to a baseline input.

Perturbation based methods are gradient free and are algorithmically very simple: they involve generating
pseudo-samples according to some procedure and measuring how the models' response changes
between each pseudo-sample and the original.
LIME \citep{ribeiro2016should} and its NLP specific LIMSSE extension
\citep{poerner2018evaluating} perturbs
the input creating a local neighborhood around it and fits interpretable linear models to explain
the predictions of any complex black box classifier.
Even simpler perturbation based techniques measure the
difference between the original input and the various perturbed candidates such as the erasure
\citep{li2016understanding} and our omission \citep{Kadar2016} method in Chapter~\ref{ch:COLI}.

Apart from practical considerations of model interpretation training complex and opaque models
from close to raw input can help us discover interesting patterns in the input data that 
are crucial in solving the task.
Deep neural networks learn to solve tasks from close to raw input, similar to
what humans receive. As such the regularities they learn can also shed light into the patterns
humans might extract from data to cope with certain tasks.
Recent methodology in probing the learned representations of LSTM language models,
in fact, resemble psycholinguistic studies.
A number of experiments using the agreement prediction paradigm
\citep{bock1991broken} suggest that LSTM language models successfully learn
syntactic regularities as opposed to memorizing surface patterns
\citep{linzen2016assessing,enguehard2017exploring,bernardy2017using,gulordava2018colorless}.

For our perpuses in Chapter~\ref{ch:COLI} we develop our explanation method to
shed light on the linguistic characteristics of the input grounded learning
models learn in contrast to text-only language models.

\section{Published work}

\subsection{Chapters}

Each of the following Chapters was presented before as a published long-paper. These are included with
the only modification of re-aligning and re-sizing a few figures.

\begin{description}
	\item[Chapter~\ref{ch:TAL}] \bibentry{kadar2015learning}
	\item[Chapter~\ref{ch:COLI}] \bibentry{kadar2017representation}
	\item[Chapter~\ref{ch:IJCNLP}] \bibentry{elliott2017imagination}
	\item[Chapter~\ref{ch:ConLL}] \bibentry{kadar2018lessons}
	\item[Chapter ] 
\end{description}

\subsection{Other publications completed during my PhD}

Worked on some of the experiments for the visually grounded sentence representation learning
model \texttt{IMAGINET}, which formed the basis in the investigations of Chapter~\ref{ch:COLI}:

\begin{itemize}
\item \bibentry{chrupala2015learning}
\end{itemize}

Other than methods I also contributed to two corpora within the vision and language domain.
During my internship at Microsoft Research Montreal my side-project was a synthetically generated visual-reasoning
dataset named FigureQA:

\begin{itemize}
\item \bibentry{kahou2017figureqa}
\end{itemize}

The second corpus I worked on was with our sister group in the communications department and a collaboration with 
Vrije Universiteit Amsterdam:  a data set of native Dutch spoken image-description with eye-tracking recordings.
The goal of this project was to provide a large data set for the community interested in referring-expression research with rich annotations already existing in the Visual Genome data set:

\begin{itemize}
\item \bibentry{van2018didec}
\end{itemize}

Outside of the vision and language domain I had the chance to explore other related topics.
Within our research group, we published on learning unsupervised sentence represenations
from speech:

\item \bibentry{chrupala2018difficulty}

Furthermore, we published a paper about attemtping reproduce a 
recurrent architecture for language modeling:

\begin{itemize}
\item \bibentry{kadar2018revisiting}
\end{itemize}

I had the opportunity to work on implementing recurrent networks for
generation in the context of referring-expression generation:

\begin{itemize}
\item \bibentry{ferreira2018neuralreg}
\end{itemize}


Finally, the largest project other than my main PhD project I worked
on was during my main project during my Microsoft Research Montreal internship.
It is linear-logic programming based system for 
generating text-based adventure games to test
generalization in (deep) reinforcement learning:

\begin{itemize}
\item \bibentry{cote2018textworld}
\end{itemize}

%\section{Vision and Language applications}
%Mention the bunch of work thats here. Do image-captioning and image-sentence ranking and then metion that we do the latter always. Say the importance of attention in this field and mention the DIDEC corpus. Mention VQA and fine-grained reasoning and the FigureQA paper.



%\section{Methods}

%\section{Data sets}
%For image-feature extraction all approaches presented in the thesis use some
%CNN architecture pre-trained on a version of the ImageNet data set \cite{deng2009imagenet} prepared for a large-scale image classification challenge \cite{russakovsky2015imagenet}. ImageNet annotates the noun synsets from WordNet \cite{miller1995wordnet} with images collected from the internet. At the time of writing this thesis on average the data set consists of 500 images per node. The section used in the pre-trained networks is the ILSVRC2012 containing 1.2 million images annotated with 1000 classes. For learning grounded language representation we use data sets introduced for image-captioning. These data sets annotate images found in online resources with descriptions through crowd-sourcing. The smallest data set we use is Flickr8K \cite{hodosh2013framing} with 8000 images, which was later extended to contain 30K images resulting in the Flickr30K benchmark \cite{young2014image} and finally the largest collection we use is the COCO image-caption benchmark \cite{chen2015microsoft} with 123K images. All data sets contain 5 captions per image.
%The descriptions collected for these data sets are largely \emph{conceptual}, \emph{concrete} and \emph{generic}. This means that descriptions do not focus too much on \emph{perceptual} information such as colors, contrast or picture quality; they do not mention many \emph{abstract} notions about images such as mood and finally the descriptions are not \emph{specific} meaning that they do not mention the names of cities, people or brands of objects. What they do end up mentioning are entities depicted in the images (frizbee, dog, boy) their attributes (yellow, fluffy, young) and their relations between each other. The images depict common real-life scenes such as bus turning left or people playing soccer in the park. As such annotation collected independently from different workers end up focusing on different aspects of these scenes. For a comprehensive overview on image-description data sets please consult \cite{bernardi2016automatic}.
%For our multi-lingual experiments we use the Multi30K data set \cite{elliott2016multi30k,elliott2017findings}; a multilingual extension of Flickr30K. It consists of a {\it translation} and a {\it comparable} portions both containing all images from the original Flickr30K. The \emph{translation} portion annotates a single English description with a German, French and Czech caption, while the \emph{comparable} the original 5 English and additional 5 German descriptions collected independently using the same crowd-sourcing process.


%\section{Architectures}
%Just do the usual GRU + CNN thing. cite Jamie (heart).

%\section{Recurrent network}
%Do the GRU mention that the LSTM is strictly more powerful according to Yoav, but it doesnt seem to matter at all according to Chung.

%\section{Convolutional network}
%Just super quick run-down on VGG and ResNet.

%\section{Optimization}
%Just mention that all this shit is optimized with some version of SGD and we use Adam all the time. Mention that Adam sucks but its just some common thing to do. Say this thing about the MSE or cosine loss that we start with that but i results in hubness actually as Angeliki points it out. So we move to the contrastive kinda losses.

%\section{Transfer learning}
%Don't really worry about it too much just say something about how awesome is that the CNNs trained on image net actually end up being so useful for us.

%\section{Multi-task learning}
%Multi-task learning has been an important paradigm since the earliest works on training neural architectures for natural-language processing \cite{collobert2008unified}. Multi-task learning involves optimizing multiple loss function jointly usually with the goal of exploiting commonalities between tasks. As described in the seminal work of \cite{caruana1997multitask}:

%\begin{quote}
%Multitask Learning is an approach to inductive transfer that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias. It does this by learning tasks in parallel while using a shared representation; what is learned for each task can help other tasks be learned better.
%\end{quote}

%Introducing inductive bias using different tasks means that the learning algorithm prefers a specific set of hypotheses over others just as in the case of other forms of regularization such as $\ell_1$ or $\ell_2$ penalties. Here I only discuss the specific instantiation of the mutli-task learning paradigm that are applicable to the chapters presented in this thesis and for a more comprehensive overview please consult \cite{ruder2017overview}. The technique used in the thesis
%is hard-parameter sharing \cite{caruana1997multitask}
