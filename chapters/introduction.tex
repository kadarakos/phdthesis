%!TEX root = ../dissertation.tex

%EK: edits November 28
serval
\chapter{Introduction}
\label{introduction}
As opposed to the continous nature of the visual world linguistic units are inherently
discrete, which is reflected in the techniques used in the computational processing
of natural language. Information in linguistic utterrances is traditionally encoded
in sparse high-dimensional count vectors, where each
dimension corresponds to the number of times a specfic feature occurs in a sample.
For each word in a sentence common features to consider take the form of:
``this token is a noun'', ``the preceding word is cat'',
``the following token has dependency label direct object''.
More often then not these vectors do not even hold counts, but 0-1 indicator features singaling
the presence or absence of a particular property.
In such a setting the relationships between features are not handled and
as a result machine learning algorithms are presented with an input representation where
``the preceding word is cat'' and ``the preceding word is dog''
are completely independent properties. Furthermore, interactions between features has to be
explicitly represented as a separate entry in the vector, which once again has no clear
relationship to its parts as far as the learning algorithms are concerned. Let us consider
for a simple example the problem of language modeling, where the aim is to find a probabilistic model
that assigns high probability to likely sequences and low probability to gibberish.
The representation of words and phrases in common n-gram language models is on the
very extreme of discrete: they are represented by their \emph{identity}.
Imagine that in the training set there were a number of sentences expressing the concept
of ``someone is walking a dog'', a little less mentions of ``someone walking a cat''
and no occurrence of ``someone walking a serval'', but the word ``serval'' did occur.
I have never seen anyone walking a serval, however, as a human I do know that it is a dog-sized
feline creature and as a consequence I can imagine someone taking a walk with one. However, a language
model trained with discrete symbolic representation cannot \emph{generalize} in such a way as the
relationship between the words ``dog'', ``cat'' and ``serval'' is not represented. Consider now the
case of walking a dolphin. Both ``walking a serval'' and ``walking a dolphin'' would be impossilbe
by the hypothesized language model, however, us humans would say that walking the serval is
kind of possible, but walking the dolphin is just surreal. We know it from reading about both animals
that they can be domesticated, however, by their physical properties while one affords to be walked
the other does not. Dogs, cats, servals and dolphins are similar in some dimensions whereas
different in others.
What the present thesis is concerned about is \emph{learning continous representations}
of words, phrases up to sentences from multiple sources of information to encode such useful properties.
Such representations allow machine learning algorithms to represent smooth functions over linguistic
inputs where the local neighborhoods of linguistic units correspond to syntactic and semantic regularities.
The well establised tradition of conituous word representations consider features extracted
from large text-corpora in a single language focusing on co-occurence statstics between words.
My thesis focuses integrating the \emph{visual modality} and jointly learning representations for
\emph{multiple languages}. Furthermore, I study the the beneficial interaction of
visually grounded and multilingual representation learning.
Grounding in visual modality is largely motivated by evidence of
perceptual grounding in human concept acquisition and representation \cite{barsalou2003grounding} and as such
it brings computational language learning systems closer to human-like learning.
Furthermore, as pointed out by the dolphin versus serval example above,
certain aspects of meaning are better captured by \emph{encyclopedic}
knowledge such as domestication abundant in textual data, however, perceptual information
can provide complementary valuable insights into physical properties rarely mentioned in texts such as size,
shape and color. In practice harnessing the visual modality to learn language
representations that link linguistic knowledge
to the external world \cite{kiela2014improving,baroni2016grounding,elliott2017imagination,kiela2017learning,yoo2017improving}
has been empirically shown to improve performance on several semantic tasks.
The first part of the thesis -- Chapter 1 and 2. -- focuses on learning grounded representations for a single language.
In Chapter 1. we start by learning visual representations for words using a novel computational
cognitive model of cross-situational word learning that takes words and high-level continuous image
representations as input. Chapter 2. introduces a modern recurrent and convolutional neural network based
model that learns from both visual-grounding signals and word-word co-occurrences.
Furthermore, we introduce a technique to interpret the learned representations of such an
architecture and investigate if certain linguistic phenomena is encoded in the learned model.
The second part of the thesis focuses on multimodal multilingual models.
Traditionally, given a task for each language separate corpora are created and separate sets of parameters are learned.
Learning shared cross-lingual representations, however, allows researchers and practitioners
to train a single model allowing to transfer knowledge between languages leading to practical consequences
such as mitigating the low-resource problem, cross-lingual applications and processing data with code-mixing.
Multilingual and multi-modal representations have their separate benefits, however, they can be beneficial
to each other as well. In my work I view the visual world as a natural common meaning representation briding
between languages. Chapter 3. provides empirical evidence that grounded learning can improve machine
translation quality and in Chapter 4. we show under what conditions multilinguality an help improve grounded
representations. Finally in Chapter REFERENCE 5 WE HOPEFULLY IMPROVE ZERO-SHOT TRANSLATION USING IMAGES AS A BRIDGE.

%Multilingual systems also offer a fertile ground computational typology as shared models are forced to exploit cross-lingual regularities.

\section{Distributed representations of language}
My work focuses on learning grounded distributed sentence representations.
The first half of the thesis is about learning the representations of words and followed-up by learning sentence embeddings for a single language. Later we move on to learning multilingual visually grounded representations. In other works of mine I contributed to generating referring expressions grounded in Wikipedia entities and learning language through interacting with text-games grounded in a knowledge-base.

\subsection{Distributed word-representations}\todo{I have the material included that i wanted to in this section.}
\label{sec:words}
The distributional approach to word meaning hypothesizes that semantically related words
tend to appear in similar contexts. This idea goes back in linguistics tradition to the the
earlier days of American structuralism \cite{nevin2002legacy}. In the seminal paper
"Distributional structure" \cite{harris1954distributional} back in 1954 Harris already claims
that distribution should be taken as an explanation for word meaning and that similarity classes
can be constructed based on co-occurrence statistics.
\cite{cruse1986lexical} writes that 'It is  assumed  that  the  semantic properties  of
a lexical  item  are  fully  reflected  in  the  appropriate  aspects  of  the  relations
it  contracts  with  actual  and  potential  contexts' and that 'the  meaning  of  a word
is constituted  by  its  contextual  relations'. Computational models of distributional
semantics implement this intuition learn real-valued word vectors based on co-occurence
statistics on large text corpora. Here we discuss such representations using the \emph{count-based}
vs. \emph{prediction-based} distinction borrowed from \cite{baroni2014don}.
%One of the first computational verification
%attempts of  this distributional hypothesis was put forward in \cite{miller1991contextual} \todo{say something about the paper man}.

\paragraph{Count-based approahces}
Early computational linguistics models of distributional semantics fall in the category of
count-based approaches: they count the number of times target words appear occur in different
contexts resulting in a co-occurence matrix. In the co-occurence matrix
$M$ each row $M_i$ represents a word $i$ and each column $M_{,j}$ a context. It has size
is a $M \in \mathbb{N}^{|V| \times |C|}$ where $V$ is the set of words and $C$ is
the set of contexts.
The entry $M_{i,j}$ is the number of times word $i$ appears in context $j$.
Contexts are typically words appearing within a certain window size or text documents.
To the counts in $M$ various re-weighting schemes are applied followed by some matrix factorization algorithm
resulting in a lower dimensional dense reprentation $M \in \mathbb{R}^{|V| \times d}$, where $d < |C|$.
Both re-weighting and low-rank approximation reduces the dimensionality of the sparse matrix and leads
to faster compute and better generalization.
The earliest approaches include Hyperspace Analogue to Language \cite{lund1996producing},
which constructs a term-term co-occurrence matrix and Latent Semantic Analysis \cite{dumais2004latent},
which applies the tf-idf re-weighting scheme on a term-document matrix followed by singular-value decomposition.
More recent approaches apply different re-weighting schemes such as pointwise mutual information and local mutual
information \cite{evert2005statistics} or different matrix factorization algorithms such as non-negative
matrix factorization \cite{baroni2014don}. For a comprehensive set of empirical experiments on count-based
approaches please consult \cite{bullinaria2007extracting} and \cite{bullinaria2012extracting}.

\paragraph{Prediction-based approaches}
In more recent years
%-- and more related to the present thesis --
various deep learning methods have been applied to learn distributed word-representations usually referred to
as ´word-embeddings´ in the literature. Contrary to count-based methods prediction-based approaches fit into the
standard supervised learning pipeline: they optimize a set of parameters to maximize the probability of words
given contexts or contexts given words where the word-embeddings themselves form a subset of the parameters
of the full model.

\paragraph{Neural language models}
The first modern approach to neural language models on realistic data sets was introduced
in \cite{bengio2003neural} laying down the framework for recent developments.
It is a feed-forward multilayer perceptron with continuous word-embeddings,
a single hidden layer and a softmax output layer.
More precisely the model is parametrized by a 1.) word-embedding matrix
$\mathbf{W} \in \mathbb{R}^{|V| \times d}$, where each row corresponds to a word-vector
$\mathbf{W_i}$ of size $d$ and each column $\mathbf{W_i,j}$ to a learned feature, 2.)
learned hidden and output weight-matrices $\mathbf{H}$ and $\mathbf{U}$. Similarly
to the case of count-based embeddings $d < |V|$.
The network takes as input the concatenation of $n$ word-vectors preceding the target word as context
representation $\hat{\mathbf{w}} = [\mathbf{w_1}; \ldots; \mathbf{w_{n-1}}]$
and outputs the probability distribution over the current word $w_n$.
The computation of the network is shown in Equation~\ref{eq:bengion}:

\begin{equation}
\label{eq:bengio}
P(w_n|w_{n<n}) \approx \text{softmax}(\mathbf{U} \text{tanh}(\mathbf{H} \mathbf{\hat{w}} + b_h ) + b_u)
\end{equation}
The model is trained to maximize the probability of
the target word given the previous fixed number of words as context over a training corpus
-- $n$-gram language model -- trained with stochastic gradient descent \cite{cauchy1847methode}
through the backpropagation algorithm \cite{rumelhart1985learning}.
The superior performance of the feed-forward neural language model has soon been shown to
improve performance in speech recognition \cite{schwenk2005training}.

Following a similar recipe the convolutional architecture of  \cite{collobert2008unified}
based on the time-delay neural network model \cite{waibel1990phoneme} took several steps towards the
by now standard practices in deep NLP.
Contrary to the simple feed-forward network language model the convolution over-time structure can handle
sequences of variable length essential in NLP applications with sentences containing varying number of words.
The paper also introduces the idea of jointly learning many tasks at the same time such as part-of-speech
tagging, chunking, named entity recognition and semantic role labeling through multi-task learning.
Finally \cite{collobert2008unified} were the first to show the utility of pre-trained word-embeddings
in other tasks. Their architecture was later refined in \cite{collobert2011natural} and the pre-trained
full model was made available alongside the standalone word-embeddings in the SENNA toolkit.

The architecture, however, that arguably became most popular in NLP and is used in
Chapters~\ref{ch:COLI,ch:IJCNLP,ch:ConLL} is the recurrent neural network. The architecture was
first introduced by the late Jeffrey L. Elman in his seminal paper \citep{elman1990finding}
as a model of human language learning and processing. Recurrent neural networks
take a variable length sequence as input perform a stateful computation over the it's elements.
In case of language modeling at each timestep $t$ the network takes an input word $w_t$
and its hidden state $\mathbf{h_{t-1}}$ maintained through previous timesteps to compute the current
state $\mathbf{h_t}$ and to predict the probability distribution over the following word $w_t$. It is
parametrized a word-embedding matrix $\mathbf{W} \in \mathbb{R}^{|V| \times d}$, input to hidden weight
matrix $\mathbf{W_i}$, hidden to hidden weight matrix $\mathbf{W_h}$ and finally the hidden to output
weight matrix $\mathbf{U}$ used to predict the unnormalized probabilities over the vocabulary entries:
\begin{align}
P(w_n|w_{n<n}) &\approx \text{softmax}(\mathbf{U} \mathbf{h_t} + \mathbf{b_o}) \\
\mathbf{h_t} &= \text{tanh}(\mathbf{W_h}\mathbf{h_{t-1}} + \mathbf{W_i}\mathbf{w_t} + \mathbf{b_h})
\end{align}
In \cite{elman1991distributed} shows when trained on simple natural language like input the
hidden states of the network $\mathbf{h_t}$ encode grammatical relations and hierarchical
constituent structure. Despite the early successes it turned out to be difficult to train
Elman networks on practical applications with longer sequences
\cite{bengio1994learning}. Never the less the RNNLM implementation of \cite{mikolov2010recurrent}
established a new state-of-the-art and interestingly the successfully
application the standard backpropagation through time algorithm  \cite{williams1995gradient}
to simple recurrent networks on real world language modeling \cite{mikolov2012statistical}
seems to only have required more computational resources and the now widely used gradient clipping trick.

\paragraph{Efficient linear models}
It wasn't until the introduction of the much simpler and faster continuous bag-of-words
and skip-gram with negative sampling (SGNS) algorithms \cite{mikolov2013efficient}
packaged into the easy to use \texttt{word2vec} toolkit that
word-embeddings became ubiquitous in computational linguistics and NLP research.
These algorithms rely on simple log-linear models as opposed to the more expensive
neural networks leading to faster training on larger corpora. The more successful SGNS
model has two word embeddings matrices one for the words
$\mathbf{W_w}  \in \mathbb{R}^{|V| \times d}$
and a separate one for the contexts $\mathbf{W_c} \in \mathbb{R}^{|V| \times d}$.
The model is trained to maximize the dot product $\mathbf{w}^T\mathbf{c}$
between true word-context pairs $(w,c)$ appearing in corpus and minimize the
dot product $\mathbf{w}^T\mathbf{\hat{c}}$ between randomly
samples contrastive examples $(w,\hat{c})$.

Finally the GloVe \cite{pennington2014glove} is another simple linear model
with word $\mathbf{W_w}  \in \mathbb{R}^{|V| \times d}$ and context
$\mathbf{W_c} \in \mathbb{R}^{|V| \times d}$ embeddings, which
represents a hybrid between count- and prediction-based techniques:
it optimizes word-embeddings to predict the re-weighted
$\log$ co-occurence counts $\log(\#(w,c))$ collected from corpora $D$:

\begin{equation}
\label{eq:glove}
\mathbf{w}^T\mathbf{c} + b_w + b_c = \log(\#(w,c))\;\;\; \forall (w,c \in D)
\end{equation}

where $b_w$ and $b_c$ are context specific bias terms.
Now let express the Glove model in the traditional re-weighted
co-occurence matrix factorization form.
Let the matrix $M \in \mathbb{R}^{|V| \times |C|}$ store the log-weighted
weighted counts and $\mathbf{b_w}$ and $\mathbf{b_c}$ be the vectors of all
$b_w$ and $b_c$ scalars, then

\begin{equation}
\label{eq:glove2}
M \approx \mathbf{W} \cdot \mathbf{C^T} + \mathbf{b_w} + \mathbf{b_c}
\end{equation}

In fact there has been work on finding relationships between
count- and prediction-based methods \cite{levy2014neural} and
using insights from both to develop novel improved
variants \cite{levy2015improving}.

\paragraph{Transferring word representations}

Later the word-embeddings $\mathbf{W}$ learned by recurrent language models were
shown to represent intriguing syntactic and semantic regularities \cite{mikolov2013linguistic}.

In earlier work where pre-trained word-embeddings were also shown for the first time through a
larger-scale empirical investigation by \cite{turian2010word} to transfer to sequence prediction tasks
when used as features in state-of-the-art systems.


\subsection{Visually grounded representations of words}\todo{The references outline what i want i need to expand them still.}
\label{sec:visualwords}
\todo{ADD MORE FROM THIS https://arxiv.org/pdf/1806.06371.pdf}

\paragraph{Language and perception}
The approaches to learn distributed representations discussed so far focus on extracting information exclusively
from text corpora. To human language learners, however, a plethora of perceptual information is available to aid the
learning process and to enrich the representations. The link between human word and concept representation
and acquistion and the perceptual-motor systems has been well established through behavioral
neuroscientific experiments \cite{pulvermuller2005brain}.
The earliest words in children's language acquistion tend to be names of concrete perceptual phenomena
such as objects, colors and simple actions \cite{bornstein2004cross}. Furthermore, children generalize to
the names of novel objects based on perceptual cues such as shape \cite{landau1998object}.
In general, the \emph{embodiment} based theories of concept representation and acquisition in the
cognitive scientific literature put forward the view that a wide variety of cognitive processes
are grounded in preception and action \cite{meteyard2008role}. That said the precise role of
sensori-motor information in language acquisition and representation is a highly
debated topic \cite{meteyard2012coming}.

Various computational cognitive models of child language acquistion investigate the issue
by learning word meanings from small scale or synthetic multi-modal data. The model presented by
\cite{yu2005emergence} uses visual information to learn the meanings of object
names whereasa the architecture of \cite{roy2002learning} learn to associate word sequences
with simple shapes in a synthetically generated data setting.

Interestingly even the articles introducing Latent Semantic Analysis
\cite{landauer1997solution} and Hyperspace Analogue to Language \cite{lund1996producing} mention
that a possible limitation of the presented distributional semantic models is the lack of
grounding in extra-linguistic reality. \cite{landauer1997solution} puts it as "But still, to be more than
an abstract system like mathematics words must touch reality at least occasionally."
The lack of relationship between symbols and the external reality is referred
to as the \emph{grounding problem} \cite{harnad1990symbol,perfetti1998limits}.
On the defense of purely textual models \cite{louwerse2011symbol} argue that the corpora
used to train distributional semantic models are generated by humans and as such reflect the perceptual world.
For a counter argument consider how many pieces of text to states obvious perceptual
facts such as "bananas are yellow" \cite{bruni2014multimodal}.
In practice much work on multi-modal distributional semantics have found that text-only spaces tend to
represent more encyclopedic knowledge, whereas mutli-modal representations capture more concrete aspects
\cite{andrews2009integrating,baroni2008concepts}. However, one does not necessarily need to
reach a conclusion on whether grounded or distributional models are superior, rather combining their merits in a
pragmatic way is an attractive alternative \cite{riordan2011redundancy}. Learning representations
from both linguistic and visual input is a step towards realistic models of language learners, note however,
it is as close as assuming children
learn language by sitting still and watching TV.
%T ADD MORE COGSCI NEUORO STUFF. maybe do the Symbol grounding problem \cite{harnad1990symbol} and empirical symbol grounding stuff \cite{glenberg2000symbol}.

\paragraph{Combined distributional and visual spaces}
When learning multimodal word representations we wish to construct a matrix $M$, where each row $M_i$
corresponds to a word and each feature column $M_{,j}$ represents a co-occurence statistic,
a perceptual feature or mixture of the two.
The first approach to learn visual word representations from realistic data sets
was introduced by \cite{feng2010visual}. They develop a multi-modal topic model trained on a
BBC News based containing the articles and the image illustrations. Each document considered by their model
is a pair $<D, I>$ comprised of a text document $D$ and image $I$.
The generative story of the multi-modal topic model starts by sampling mutli-modal topics,
which generate both the text document $D$ and the images $I$ in the articles.
To be able to apply topic models \cite{feng2010visual} represents images
in a discrete space using the bag-of-visual-words approach \cite{csurka2004visual}
a popular computer vision method at the time. They apply difference-of-Gaussians point detector to segment images
into local regions, each of which are represented by SIFT features \cite{lowe1999object}.
These features are then clustered with K-means, which leads to a bag-of-visual-words (BoVW) representation i.e.
a vector of counts, where each entry is the number of regions on the image that corresponds to certain SIFT-cluster.
Given count-based representations for both text documents and their paired images the standard
Latent Dirichlet Allocation topic modeling algorithm is applied. For the algorithm each
pair $<D, I>$ is represented by the concatenation $[\psi(D);\phi(I)]$ of textual
$\psi(D)$ and visual features $\phi(I)$. After convergence word is represented by a vector,
 where each entry is the probability of that word given a particular topic.
Note that these topics are latent variables inferred from
both distributional and perceptual features of the documents hence represent a joint
visual-textual multimodal space. \cite{feng2010visual}
shows that the multi-modal LDA model outperforms the text-only LDA representations by a
large-margin on word association and word-similarity experiments.

The perceptually grounded word representations \cite{bruni2012distributional} applies
a similar technique combining distributional semantic models and BoVW pipelines.
Rather than having a collection of documents $<D, I>$ they consider a set of words for, which
both distributional and image features are available. For visual word representations
they use images labeled with tags by annotators. Similarly to \cite{feng2010visual} they
apply the bag-of-visual-words feature extraction pipeline with SIFT and color features to
represent images. Contrary to \cite{feng2010visual}, however, a visual-only representation is
created for each tag-word by summing over the features for all images corresponding to the tag.
For text-only models they construct several types of distributional semantic spaces from text-only
corpora unrelated to the images. Note that as in the previous example we wish to create a multimodal
word representation and applied separate functions to extract textual $\psi$ and visual
$\phi$ features. Finally they create the multi-modal space through concatenation.
On word-similarity benchmarks they show that the text-only model performs better than visual-only
and that the combination of the two surpasses both. They also find that distributional semantics
models perform poorly on finding the typical colors of concrete nouns,
whereas the visual and multi-modal models perform perfectly. In these experiments
distributional semantics models fail to capture the obvious fact that ``the grass is green''
providing evidence against the theoretical argument that perceptual information is available in
large collections of texts and so grounded representations are superfluos \cite{louwerse2011symbol}.
Combining bag-of-visual-words and count-based distributed word-representations was the standard methodology
in many other works on multi-modal word representations at the time
\cite{bruni2011distributional,leong2011going,leong2011measuring}.

\cite{bruni2014multimodal} introduces a general multi-modal distributional semantics framework taking as input
separate textual and visual features for words followed by re-weighting and matrix factorization.
Such a unified view allowed to improve multi-modal representations using independent advancements
in distributional semantics and image feature-representations. Researchers can apply vector-space
models to create the distributional space $M^d$ and computer vision techniques to create
visual feature representations for the same words $M^v$. The sepaprate feature spaces
are mixed together by first concatenation $M = [M^d;M^v]$ optionally followed by singular value decomposition
to combines the visual and textual features.

Based on this framework \cite{kiela2014learning} runs the skip-gram with negative sampling algorithm
on large text corpora resulting and takes the word-embedding parameters
$\mathbf{W} \in \mathbb{R}^{|V| \times d}$ as the distributional space. On top of experimenting with
bag-of-visual-words features they apply pre-trained convolutional neural networks (CNN) as black-box image
feature extractors. These architectures learn a hierarchy of blocks of image filters followed by
pre-defined pooling operations optimized for a particular task.
It has been observed in the computer vision community that the lower layers of
deep CNNs trained across various data sets and tasks tend to learn filter maps
that resemble Gabor-filters and color-blobs. Intuitively these low-level features appear to be general
and as such afford \emph{transfer}. Around the time there was extensive work on exploring the transferability
of CNN features to various computer vision tasks through fine-tuning
 \cite{donahue2014decaf,oquab2014learning} or simply taking the last layer representation of CNNs
as high-level features as inputs to linear classifiers \cite{girshick2014rich,sharif2014cnn}.
Given such successes in vision it is natural to apply CNNs in the visual grounded language learning
community as black-box image feature extractors. Similary to \cite{bruni2014multimodal} in
\cite{kiela2014learning} the visual features of words are computed as the summary the feature vectors
extracted from all images they co-occur with $\mathbf{I} \in \mathbb{R}^{|V| \times i}$, where $i$ is
the size of the CNN image-embedding.
Applying the simple concatenation operation to the two spaces $[\mathbf{W};\mathbf{I}]$
they show that CNN features outperform BoVW features on word-similarity experiments with the multi-modal
word-representaitons.

All approaches described so far require both visual and textual information for the same concepts.
The multi-modal skip-gram \cite{lazaridou2015combining} model was developed to alleviate such limiation:
it is a multi-task extension of the skip-gram algorithm predicting the both the context of words, but also
the visual representations of concrete nouns. Visual representations are constructed to averaging
the CNN representations \cite{krizhevsky2012imagenet} of 100 pictures sampled from ImageNet \cite{deng2009imagenet}.
This architecture was later proposed as a model of child language learning in and
was applied to the CHILDES corpus \cite{macwhinney2014childes} with modifications to model referential
uncertainty and social cues \cite{lazaridou2016multimodal} and compared to human learning performance \cite{lazaridou2017multimodal}.

\paragraph{Our contribution}
Chapter~\ref{ch:TAL} presents a computational cognitive model of word learning using only
visual information developed at the same time as the multi-modal skip-gram approach.
Similarly to \cite{feng2010visual} we assume pairs of text and images
$<D, I>$. However, in our data set images represent every day scenes and are paired with descriptive
sentences to mimic the langauge environment of children.
Our model implements the extreme approach of cross-situational language
learning and assumes that the representations of words are learned exclusively through the co-occurences
between visual features and words. Contrary to \cite{kiela2014learning} rather than building a
matrix of image-features per word and then summarizing, we run an online expectation-maximization based
algorithm to align words with image features mimicing child lanuage learning.
In essence approach combines the cross-situational incremental word-learning model
of \cite{fazly.etal.10csj} with the modern convolutional image representations.
The result of the learning process is a word embedding matrix $\mathbf{W_i}$,
where each row $\mathbf{W_i}$ corresponts to a word, each column $\mathbf{W_{,j}}$
to a CNN feature and each entry $\mathbf{W_{i,j}}$ to the strength
of the relationship between the word $i$ and image-feature $j$.
We show through word-similarity experiments that, while our approach performs on
par with the SGNS trained on the same data set, there is a qualitiative difference between
the learned embeddings: our visual word-embeddings represent concrete nouns closer to human
judgements then abstract nouns. As each word embedding $\mathbf{W_i}$ is represented in the
image-feature space we conduct show that our model can label images with related nouns through
simple nearest neighbor search.

\subsection{From words to sentences}\todo{This section is very weak now gappy.}
\label{sec:sentences}
Applying the distributional intuition to model the meaning of sentences is not as straightforward
as it is for words.
Imagine building a sentence-embedding matrix $M$ where each row $M_i$ corresponds to
a possible sentence $i$ and each column $M,j$ to a feature $j$.
How many rows should we include? Intuitively the number of words in a corpus is much
lower than the number sentences and one can assume a large, but finite set
of existing words and a infinite set of
potential sentences to \emph{compose} from them.
Words can be though of as \emph{atomic units} and
in downstream applications one can use a lookup operation on a word embedding matrix
to represent units in the input. However, it is infeasible to lookup full sentences.
In fact, from a method that represents sentences in a continuous space one would expect
to also represent and generalize to unseen sentences at test time.
Furthermore, given two sentences $\mathit{John \; loves \; Mary}$ and
$\mathit{Mary \; loves \; John}$ we wish our sentence encoding function
$f_{enc}$ to represent the meaningful difference stemming from the word order
$f_{enc}(\mathit{John \; loves \; Mary}) \neq f_{enc}(\mathit{Mary \; loves \; John})$.
As such we seek to learn a sentence encoder $f_{enc}$ that is sensitive to
syntactic structure or in other words \emph{compositionality} i.e.:
the notion that the meaning of an expression is
a function of its parts and the rules combining them \cite{montague1970english}.

Earlier work developed under the framework of compositional distributional semantics produce
continuous representations for phrases up to sentences using additive and multiplicative interactions
of distributed word-representations \cite{mitchell2008vector} or combine symbolic and
continuous representations with tensor-products \cite{clark2007combining}.
The latter line of work culminated in a number of unified theories of
distributional semantics and formal type logical and categorical grammars
\cite{coecke2010mathematical,clarke2012context,baroni2014frege}.
These approaches assume that words are represented by distributional word embeddings
and define compositional operations on to of them motivated by particular formal
considerations. This line of work have not resulted in
machine learning approaches to solve natural language tasks
on real world data probably due to the different scope and
the computationally expensive high-order
tensor operations involved \cite{bowman2016modeling}.

However, before going forward with the more recent neural models,
it is worth mentioning that bag-of-words
based representations bypassing the issue of compositionality form a set of strong baselines
for a number of sentence level tasks \cite{hill2016learning}.
These simple baselines include unigram tf-idf vectors, which represent sentences as
smoothed word-count vectors or the sum/average of SGNS/GloVe word-embeddings.

\paragraph{Transferable sentence representations}
\label{sec:sentence}

Section \ref{sec:words} discusses the feed-forward \cite{bengio2003neural}
and recurrent network based models \cite{mikolov2010recurrent} from the perspective
of learning word-representaitons. However, both architectures learn embeddings
of sequences of larger sizes as such they do not only learn to represent
words, but phrases and potentially full sentences. When learning transferable
sentence representations there are two main considerations we will discuss:
1.) which architecture to choose and 2.) what objective to optimize.

Various neural network architectures have been proposed that operate on
handle variable sized data structures: recurrent networks such as
Elman network \cite{elman1991distributed},
LSTM \cite{hochreiter1997long} or GRU \cite{cho2014properties} take the input sequentially
b.) convolutional neural networks
\cite{kalchbrenner2014convolutional,zhang2015character,conneau2016very,chen2013learning}
process fixed sized n-gram patterns up to a large window,
c.) recursive neural networks \cite{goller1996learning.socher2011parsing,tai2015improved}
take a tree data structure as input such as a sentence according to the traversal of a dependency tree and
syntactic/semantic-dependency representations.
These architectures take word-embeddings as input compute a fixed size representations of
sentences. These representations are tuned to a specific task such as sequence tagging,
sentence classification, machine translation or the aforementioned language modeling.
d.) graph neural networks operate on graphs \cite{marcheggiani2017encoding} such as

In Chapters \ref{ch:COLI,ch:IJCNLP,ch:ConLL} we decided to apply
recurrent neural networks as sentence encoders.
Recursive neural networks provide a principled approach to
compute representations along the nodes of contituency
\cite{socher2013recursive} or dependency \cite{socher2014grounded} parse trees.
In practice, however, these architectures require parse trees as input,
which makes them impractical for our mission of learning visually grounded
representations for multiple languages.
For each language considered, the training procedure requires finding a good
pipeline from text pre-processing to parsing to generate the input representations
for the networks. Furthermore, tree structures by nature do not
afford straightforward batched computation directly and tend to run dramatically
slower than recurrent or convolutional models. In terms of performance the jury
is still out, however, so far only modest improvements have been observed
over recurrent models on specific tasks in specific settings
\cite{li2015tree,tai2015improved}. For graph structures neural networks the
same argument holds. Two equally practical alternatives to RNNs
that operate on raw sequences are convolutional neural networks
\cite{bai2018empirical} and transformers \cite{vaswani2017attention}.


Contrary to word-embeddings the representations
and the composition function these models learn are task specific and not \emph{universal}.
For learning transferable and general distributed sentence representations the first notable approach is the
skip-thought vectors model \cite{kiros2015skip}. This approach extends distributional intuition
of the skip-gram approach to sentence level. They train their a sentence encoder on contiguous sentence sequences
-- such as books -- which learnes representations predictive of the sentences around it. More specifically
they train a recurrent encoder to encode sentence $s_i$ and use two separate recurrent decoders to
generate the pre- $s_{i-1}$ and post-contexts $s_{i+1}$.
This method was confirmed to be a successful unsupervised method for transfer learning in a number of sentence
classifications tasks beating simpler approaches such as bag-of-words approaches based on skip-gram
or CBOW embeddings, tf-idf vectors and auto-encoders \cite{hill2016learning}. A convolutional
variant of the encoder was introduced in \cite{gan2016unsupervised} and several other works
train simple sum/average pre-trained word-embedding encoders using the same context prediction-style
loss \cite{kenter2016siamese,hill2016learning}. The larger context of paragraphs are
explored in \cite{jernite2017discourse} where the text is to predict discorse coherence
labels in an unsuprevised fashion.

Later approaches have also focused on moving away from distributional clues
and identifying supervised tasks  that lead to representations that generalize
well to a wide variety of other tasks. Natural language inference as an objetive
was idetified to learn good sentence embeddings \cite{conneau-EtAl:2017:EMNLP2017}
and \cite{subramanian2018learning} combinea a number of other supervised tasks
with unsupervised training through multi-task learning.
The state-of-the-art in learning universal sentence representations at the time of writing the present thesis is
represented by ELMo approach \cite{peters2018deep}. This approach goes back to
exploiting distributional cues and trains a stack convolution and recurrent layers
on large-scale bidirectional language modeling.
For each task the forward and backward representations are extracted from the model
for each word position and then fed to a task specific recurrent network.
Such contextualized word-representations have been explored before ELMo using
the LSTM states of machine translation encoders \cite{mccann2017learned}.
This approach poses a powerful alternative to universal sentence encoders.

\subsection{Visually grounded sentence representations}\todo{super difficult}
\label{sec:visualsentences}

When aiming to learn general sentence representations text-only tasks
are usually considered by researchers leadning to the grounding problem
as discussed in Section~\ref{sec:visualwords}. Given that visual information
has been shown to contain useful information for word-representations it
is a natural question to ask whether this observation generalizes to sentence
embeddings. We have seen in Section~\ref{sec:sentence} that sentence embedding
models exploit the distributional hypothesis by treating sentences as units and
predict their context. This intuition can be adopted to visual grounding i.e:
train sentence embeddings to be predictive of their visual context. The
structure of the data sets required for such a setup has the same form
as the visual-textual LDA approach of \cite{feng2010visual} or our
cross-situational word learning model \cite{kadar2015learning}:
pairs of sentences $s$ and images $i$ $<s, i>$.

The architecture we chose in
Chapters \ref{ch:COLI,ch:IJCNLP,ch:ConLL} trains a recurrent neural network to
represent variable length sentences and extract image features using pre-trained
convolutional neural networks. In Chapter \ref{ch:COLI} our loss function
is quite straightforward: the objective is to minimize the cosine distance
between the learned sentence and image representations. In later chapters we
apply a loss function similar to the negative sampling
in the skip-gram model \cite{mikolov2013distributed}: it minimizes the cosine
distance between true sentence-image pairs $<s, i>$ and maximizes the distance
between random constrastive pairs $<\hat{s}, i>$ and $<s, \hat{i}>$.
Representations learned by such a model has been shown \cite{kiela2017learning}
to improve perfomance when combined with skip-thought embeddings
on a large number of semantic sentence classifications tasks compared to
skip-thought only. Their findings were confirmed and improved upon using a
self-attention mechanism \cite{yoo2017improving}.

\paragraph{Note on data sets}


The data sets used for this purpose in all chapters were originally
introduced for image-captioning.
These data sets annotate images found in online resources with descriptions
through crowd-sourcing.
The descriptions collected for these data sets are largely \emph{conceptual}, \emph{concrete}
and \emph{generic}.
This means that descriptions do not focus too much on \emph{perceptual}
information such as colors, contrast or picture quality; they do not mention
many \emph{abstract} notions about images such as mood and finally the descriptions
are not \emph{specific} meaning that they do not mention the names of cities,
people or brands of objects.
What they do end up mentioning are entities depicted in the images
(frizbee, dog, boy) their attributes (yellow, fluffy, young) and their
relations between each other.
The images depict common real-life scenes such as bus turning left or people
playing soccer in the park. As such annotation collected independently from
different workers end up focusing on different aspects of these scenes\footnote{For a comprehensive overview on image-description data sets
please consult \cite{bernardi2016automatic}}.
.
%For our multi-lingual experiments we use the Multi30K data set \cite{elliott2016multi30k,elliott2017findings}; a multilingual extension of Flickr30K. It consists of a {\it translation} and a {\it comparable} portions both containing all images from the original Flickr30K. The \emph{translation} portion annotates a single English description with a German, French and Czech caption, while the \emph{comparable} the original 5 English and additional 5 German descriptions collected independently using the same crowd-sourcing process.

\paragraph{Our contribution}


In \cite{chrupala2015learning} we train a multi-task architecture consisting
of a shared word-embedding matrix and two pathways: 1.) common recurrent language
model trained to maximize the probability of a word given the history,
2.) grounded representation learning model predicting the image representation paired with the sentences.
This model combines the distributional hypothsis for learning the meaning of words
and the grounded learning on the sentence level.
We show that the word-representations learned by the two path architecture
predict human word-similarity judgements with higher accuracy then the one-path
architecutres. Furthermore, the combined model is more sensitive to word-order than
the visual-only model.  Lastly we show that images act as an
anchor and the architecture performs paraphrase retrieval well above chance
level.

Chapter~\ref{ch:COLI} is dedicated to develop techniques to interpret the
opaque continuous representations of this model. It provides an in-depth
comparative analyisis of the linguistic knowledge represented by the
language-model and visual-pathways of this architecture. Our
\emph{omission-score} and hidden-unit visualization technique has inspired the
LSTMVis recurrent netowrk visualization toolkit \cite{strobelt2018lstmvis} and
OTHER SHIT.

In Chapter~\ref{ch:IJCNLP} we show that in the domain of image-captions
grounded learning improves translation quality and that learning multi-modal
representations provides gains on top of learning from larger bilingual corpora.
This lead to the hypothesis that much of the observed improvements of
multi-modal translation models over text-only baselines is due to grounded
learning and not to the effective use of visual context. This was later confirmed
in CITE DESMOND where only one of the considered three state-of-the-art multi-modals
shown sensitivity to the input image. SOMEOW MENTION THE FOLLOWUP HUMAN STUDY TOO.


%\subsection{Multilingual representations of words and sentences}\todo{this will be challenging, not sure yet how to build it up.}
%Multilingual or cross-lingual word embedding approaches map words of different languages in a shared space. These representations allow to transfer knowledge between languages and
%perform cross-lingual tasks such as cross-lingual document retrieval. The bulk of such approaches apply the techniques developed for monolingual word-embedding induction as a component
%of their full pipeline. One of the early approaches trains two separate word spaces with using the negative-sampling implementation of the skip-gram algorithm and then maps these spaces onto eachother by minimizing the squared loss between the 5000 top most frequent word-vectors \cite{mikolov2013exploiting}. This main approach went through improvents
%such as by switching mean-squared error to ranking loss \cite{lazaridou2015hubness} or enforcing orthogonality constraint on the mapping matrix
%\cite{xing2015normalized}   (WE DID THE SAME OVER TIME). Another prominent technique to learn a mapping between word (and other types of) spaces is Canonical Correlation Analysis, first
%used by \cite{faruqui2014improving}, which was later improved by the application of Deep CCA \cite{lu2015deep}. Another approach is to create a pseudo-bilingual corpora by replacing words
%in a source language by their translation and train an embedding model on the resulting corpus
%The uber multilingual word-embedding \cite{ammar2016massively}.

\subsection{Visually grounded multilingual representations}\todo{not much work here so this will be easy.}
On top of the visual modality anchoring linguistic representations to perceptual reality it
also provides a universal representation to bridge languages of varying typological distance.
On word level images as a common ground have been applied to induce bilingual lexicons without parallel corpora.
The lack of bi-text in this setting have been traditionally solved by methods relying on textual features
such as ortographic similarity \cite{haghighi2008learning} or similar diachronic distributional
statistical trends between words \cite{schafer2002inducing}.
However, images tagged with various labels in a multitude of languages are available on the internet
and given a model of image-similarity can act as pivot to find translations between such tags.
The first approach to construct a dictionary based on image similarity was developed by \cite{bergsma2011learning}.
They use Google image search to find relevant images for the names of physical objects in multiple languages.
These images are then represent by bag-of-visual-words vectors based  on SIFT and color features.
Their results are vastly improved by applying pre-trained convolutional neural networks to represent words
in the visual space \cite{kiela2014improving}. However, \cite{hartmann2017limitations} present a negative result
showing that such techniques scale poorly to non-nouns such as adjectives or verbs.
\cite{vulic2016multi} combine image-pivot based bilingual word-representations with more traditional multi-lingual
word-embedding techniques and show that the combined multi-modal representations are superior compared to
their uni-modal counterparts. At the time of writing this thesis the first large-scale vision based bilinigual
dictionary induction data set and method was put forward by \cite{hewitt2018learning}. They create a data set
of \~200K words and 100 images per each using Google Image Search and perform experiments with 32 languages.
They confirm the finding of \cite{hartmann2017limitations} that image-pivoting is most effective for
nouns, however, find that using their larger data set adjectives can also be translated reliably. They also
show through combining visual and textual word-representations for word-translation through a re-ranking
experiment that visual representations improve the state-of-the-art text-only methods in the case of
\emph{concrete} concepts.
Images have been also used as a pivot to aid translation of visually descriptive sentences.
A common technique in statistical machine translation is to apply a heuristic re-ranking algorithm
to a sampled list of candidate translations from a translation model given the source sentence.
\cite{hitschler2016multimodal} consider the setting of image-caption translation where an
image-caption pair is given on the source side and on top of the parallel source-target sentence corpus
additional target-side image-caption corpus is available. The show that using the images in the re-ranking
pipeline improves translation performance upon using solely textual-similarity.
\cite{nakayama2017zero} apply image-pivoting in the zero-resource machine translation setting.
The task is to translate from English to German without aligned parallel corpora, however,
separate image-description data sets are available in both languages. They solve the problem by
training two components: 1.) visual-sentence encoder that matches images and their descriptions,
2.) image-description generator maximizing the likelihood of gold standard captions given the images.
At test time the visual-sentence encoder representation of the source sentence is fed to the
image-description generator to produce the translation. There results were improved later by modeling
the image-pivot based zero-resource translation setup as a multi-agent communication game between
encoder and decoder \cite{chen2018zero,lee2017emergent}.
Images and their descriptions in multiple languages can be taken as different views of the
same underlying semantic concepts. From this multi-view perspective learning common representations
of multiple languages and perceptual stimuli can potentially exploit the complementary information
between views to learn better representations, but also leads to practical applications such as
cross-modal and cross-lingual retrieval or similarity. The deep partial canonical correlation
analysis approach learns multilingual English-German sentence embeddings conditioned on the
representation of the images they are aligned to \cite{rotman2018bridging}. They show that their model using
the visual modality as an extra view finds better mulilingual sentence and word representations
as demonstrated by cross-lingual paraphrasing and word-similarity results.
The bridge correlational neural network approach \cite{rajendran2015bridge} learns common representations
even when the different views only need to be aligned with one pivot view.
In the vision and language domain they preform image-sentence retrieval experiments in French or German where
the image-caption data set is only available for English, however, there is parallel corpora between German
or French and English. In other words English acts as a pivot.
\cite{gella2017image} considers images as a pivot bridging English and German and train a multilingual
image-sentence ranking model -- recurrent-convolutional architecture as in Chapters \ref{ch:COLI,ch:IJCNLP,ch:ConLL}.
There results suggest that the multilingual models outperform the monolingual ones on image-sentence
ranking, however, they do not show consistent gains across languages and model settings.
In Chapter \ref{ch:ConLL} we implement a similar setup and do show that both English and German image-sentence
ranking performance is reliably improved by bilingual joint training. We expand the results further and
provide evidence that more gains can be achieved by adding more views: on top of English and German, we
add French and Czech captions. Revisiting the issue of alignment we find that the unlike in previous
experiments \cite{gella2017image,calixto2017multilingual,rotman2018bridging} image-pivoting leads to
better visual-sentence embeddings even when the images are not matched between languages. Finally
the performance on the lower resource French and Czech languages is improved when we add the larger
English and German image-caption sets, showing successfull multilingual transfer in the vision-language
domain. A similar experiment was conducted in the image-captioning domain where \cite{miyazaki2016cross}
transfer knowledge from the larger original English corpus to smaller Japanese.



%\subsection{Grounding and other modalities}
%Grounding words in auditory signals \cite{kiela2015multi,lopopolo2015sound} and
% olfactory perception \cite{kiela2015grounding}r. Growing body of literature in learning joint representations
% of speech and images \cite{harwath2016unsupervised,chrupala2017representations,harwath2018jointly}


%Mention semantic parsing and the WebNLG paper i did with Thiago. Mention the TextWorld thing. Grzegorz
%and Afra´s work speech, plus the fail paper that im on too.

\subsection{Interpreting continuous representations}\todo{This section could be infinite, but i tried to give a consice view on the matter.}
\label{sec:interpret}
The word, phrase and sentence representations learned through neural architectures are notoriously opaque.
Empirical evidence shows that incorporating various views such as multiple languages and vision to
learn representations improves performance on downstream tasks. But why? What are the linguistic
regularities represented in the recurrent states that lead to the final performance metric? Did the
model learn to exploit trivial artifacts in the training data or did it learn to generalize?
What characterizes the features individual features recurrent networks extract from the input strings?
The main topic of my thesis is learning visually grounded representations for linguistic units.
I argue that it is crucial to assess the difference in the representations between textual only
and multimodal representations not only from a quantitative point of view,
but also from a qualitative linguistics angle. This is what Chapter \ref{ch:COLI} is dedicated to.

Developing techniques for interpreting machine learning models have multiple goals.
From the practical point of view as learning algorithms make their way into critical applications
such as medicine humans and machines need to be able to co-operate to avoid catastrophic
outcomes \cite{caruana2015intelligible}. As such there is a growing interest in deriving methods
to \emph{explain} the decision of such architectures.
These methods assign a real-valued "relevance" score to each unit in the input signal,
which signifies how much impact it had on the final prediction of the model.
One of the first paradigms in generating such relevance scores is gradient based methods:
they take the gradient of the output of the network with respect to the input \cite{simonyan2013deep}.
Deep neural models of language tasks learn distributed representations of input symbols
 and as such further operations have to applied to reduce the resulting gradient vectors to
single scalars e.g. using $\ell_2$ norm \cite{bansal2016ask}. Another prominent and well studied
approach still based on gradient information is layerwise
relevance propagation \cite{bach2015pixel}. The output of the final layer
is written as the sum of the relevance-scores from the input and similarly to the back-propagation
algorithm the relevance of each neuron
recursively depends on the lower-layer all the way down to the input signal.
Different versions of relevance propagation run the backward pass with different rules taking as
input gradient information and activation values. It was later theoretically analyzed and generalized
in to deep taylor decomposition method \cite{binder2016layer} and shown to be equivalent to taking the
gradients with respect to the input at in \citep{simonyan2013deep} and multiplying it elementwise
with the input itself \cite{shrikumar2017learning}.
LRP was also later derived for recurrent architectures \cite{arras2017explaining} to describe
sentiment classifiers. Another backpropagation based algorithm is DeepLIFT, which instead of
using the gradients for computing the relevance scores it uses the difference between the activations
that result from a specific compared  to a baseline input.
Perturbation based methods are also gradient free.
LIME \cite{ribeiro2016should} and its NLP specific extension LIMSSE \cite{poerner2018evaluating} perturbs the
the input creating a local neighborhood around it and fits interpretable linear models to explain
the predictions of any complex black box classifier.
Even simpler perturbation based techniques apply perturbations to the input and measure the
difference between the original input and the various perturbed candidates such as the erasure
\cite{li2016understanding} and our omission \cite{Kadar2016} method.

This is in line with another angle of model interpretability;
training a complex opaque model and then using it to discover patterns in the
input data that are crucial in solving the task.
Deep neural networks learn to solve tasks from close to raw input that humans receive and as such
uncovering the regularities they learn can shed light into the patterns humans might extract
from data to cope with certain tasks.
Recent methodology in probing the learned representations of LSTM language models, in fact, resemble psycholinguistic studies.
A number of experiments using the agreement prediction paradigm \cite{bock1991broken} suggest that
LSTM language models successfully learn syntactic regularities as opposed to memorizing surface patterns
\cite{linzen2016assessing,enguehard2017exploring,bernardy2017using,gulordava2018colorless}.

\paragraph{Our contribution}
The \emph{omission} method we develop in Chapter~\ref{ch:COLI} removes words from the input while the \emph{occlusion} \cite{li2016understanding}
method replaces these inputs with a baseline.  However, our aim in Chapter~\ref{ch:COLI} was not to develop a method for explanation, but to
shed light on the linguistic characteristics of the input grounded learning models learn in contrast to text-only language models.

%\section{Vision and Language applications}
%Mention the bunch of work thats here. Do image-captioning and image-sentence ranking and then metion that we do the latter always. Say the importance of attention in this field and mention the DIDEC corpus. Mention VQA and fine-grained reasoning and the FigureQA paper.



%\section{Methods}

%\subsection{Data sets}
%For image-feature extraction all approaches presented in the thesis use some
%CNN architecture pre-trained on a version of the ImageNet data set \cite{deng2009imagenet} prepared for a large-scale image classification challenge \cite{russakovsky2015imagenet}. ImageNet annotates the noun synsets from WordNet \cite{miller1995wordnet} with images collected from the internet. At the time of writing this thesis on average the data set consists of 500 images per node. The subsection used in the pre-trained networks is the ILSVRC2012 containing 1.2 million images annotated with 1000 classes. For learning grounded language representation we use data sets introduced for image-captioning. These data sets annotate images found in online resources with descriptions through crowd-sourcing. The smallest data set we use is Flickr8K \cite{hodosh2013framing} with 8000 images, which was later extended to contain 30K images resulting in the Flickr30K benchmark \cite{young2014image} and finally the largest collection we use is the COCO image-caption benchmark \cite{chen2015microsoft} with 123K images. All data sets contain 5 captions per image.
%The descriptions collected for these data sets are largely \emph{conceptual}, \emph{concrete} and \emph{generic}. This means that descriptions do not focus too much on \emph{perceptual} information such as colors, contrast or picture quality; they do not mention many \emph{abstract} notions about images such as mood and finally the descriptions are not \emph{specific} meaning that they do not mention the names of cities, people or brands of objects. What they do end up mentioning are entities depicted in the images (frizbee, dog, boy) their attributes (yellow, fluffy, young) and their relations between each other. The images depict common real-life scenes such as bus turning left or people playing soccer in the park. As such annotation collected independently from different workers end up focusing on different aspects of these scenes. For a comprehensive overview on image-description data sets please consult \cite{bernardi2016automatic}.
%For our multi-lingual experiments we use the Multi30K data set \cite{elliott2016multi30k,elliott2017findings}; a multilingual extension of Flickr30K. It consists of a {\it translation} and a {\it comparable} portions both containing all images from the original Flickr30K. The \emph{translation} portion annotates a single English description with a German, French and Czech caption, while the \emph{comparable} the original 5 English and additional 5 German descriptions collected independently using the same crowd-sourcing process.


%\subsection{Architectures}
%Just do the usual GRU + CNN thing. cite Jamie (heart).

%\subsubsection{Recurrent network}
%Do the GRU mention that the LSTM is strictly more powerful according to Yoav, but it doesnt seem to matter at all according to Chung.

%\subsubsection{Convolutional network}
%Just super quick run-down on VGG and ResNet.

%\subsubsection{Optimization}
%Just mention that all this shit is optimized with some version of SGD and we use Adam all the time. Mention that Adam sucks but its just some common thing to do. Say this thing about the MSE or cosine loss that we start with that but i results in hubness actually as Angeliki points it out. So we move to the contrastive kinda losses.

%\subsection{Transfer learning}
%Don't really worry about it too much just say something about how awesome is that the CNNs trained on image net actually end up being so useful for us.

%\subsection{Multi-task learning}
%Multi-task learning has been an important paradigm since the earliest works on training neural architectures for natural-language processing \cite{collobert2008unified}. Multi-task learning involves optimizing multiple loss function jointly usually with the goal of exploiting commonalities between tasks. As described in the seminal work of \cite{caruana1997multitask}:

%\begin{quote}
%Multitask Learning is an approach to inductive transfer that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias. It does this by learning tasks in parallel while using a shared representation; what is learned for each task can help other tasks be learned better.
%\end{quote}

%Introducing inductive bias using different tasks means that the learning algorithm prefers a specific set of hypotheses over others just as in the case of other forms of regularization such as $\ell_1$ or $\ell_2$ penalties. Here I only discuss the specific instantiation of the mutli-task learning paradigm that are applicable to the chapters presented in this thesis and for a more comprehensive overview please consult \cite{ruder2017overview}. The technique used in the thesis
%is hard-parameter sharing \cite{caruana1997multitask}
