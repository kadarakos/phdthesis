\section{Conclusions}
%\newcite{gella2017image} showed that monolingual image-sentence ranking can be improved by bilingual joint training on comparable captions using the same images. 
We learn multilingual multimodal sentence embeddings and show that multilingual joint training improves over bilingual joint training. We also demonstrate that low-resource languages can benefit from the additional data found in high-resource languages. Our experiments suggest that either translation pairs or independently-collected captions improve the performance of a multilingual model, and that the latter data setting provides further improvements through a caption--caption ranking objective. We also show that when collecting data in an additional language, it is better to collect captions for the existing images because we can exploit the caption--caption objective. Our results lead to several directions for future work. We would like to pin down the mechanism via which multilingual training contributes to improved performance for image-sentence ranking. Additionally, we only consider four languages and show the gain of multilingual over bilingual training only for the English-German language pair. In future work we will incorporate more languages from data sets such as the Chinese Flickr8K \citep{li2016adding} or Japanese COCO \citep{P16-1168}. 
%Lastly, we share all parameters across the languages; in future we will investigate other parameter sharing regimes, including character-level modeling which enables more flexible parameter sharing between languages.

%We train multilingual, multimodal sentence embeddings and through a series of image-sentence ranking experiments we show that multilingual training improves over bilingual joint training and  low-resource languages can benefit from data from high-resource languages. Our experiments suggest that both translation pairs and independently-collected captions improve performance in the multilingual setup and that the latter data setting provides more gains overall. We also find that when collecting data in an additional language, reusing the same images is more beneficial as it is possible to exploit the caption-caption (c2c) objective which enables further gains. Our results, however, lead to several new questions to be answered in future work. We would like to pin down the mechanism via which multilingual training contributes to improved performance for image-sentences ranking. Additionally we only consider four languages and show the gain of multilingual over bilingual training only for the English-German language pair. In future work we will incorporate more languages from data sets such as the Chinese Flickr8K \cite{li2016adding} or Japanese COCO \cite{P16-1168}. 
%Lastly, we share all parameters across the languages; in future we will investigate other parameter sharing regimes, including character-level modeling which enables more flexible parameter sharing between languages.