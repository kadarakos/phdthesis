\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Representation of linguistic form and function in recurrent neural networks}{85}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:COLI}{{4}{85}{Representation of linguistic form and function in recurrent neural networks}{chapter.4}{}}
\@writefile{toc}{\contentsline {paragraph}{abstract}{85}{chapter.4}}
\citation{elman1990finding}
\citation{vinyals2015grammar}
\citation{bahdanau2014neural}
\citation{gregor2015draw}
\citation{visin2015reseg}
\citation{karpathy2015deep}
\citation{yu2015video}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Introduction}{86}{section.4.1}}
\newlabel{sec:intro}{{4.1}{86}{Introduction}{section.4.1}{}}
\citation{chrupala2015learning}
\citation{cho2014properties,chung2014empirical}
\citation{elman1991distributed,karpathy2015visualizing}
\newlabel{explainimaginet}{{4.1}{87}{Introduction}{section.4.1}{}}
\citation{elman1990finding}
\citation{jordan1986attractor}
\citation{elman1991distributed}
\citation{giles1992extracting}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Related work}{88}{section.4.2}}
\newlabel{sec:related}{{4.2}{88}{Related work}{section.4.2}{}}
\citation{li2015visualizing}
\citation{hochreiter1997long}
\citation{bahdanau2014neural}
\citation{rocktaschel2016reasoning}
\citation{karpathy2015visualizing}
\citation{li2015convergent}
\citation{krizhevsky2012imagenet}
\citation{erhan2009visualizing,simonyan2013deep,yosinski2015understanding,nguyen2016multifaceted}
\citation{mahendran2015visualizing,dosovitskiy2015inverting}
\newlabel{edit:attention}{{4.2}{90}{Related work}{section.4.2}{}}
\citation{li2016understanding}
\citation{adi2016fine}
\citation{linzen2016assessing}
\citation{simonyan2013deep,yosinski2015understanding,mahendran2015understanding}
\citation{eigen2013understanding}
\citation{zhou2014object}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Models}{92}{section.4.3}}
\citation{chrupala2015learning}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Gated Recurrent Neural Networks}{93}{subsection.4.3.1}}
\newlabel{sec:gru}{{4.3.1}{93}{Gated Recurrent Neural Networks}{subsection.4.3.1}{}}
\newlabel{eq:gru-update}{{4.2}{93}{Gated Recurrent Neural Networks}{equation.4.3.2}{}}
\newlabel{eq:gru-cand}{{4.3}{93}{Gated Recurrent Neural Networks}{equation.4.3.3}{}}
\newlabel{eq:gru-reset}{{4.4}{93}{Gated Recurrent Neural Networks}{equation.4.3.4}{}}
\citation{chrupala2015learning}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Structure of {\sc  Imaginet}, adapted from \cite {chrupala2015learning}.\relax }}{94}{figure.caption.26}}
\newlabel{fig:imaginet}{{4.1}{94}{Structure of {\sc Imaginet}, adapted from \protect \cite {chrupala2015learning}.\relax }{figure.caption.26}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Imaginet}{94}{subsection.4.3.2}}
\newlabel{sec:imaginet}{{4.3.2}{94}{Imaginet}{subsection.4.3.2}{}}
\citation{simonyan2014very}
\citation{chrupala2015learning}
\citation{chrupala2015learning}
\newlabel{eq:losscombo}{{4.11}{95}{Imaginet}{equation.4.3.11}{}}
\newlabel{edit:dumdumeddy}{{\TextOrMath  {\textasteriskcentered }{*}}{95}{}{Hfootnote.18}{}}
\newlabel{ft:imaginet}{{\TextOrMath  {\textdagger }{\dagger }}{95}{}{Hfootnote.19}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.3}Unimodal language model}{96}{subsection.4.3.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.4}Sum of word embeddings}{96}{subsection.4.3.4}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Experiments}{96}{section.4.4}}
\newlabel{sec:experiments}{{4.4}{96}{Experiments}{section.4.4}{}}
\citation{lin2014microsoft}
\citation{simonyan2014very}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Computing Omission Scores}{97}{subsection.4.4.1}}
\newlabel{sec:computeomission}{{4.4.1}{97}{Computing Omission Scores}{subsection.4.4.1}{}}
\newlabel{edit:whyposdep}{{4.4.1}{97}{Computing Omission Scores}{subsection.4.4.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Omission scores for the example sentence {\it  a baby sits on a bed laughing with a laptop computer open} for {\sc  LM} and the two pathways, {\sc  Textual} and {\sc  Visual}, of {\sc  Imaginet.}\relax }}{98}{figure.caption.27}}
\newlabel{fig:omissionex}{{4.2}{98}{Omission scores for the example sentence {\it a baby sits on a bed laughing with a laptop computer open} for {\sc LM} and the two pathways, {\sc Textual} and {\sc Visual}, of {\sc Imaginet.}\relax }{figure.caption.27}{}}
\newlabel{eg:omit}{{4.12}{98}{Computing Omission Scores}{equation.4.4.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Images retrieved for the example sentence {\it  a baby sits on a bed laughing with a laptop computer open} (left) and the same sentence with the second word omitted (right).\relax }}{99}{figure.caption.28}}
\newlabel{fig:omissionexpic}{{4.3}{99}{Images retrieved for the example sentence {\it a baby sits on a bed laughing with a laptop computer open} (left) and the same sentence with the second word omitted (right).\relax }{figure.caption.28}{}}
\newlabel{edit:retrievalexplain}{{4.4.1}{99}{Computing Omission Scores}{equation.4.4.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.2}Omission score distributions}{99}{subsection.4.4.2}}
\newlabel{sec:omitimaginet}{{4.4.2}{99}{Omission score distributions}{subsection.4.4.2}{}}
\newlabel{subsec:omission-text-vis}{{4.4.2}{99}{Omission score distributions}{subsection.4.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Distribution of omission scores for POS (left) and dependency labels (right), for the {\sc  Textual} and {\sc  Visual} pathways and for {\sc  LM}. Only labels which occur at least 1250 times are included.\relax }}{100}{figure.caption.29}}
\newlabel{fig:omission-imaginet}{{4.4}{100}{Distribution of omission scores for POS (left) and dependency labels (right), for the {\sc Textual} and {\sc Visual} pathways and for {\sc LM}. Only labels which occur at least 1250 times are included.\relax }{figure.caption.29}{}}
\newlabel{ft:boxplots}{{\TextOrMath  {\textsection }{\mathsection }}{100}{}{Hfootnote.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Distributions of log ratios of omission scores of {\sc  Textual} to {\sc  Visual} per POS (left) and dependency labels (right). Only labels which occur at least 1250 times are included.\relax }}{101}{figure.caption.30}}
\newlabel{fig:omission-imaginet-ratio}{{4.5}{101}{Distributions of log ratios of omission scores of {\sc Textual} to {\sc Visual} per POS (left) and dependency labels (right). Only labels which occur at least 1250 times are included.\relax }{figure.caption.30}{}}
\newlabel{edit:textualomission}{{4.4.2}{101}{Omission score distributions}{figure.caption.30}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Distributions of log ratios of omission scores of {\sc  LM} to {\sc  Textual} per POS (left) and dependency labels (right). Only labels which occur at least 1250 times are included.\relax }}{103}{figure.caption.31}}
\newlabel{fig:omission-imaginet-quotient}{{4.6}{103}{Distributions of log ratios of omission scores of {\sc LM} to {\sc Textual} per POS (left) and dependency labels (right). Only labels which occur at least 1250 times are included.\relax }{figure.caption.31}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.3}Beyond Lexical Cues}{103}{subsection.4.4.3}}
\newlabel{sec:beyondlexical}{{4.4.3}{103}{Beyond Lexical Cues}{subsection.4.4.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Proportion of variance in omission scores explained by linear regression.\relax }}{104}{table.caption.32}}
\newlabel{tab:lr-r2}{{4.1}{104}{Proportion of variance in omission scores explained by linear regression.\relax }{table.caption.32}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces Proportion of variance in omission scores explained by the linear regression models for {\sc  Sum}, {\sc  LM}, {\sc  Visual} and {\sc  Textual}, relative to regressing on word identity and position only. \relax }}{106}{figure.caption.33}}
\newlabel{fig:rsquared}{{4.7}{106}{Proportion of variance in omission scores explained by the linear regression models for {\sc Sum}, {\sc LM}, {\sc Visual} and {\sc Textual}, relative to regressing on word identity and position only. \relax }{figure.caption.33}{}}
\@writefile{toc}{\contentsline {subsubsection}{Sensitivity to grammatical function}{106}{figure.caption.33}}
\newlabel{sec:gramfunc}{{4.4.3}{106}{Sensitivity to grammatical function}{figure.caption.33}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces Distribution of omission scores per dependency label for the selected word types.\relax }}{107}{figure.caption.34}}
\newlabel{fig:top_words}{{4.8}{107}{Distribution of omission scores per dependency label for the selected word types.\relax }{figure.caption.34}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.9}{\ignorespaces Coefficients on the y-axis of {\sc  LR full} corresponding to the position variables on the x-axis.\relax }}{108}{figure.caption.35}}
\newlabel{fig:posrqs}{{4.9}{108}{Coefficients on the y-axis of {\sc LR full} corresponding to the position variables on the x-axis.\relax }{figure.caption.35}{}}
\@writefile{toc}{\contentsline {subsubsection}{Sensitivity to linear structure}{108}{figure.caption.34}}
\newlabel{subsec:information-struct}{{4.4.3}{108}{Sensitivity to linear structure}{figure.caption.34}{}}
\citation{karpathy2015visualizing,li2015convergent}
\newlabel{edit:topiccomment}{{4.4.3}{109}{Sensitivity to linear structure}{figure.caption.35}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.4}Lexical versus abstract contexts}{109}{subsection.4.4.4}}
\newlabel{sec:contexts}{{4.4.4}{109}{Lexical versus abstract contexts}{subsection.4.4.4}{}}
\citation{li2015convergent}
\@writefile{lof}{\contentsline {figure}{\numberline {4.10}{\ignorespaces Distributions of the mutual information scores for the three networks and the six context types.\relax }}{111}{figure.caption.36}}
\newlabel{fig:raw_mutual}{{4.10}{111}{Distributions of the mutual information scores for the three networks and the six context types.\relax }{figure.caption.36}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.11}{\ignorespaces Bootstrap distributions of log ratios of median mutual information scores for word and dependency contexts. Left: {\sc  Textual} vs {\sc  Visual}; right: {\sc  LM} vs {\sc  Textual}\relax }}{112}{figure.caption.37}}
\newlabel{fig:mi-boot}{{4.11}{112}{Bootstrap distributions of log ratios of median mutual information scores for word and dependency contexts. Left: {\sc Textual} vs {\sc Visual}; right: {\sc LM} vs {\sc Textual}\relax }{figure.caption.37}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces Dimensions most strongly associated with the dependency trigram context type, and the top five contexts in which these dimensions have high values.\relax }}{113}{table.caption.38}}
\newlabel{tab:mi-examples}{{4.2}{113}{Dimensions most strongly associated with the dependency trigram context type, and the top five contexts in which these dimensions have high values.\relax }{table.caption.38}{}}
\citation{kai2015treelstm}
\citation{yoonneural2014}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Discussion}{114}{section.4.5}}
\newlabel{sec:conclusion}{{4.5}{114}{Discussion}{section.4.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.1}Generalizing to other architectures}{114}{subsection.4.5.1}}
\newlabel{edit:omitgeneral}{{4.5.1}{114}{Generalizing to other architectures}{subsection.4.5.1}{}}
\citation{sutskever2014sequence}
\citation{kiros2015skip}
\citation{bahdanau2014neural}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.2}Future directions}{115}{subsection.4.5.2}}
\newlabel{edit:humanjudgement}{{4.5.2}{115}{Future directions}{subsection.4.5.2}{}}
\@setckpt{chapters/COLI/double_paper}{
\setcounter{page}{117}
\setcounter{equation}{13}
\setcounter{enumi}{4}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{4}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{4}
\setcounter{section}{5}
\setcounter{subsection}{2}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{11}
\setcounter{table}{2}
\setcounter{ContinuedFloat}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{r@tfl@t}{0}
\setcounter{parentequation}{0}
\setcounter{float@type}{8}
\setcounter{algorithm}{1}
\setcounter{ALG@line}{18}
\setcounter{ALG@rem}{0}
\setcounter{ALG@nested}{0}
\setcounter{ALG@Lnr}{2}
\setcounter{ALG@blocknr}{10}
\setcounter{ALG@storecount}{0}
\setcounter{ALG@tmpcounter}{0}
\setcounter{treecount}{0}
\setcounter{branchcount}{0}
\setcounter{dt@labelid}{0}
\setcounter{DefaultLines}{2}
\setcounter{DefaultDepth}{0}
\setcounter{L@lines}{0}
\setcounter{L@depth}{0}
\setcounter{Item}{4}
\setcounter{Hfootnote}{21}
\setcounter{Hy@AnnotLevel}{0}
\setcounter{bookmark@seq@number}{68}
\setcounter{eu@}{0}
\setcounter{eu@i}{0}
\setcounter{mkern}{0}
\setcounter{lofdepth}{1}
\setcounter{lotdepth}{1}
\setcounter{@pps}{0}
\setcounter{@ppsavesec}{0}
\setcounter{@ppsaveapp}{0}
\setcounter{NAT@ctr}{0}
\setcounter{@todonotes@numberoftodonotes}{0}
\setcounter{section@level}{0}
}
