%\documentclass[shortpaper]{clv3}  %for single-spaced version
%\documentclass[manuscript]{clv3} %for double-spaced version

%\usepackage[utf8]{inputenc}
%\usepackage{amsmath}
%\usepackage{graphicx}
%\usepackage{algorithm}

%\usepackage{algorithmicx}
%\usepackage{algpseudocode}
%\usepackage{amssymb}
%\usepackage{pifont}
%\usepackage{todonotes}
%\usepackage{url}
%\usepackage{array,multirow,graphicx}
%\usepackage{rotating}
%\usepackage{times}
%\usepackage{latexsym}
%\usepackage{arydshln}
%\usepackage{enumitem}
%\setlist{nolistsep} % needs enumitem

%\issue{43}{3}{2017}

\chapter{Representation of linguistic form and function in recurrent neural networks}
\label{ch:COLI}


%\author{Ákos Kádár \thanks{Tilburg Center for Cognition and Communication,
%Tilburg University, 5000 LE Tilburg, The Netherlands,
%E-mail: \texttt{\{a.kadar, g.chrupala, a.alishahi\}@uvt.nl}} }
%\affil{Tilburg University}

%\author{Grzegorz Chrupała \footnotemark[1]}
%\affil{Tilburg University}

%\author{Afra Alishahi \footnotemark[1]}
%\affil{Tilburg University}

%\runningauthor{Kádár, Chrupała and Alishahi}

%\historydates{Submission received: 21st July, 2016 \\
%Revised version received: 5th June, 2017 \\
%Accepted for publication: 7th July, 2017}

%\begin{document}
%\maketitle

\paragraph{abstract}
  We present novel methods for analyzing the activation patterns of
  RNNs from a linguistic point of view and explore the types of
  linguistic structure they learn. As a case study, we use a standard
  standalone language model, and a multi-task gated
  recurrent network architecture consisting of two parallel pathways with
  shared word embeddings; The {\sc Visual} pathway is trained on predicting the
  representations of the visual scene corresponding to an input
  sentence, whereas the {\sc Textual} pathway is trained to predict the
  next word in the same sentence.
  We propose a method for estimating the amount of contribution
  of individual tokens in the input to the final prediction of the networks.
  Using this method, we show that the {\sc Visual} pathway pays
  selective attention to lexical categories and grammatical functions
  that carry semantic information, and learns to treat word types
  differently depending on their grammatical function and their position
  in the sequential structure of the sentence. In contrast, the language
  models are comparatively more sensitive to words with a syntactic
  function. Further analysis of the most informative n-gram contexts for
  each model shows that in comparison to the {\sc Visual} pathway,
  the language models react more strongly to abstract contexts
  that represent syntactic constructions.

\newpage

\paragraph{This chapter is based on} Kádár, Á., Chrupała, G., \& Alishahi, A. (2017).
Representation of linguistic form and function in recurrent neural networks. \textit{Computational Linguistics}, 
43(4), 761-780.

\newpage

\input{chapters/COLI/intro}
\input{chapters/COLI/related}
\input{chapters/COLI/models}
\input{chapters/COLI/experiments}
\input{chapters/COLI/conclusions}

%\bibliographystyle{compling}
%\starttwocolumn http://cljournal.org/commonprobs.html says we need
%this, but the template doesn't do it
%\bibliography{refs}

%\end{document}
