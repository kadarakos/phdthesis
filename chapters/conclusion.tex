%!TEX root = ../dissertation.tex
\chapter{Conclusion and discussion}
\label{ch:conclusion}

Traditional techniques to learn representations of words and sentences only consider 
linguistic context as a source of information and focus on the representation of a single language.
The aim of this thesis was to study make advances towards learning 
and understanding \emph{visually grounded} and \emph{multilingual} representations. 
This final chapter is dedicated not only to briefly summarize our main findings, but also to 
point to some of the limitations and implications of our work.


\section{Visually grounded word representations}
The starting point of the thesis is visual grounding on the word level for a single language.
Chapter~\ref{ch:TAL} is dedicated to learning word-representations based on the co-occurrences between 
words and high-level image features. We implemented a model taking inspiration from
the cross-situational account to word acquisition; a framework of lexical developement
based on the process of children repeatedly being exposed to pairs of linguistic utterrances and 
perceptual stimuli.  Throughout this process they learn a mapping between linguistic 
units and their referents updating their hypothesis based on a stream of co-occurrence statistics
between modalities as evidence. There has been a large number of experimental studies showing
that children use cross-situational evidence for word learning to learn this mapping 
fast \citep{smith.yu.08} and under a high level of \emph{referential uncertainty}
\citep{vouloumanos.werker.09}. 

We depart in two major ways from the canonical computational cross-situational models 
 \citep{siskind.96,fontanari2009cross,fazly.etal.10,kachergis2012associative,matusevych2013automatic,yu2013grounded}: 
1.) we use large image--description benchmark data sets developed for machine learning purposes, 
which naturally presents a high level of referential uncetainty and ambiguity
2.) take continous image representations rather than artificial symbolic scene descriptors.
More concretely approach adapts the cross-situational word learning model of \citep{fazly.etal.10},
which is an online version of the IBM Model 1. word-translation model \citep{BrownPPM94} to take
continuous image feautres to represent scenes coming from pre-trained convolutional neural networks.
Even more unorthodox in the cross-stuational literature our model does not learn a mapping between 
words and objects 
\citep{fazly.etal.10,lazaridou2016multimodal}, but the strength of the relationship between words and high-level
global image feature descriptors. As such our main experimental protocol is related departs from
the standard word-object retrieval evaluation performed on the CHILDES data set \citep{goodman2008bayesian,kievit2013naturalistic,lazaridou2016multimodal} and is more similar
to performing image--tagging \citep{weston2010large}. Through these retrieval experiments we find that
our model tags images with relevant concepts.

In another experiment we measure the
correlation between word similarities under our model and according to human participants and comapre
the results to state-of-the-art distributional model trained on the same sentences.
Word-similarity benchmark experiments originate from \citep{rubenstein1965contextual} who 
intended to provide empirical evidence for the distributional hypothesis of  \citep{harris1954distributional}.
Later they became standard in comparing various approaches of computational using such 
similarity judgements \citep{faruqui2014community}.\footnote{That said many problems with the validity and
consistency of this methodology have been raised around that time \citep{faruqui2016problems}.}
Using this protocol allows us to directly compare a cross-situational and visually grounded computational
model of child word learning to the state of the art distributional word2vec approach of the time. 
The models perform on the same level, however, more interestingly by creating
\emph{abstract} and  \emph{concrete} portions of word similarity benchmarks we find that
the distributional word2vec is better on the \emph{abstract}, while the cross-situational model on the 
\emph{concrete} set.   

The first main result of Chapter~\ref{ch:TAL} is that our cross-situational word learning model learns
meaningful word representations as demonstrated by our image--word matching experiments. 
Secondy, we train a distributional model and our visually grounded model on the same same sentences;
the former learning word representations from linguistic the second from visual contexts. 
They perform on a similar level in terms of approximating human similarity judgements, however,
the visual context is more informative in the case of concrete words, whereas distributional cues 
are more helpful for learning the meaning of abstract words. Our result highlights the complementarity
between these different sources of learning signals.

Note, however, that even though our algorithm introduced in Chapter~\ref{ch:TAL} is incremental
and is based on well established models of word-learning \citep{fazly.etal.10} it is not intended as a
complete account of child lexical developement e.g.: consider 
the lack of integration of social cues or parent directed attention, both of which 
have been shown to boost learning in children 
\citep{gleitman1990structural,tomasello1995two} and in computational models of cross situational
models of word learning \citep{yu2007unified,lazazaridou2016multimodal}.

\section{Visually grounded sentence representations}
Moving from representations of words to full sentences in Chapter~\ref{ch:COLI} we contineou 
our comparative study between distributional and visually grounded representations. 
Concretely, in Chapter~\ref{ch:COLI} we analyze the patterns of hidden activations of 
our recurrent visually grounded sentence representations learning model 
\texttt{IMAGINET} \citep{chrupala2015learning}. This architecture consists of two separate gated
recurrent unit networks coupled through a shared word embedding matrix. The \texttt{VISUAL}
pathway performs visually grounded learning through the image--sentence ranking objective, 
while \texttt{TEXTUAL} is trained as a language model to maximize the likelihood of the following word
given the history.

In  \citep{chrupala2015learning} we compare the word representations learned by only training one
of the subnetworks as compared to training them jointly and find that the word-similarity judgements 
of \texttt{VISUAL} correlate better with human similarity judgements than \texttt{TEXTUAL} and that 
the joint \texttt{MultiTask} model is superior to both. On the task of image--sentence ranking 
\texttt{VISUAL} outperforms our bag-of-words-baseline, but bya large margin. This led us test
whether the \texttt{VISUAL} trained on caption with randomly permutated word order can perform 
on equal evel to it trained on the original captions. We find that the model trained on the original
captions does perform better suggesting that it successfully exploit word order information.

Chapter~\ref{ch:COLI} is dedicated to a more in depth investigation
of the kind of linguistic structure that is encoded in the hidden activations of the learned representations. 
Crucially, we perform a controlled comparison between the grounded \texttt{VISUAL} and the distributional 
 \texttt{TEXTUAL} pathway: both models have the same architecture and share word-embeddings.
 As the linguistic interpretation of representations learned by recurrent networks did not enjoy a 
 large amount of  attention at the time we introduced two novel techniques. 
 
 Firstly, we introduced the \emph{omission score}, an input perturbation based saliency metric assigning a 
 real-valued improtance score to each input token signaling how much impact did it have on 
 the final sentence representation. Through computing the 
 log ratios of the omission score distribution over part-of-speech and dependency labels  between
 \texttt{VISUAL} and \texttt{TEXTUAL} we find that 	the former payes more attention to categories
 usually filled with semantically contentful words, while the attention distribution of \texttt{TEXTUAL}
 is fairly uniform.
 
 To disintangle word forms from their functions we fit ridge regression models 
 to predict omission scores for tokens using the word
 identity as predictor and adding dependency relation ( \texttt{+dep}) or
 sentence position variables or both and their interactions. 
 We find that the word-identity is much less predictive of omission score for \texttt{TEXTUAL}
 than it is for  \texttt{VISUAL} and that position is most informative for \texttt{TEXTUAL},
 while for \texttt{VISUAL} dependency relation and position variables provide similar increases
 in $R^2$. In a more fine-grained analysis we examined the omission distributions
 computed for the \texttt{VISUAL} pathway of words for which the increase in 
 $R^2$ from the word-identity regression model to the \texttt{+dep} model is the highest.
 We find a general pattern that words generally produce the highest omission scores
 when they fill the noun subject or root function in a sentence and smallest when they appear 
 as conjuncts.

Lastly we turn to contrasting what the individual dimensions of the two pathways encode.
To do so we compute the mutual information between the binned activation values and different
type of contexts. For these contexts we consider word and dependency relation n-grams up to
order three. This results in distributions of mutual information values between activations and
contexts. We take the median of these distributions to compare the types of contexts that
are more related to the activation patters in  \texttt{VISUAL} than  \texttt{TEXTUAL}. We find
the significant difference between the pathways that features encoded by  \texttt{TEXTUAL} are
more related to dependency labels while that of \texttt{VISUAL} are more related to words.
We can also use these mutual informatiton scores to find for each dimension of the hidden states
of the recurrent networks, which contexts they are most related to. 
On visual inspection we find that  for both 
\texttt{VISUAL} and \texttt{TEXTUAL} that these contexts
tend to be a combination of semantic/syntactic constructions. Consider for example two of these
top contexts for one of the hidden units of  \texttt{TEXTUAL}: \emph{male on a}, \emph{person rides a}.
For \texttt{VISUAL} we find contexts to be more topically than syntactically related, however.

Through the developement and application of the omission score and 
mutual information based interpretation techniques we have found similarities and differences between
the sentence representations learned by a image--sentence ranking model 
and a language model. 

Since our work, however, there have been a large number input token saliency 
measures introduced as discussed in Section~\ref{sec:interpret}. \cite{shrikumar2017learning}
points out that perturbation based techniques struggle due to the limited perturbation window i.e.: the
important context might be larger than the scope of the perturbation function. 
On the explanation evaluation benchmark experiments of \cite{poerner2018evaluating} 
perturbation based methods (including ours) lead to inconsistent results and backpropagation style
methods \citep{bach2015pixel,shrikumar2017learning} have a stronger performance in general.

To find typical inputs to hidden units \cite{poerner2018interpretable} develops an alternative
to our mutual information based approach and applies it to our \texttt{IMAGINET} architecture.
They perform gradient ascent on maximizing the activation values for particular dimensions.
Their results reproduce our findings that the hidden units of \texttt{TEXTUAL} tend to be more 
syntax aware than that of \texttt{VISUAL}.

Interpretation of deep models of language have become a subfield of NLP in its' own right
with dedicated venues such as the BlackboxNLP Workshop at EMNLP 2018
\citep{alishahi2019analyzing}. Future work in the field linguistic representation learning
can benefit from these analysis methods by shedding more light onto the differences between
learning from different cues such as the linguistic versus perceptual contexts we considered in 
this thesis.



\section{Improving translation with visual grounding}

Multimodal machine translation has been introduced as a shared task \citep{specia2016shared} in the
First Conference of Machine Translation (WMT16) initially with German and English  \citep{elliott2016multi30k}
later with added French and Czech data \citep{elliott2017findings}. The task is 
machine translation with added side information in the form of images. 
Through a review of the state-of-the-art we observe in Chapter~\ref{ch:IJCNLP} 
that the multi-modal and text-only versions of 
the top performing systems during the first competition
have very similar performances \citep{specia2016observed}. 
Furthermore, when systems do perform better it is unclear if they successfully use images as 
context to aid translation or they learn better representations though visual grounding.

In Chapter~\ref{ch:IJCNLP} rather than conditioning translation on the additional image context we 
introduce an architecture to learn visually grounded representations jointly with translation.
More precisely we train a shared
encoder which provides input representation to an image--sentence ranking and a
attentional translation decoder. The two sub-models are trained jointly with multi-task learning, much
like \texttt{IMAGINET} in \cite{chrupala2015learning}.

We considered two setups:
1.) \emph{aligned} setting where the input is triplets composed of an English sentence, 
it's German translation and a corresponding image, 2.) \emph{disjoint} where we have a
separate English-German parallell corpus and an image--sentence corpus in English.
We find no significant difference in performance between the \emph{aligned} and \emph{disjoint}
settings. This result provides evidence that visual grounding can provide a useful inductive bias to 
improve translation quality.

We also performed a follow up experiment where we improved our text-only baseline with 
training on an additional English-German parallell corpus. We observed that even when
extra translation data is available visual grounding still provided improvements in performance.
Finally, our full system with the extra parallell corpus and an additional image--sentence data set achieved
2nd place according the human judgements in the WMT Shared Task on Multimodal Translation 
and Multilingual Image Description \citep{elliott2017findings} 
on the unconstrained task out of 16 submissions. 

On this Shared Task for some
systems the multi-modal variant achieves better performance
\citep{caglayan2017lium}, while the text-only is better for others \citep{ma2017osu}. 
The results of the thrid shared task are in line with the previous years:
adding images as extra context to translation systems result in marginal 
differences in translation quality \citep{barrault2018findings}.
\cite{elliott2018adversarial} investigate the issue further and introduces
a measure of the image awereness of multi-modal translation models and conclude that the current 
version of the Multi30K data set probably does not contain many training samples where the models 
need to take the visual modality into account for translation. The role of visual context in translation
remains an active area of research at the time of writing this thesis: the "Best Short Paper" title
at the North American Chapter of Association for Computational Linguistics 2019 was awarded to
\citep{caglayan2019probing} who show that multi-modal systems are insensitive to images in general
, however, the visual contexts can help models recover from corruptions to the source sentences.


\section{Multilingual visually grounded sentence representations}

We show in Chapter~\ref{ch:IJCNNLP} that visually grounded sentence representation learning
provides a useful inductive bias for machine translation. \cite{kiros2018illustrative} also report 
improved translation performance on the English--German and English--French Multi30K 
and IWSLT 2014 German-English  \citep{cettolo2014report} translation tasks, when initializing
the translation models with their grounded Picturebook word-embeddings. They also show that
combining pre-trained GloVe \citep{pennington2014glove} and Picturebook embeddings lead 
to improvements on natural language inference on the SNLI 
\citep{bowman2015large} and MultiNLI \cite{williams2017broad} data sets and on predicting
semantic relatedness on the SICK data set \citep{marelli2014semeval}. 
\citep{kiela2017learning} through transfer experiments on
entailement and sentence classification tasks find that the representations learned an 
image--sentence ranking model train on COCO -- similar to the ones trained in Chapters~\ref{ch:IJCNLP}, 
\ref{ch:ConLL}, \ref{ch: EMNLP}  -- outperform that of their SkipThought \citep{kiros2015skip} 
implementation. They conclude that visual grounding leads to qualitatively different representations, which
can be beneficial many tasks.

Given the promising results on the benefit of visual grounding on sentence level tasks it seems
to be an important avenue to explore techniques to learn better quality and more 
general multi-modal representations. To this end in Chapters~\ref{ch:ConLL} and ~\ref{ch:EMNLP} 
we turned to learning grounded representations for multipe languages jointly. 
The benefit of multilingual joint learning have been demonstrated for example in dependency parsing
 \citep{ammar2016many}, machine translation \citep{johnson2016google} and grapheme-to-phoneme
 conversion \citep{peters2017massively}. 
 In the visually grounded representation learning \citep{gella2017image} showed improved 
 performance in some cases for image--sentence ranking and semantic textual similarity when training
 on two languages jointly.
 
  In Chapter~\ref{ch:ConLL} we improve and expand the results of 
 \cite{gella2017image}. Firstly, contrary to their results we report consistent performance gains in the
 bilingual setting in our setup: bilingual joint training improves for on both English and German image--sentence
 ranking experiments and the cross--lingual caption--caption ranking objective provides further benefits
 in both cases. Furthermore, we show that the results of the bilingual model can be improved by training
 with two additional langauges: French and Czech. We 





%Given this results, apart from exploring visually grounded learning as an inductive bias for translation,
%we intended to provide empirical evidence that in the way the challenge was setup the images are not
%necessarily useful contexts. In fact, our model has achieved state-of-the-art level performance without 
%conditioning translation on image input.


%In fact, we observe consistent benefits of visually grounded 
l%earning for translation in the Multi30K challenge. 

\section{Future directions}


\subsection{Raw input}
I've focused on words, but char level processing is great. Works on speech too.

\subsection{Trasnferring grounded representations}
Does groudned learning improve anything really?



