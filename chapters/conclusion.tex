%!TEX root = ../dissertation.Tex
\chapter{Conclusion and discussion}
\label{ch:conclusion}

Traditional techniques to learn representations of words and sentences only consider 
linguistic context as a source of information and focus on the representation of a single language.
The aim of this thesis was to study make advances towards learning 
and understanding \emph{visually grounded} and \emph{multilingual} representations. 
This final chapter is dedicated not only to briefly summarize our main findings, but also to 
point to some of the limitations and implications of our work.


\section{Visually grounded word representations}
The starting point of the thesis is visual grounding on the word level for a single language.
Chapter~\ref{ch:TAL} is dedicated to learning word-representations based on the co-occurrences between 
words and high-level global image features. We implemented a model taking inspiration from
the cross-situational account to word acquisition; a framework of lexical development
based on the process of children repeatedly being exposed to pairs of linguistic utterances and 
perceptual stimuli.  Throughout this process they learn a mapping between linguistic 
units and their referents updating their hypothesis based on a stream of co-occurrence statistics
between modalities as evidence. 

%There has been a large number of experimental studies showing
%that children use cross-situational evidence for word learning to learn this mapping 
%fast \citep{smith.yu.08} and under a high level of \emph{referential uncertainty}
%\citep{vouloumanos.werker.09}. 

We departed in two major ways from the canonical computational cross-situational models 
 \citep{siskind.96,fontanari2009cross,fazly.etal.10,kachergis2012associative,matusevych2013automatic,yu2013grounded}: 
1.) we use large image--description benchmark data sets developed for machine learning purposes, 
which naturally present a high level of referential uncertainty and ambiguity
2.) take continuous image representations rather than artificial symbolic scene descriptors.

We adapted the cross-situational word learning model of \citep{fazly.etal.10}, an 
online version of the IBM Model 1. word-translation model \citep{BrownPPM94}, to take as input scene
representations convolutinal neural network activations.
Even more unorthodox in the cross-situational literature our model does not learn a mapping between 
words and objects 
\citep{fazly.etal.10,lazaridou2016multimodal}, but the strength of the relationship between words and high-level
global image feature descriptors. As such our main experimental protocol is related, but departs from
the standard word-object retrieval evaluation performed on the CHILDES data set \citep{goodman2008bayesian,kievit2013naturalistic,lazaridou2016multimodal} and is more similar
to performing image--tagging \citep{weston2010large}. Through these retrieval experiments we find that
our model tags images with relevant concepts and as such learns to ground the meaning of words in visual
scenes.

In another experiment we measure the
correlation between word similarities under our model and according to human participants and compare
the results to state-of-the-art distributional model trained on the same sentences.
%Word-similarity benchmark experiments originate from \citep{rubenstein1965contextual} who 
%intended to provide empirical evidence for the distributional hypothesis of  \citep{harris1954distributional}.
Word-similarity benchmarks were the standard method at the time to comparing various approaches of computational using such similarity judgments \citep{faruqui2014community},
we do note that concerns about the validity and consistency of this methodology has been 
raised \citep{faruqui2016problems}.

Using this protocol allowed us to directly compare a cross-situational and visually grounded computational
model of child word learning to the state of the art distributional word2vec approach of the time. 
The models perform on the same level, however, more interestingly by creating
\emph{abstract} and  \emph{concrete} portions of word similarity benchmarks we find that
the distributional word2vec is better on the \emph{abstract}, while the cross-situational model on the 
\emph{concrete} set. This results highlights the complementarity
between these different sources of learning signals.

%The first main result of Chapter~\ref{ch:TAL} is that our cross-situational word learning model learns
%meaningful word representations as demonstrated by our image--word matching experiments. 
%Secondy, we train a distributional model and our visually grounded model on the same same sentences;
%the former learning word representations from linguistic the second from visual contexts. 
%They perform on a similar level in terms of approximating human similarity judgements, however,
%the visual context is more informative in the case of concrete words, whereas distributional cues 
%are more helpful for learning the meaning of abstract words. 

Even though our algorithm introduced in Chapter~\ref{ch:TAL} is incremental
and is based on a well established model of word-learning \citep{fazly.etal.10} it is not intended as a
complete account of child lexical development. Consider 
the lack of integration of social cues or parent directed attention, both of which 
have been shown to boost learning in children 
\citep{gleitman1990structural,tomasello1995two} and in computational cross situational
models of word learning \citep{yu2007unified,lazazaridou2016multimodal}.

\section{Visually grounded sentence representations}
Moving from representations of words to full sentences in Chapter~\ref{ch:COLI} we continue
our comparative study between distributional and visually grounded representations. 
Concretely, in Chapter~\ref{ch:COLI} we analyze the patterns of hidden activations in 
our recurrent visually grounded sentence language learning model 
\texttt{IMAGINET} \citep{chrupala2015learning}. 

\texttt{IMAGINET} consists of two separate recurrent networks coupled through a 
shared word embedding matrix. The \texttt{VISUAL}
pathway performs visually grounded learning through the image--sentence ranking objective, 
while \texttt{TEXTUAL} is trained as a language model to maximize the likelihood of the following word
given the history.

%In  \citep{chrupala2015learning} we compare the word representations learned by only training one
%of the subnetworks as compared to training them jointly and find that the word-similarity judgements 
%of \texttt{VISUAL} correlate better with human similarity judgements than \texttt{TEXTUAL} and that 
%the joint \texttt{MultiTask} model is superior to both. On the task of image--sentence ranking 
%\texttt{VISUAL} outperforms our bag-of-words-baseline, but bya large margin. This led us test
%whether the \texttt{VISUAL} trained on caption with randomly permutated word order can perform 
%on equal evel to it trained on the original captions. We find that the model trained on the original
%captions does perform better suggesting that it successfully exploit word order information.

Chapter~\ref{ch:COLI} is dedicated to an in depth investigation
of the kind of linguistic structure that is encoded in the hidden activations of the learned representations. 
Crucially, we perform a controlled comparison between the grounded \texttt{VISUAL} and the distributional 
 \texttt{TEXTUAL} pathway: both models have the same architecture and share word-embeddings.
 As the linguistic interpretation of representations learned by recurrent networks did not enjoy a 
 large amount of  attention at the time we introduced two novel techniques. 
 
 Firstly, we introduced the \emph{omission score}, an input perturbation based saliency metric assigning a 
 real-valued importance score to each input token signaling how much impact  they have on 
 the final sentence representation. Through computing the 
 log ratios of the omission score distribution over part-of-speech tags and dependency labels between
 \texttt{VISUAL} and \texttt{TEXTUAL} we find that 	the representations of the former
 is more impacted by words belonging to categories
 usually filled with semantically content, 
 while for \texttt{TEXTUAL} the distribution is fairly uniform.
 
 To disentangle the impact of word forms and their functions on the representations 
 we fit ridge regression models 
 to predict omission scores for tokens using the word
 identity as predictor or adding dependency relation ( \texttt{+dep}), 
 sentence position variables or both and their interactions. 
 We find that the word-identity is much less predictive of omission score for \texttt{TEXTUAL}
 than it is for  \texttt{VISUAL} and that position is most informative for \texttt{TEXTUAL},
 while for \texttt{VISUAL} dependency relation and position variables provide similar increases
 in $R^2$. 
 
 In a more fine-grained analysis we examined the omission distributions
 computed for the \texttt{VISUAL} pathway of words for which the increase in 
 $R^2$ from the word-identity regression model to the \texttt{+dep} model is the highest.
 We find a general pattern that words generally produce the highest omission scores
 when they fill the noun subject or root function in a sentence and smallest when they appear 
 as conjuncts.

Lastly we turn to contrasting what the individual dimensions of the two pathways encode.
To do so we compute the mutual information between the binned activation values and different
type of contexts. For these contexts we consider word and dependency relation n-grams up to
order three. This results in distributions of mutual information values between activations and
contexts. We take the median of these distributions to compare the types of contexts that
are more related to the activation patters in  \texttt{VISUAL} than  \texttt{TEXTUAL}. We find
the significant difference between the pathways that features encoded by  \texttt{TEXTUAL} are
more related to dependency labels while that of \texttt{VISUAL} are more related to words.

We can also use these mutual information scores to find for each dimension of the hidden states
of the recurrent networks, which contexts they are most related to. 
On visual inspection we find that  for both 
\texttt{VISUAL} and \texttt{TEXTUAL} these contexts
tend to be a combination of semantic/syntactic constructions. Consider for example two of these
top contexts for one of the hidden units of  \texttt{TEXTUAL}: \emph{male on a}, \emph{person rides a}.
In general for \texttt{VISUAL} we find contexts to be more topically than syntactically related.

Through the development and application of the omission score and 
mutual information based interpretation techniques we have found similarities and differences between
the sentence representations learned by a image--sentence ranking model 
and a language model. 

Since our article was published there have been a large number input token saliency 
measures introduced in the liter tire  as discussed in Section~\ref{sec:interpret}. 
\cite{shrikumar2017learning}
points out that perturbation based techniques like our omission score tend to struggle
 due to the limited perturbation window i.e.: the
important context might be larger than the scope of the perturbation function. 
On the explanation evaluation benchmark experiments of \cite{poerner2018evaluating} 
perturbation based methods (including ours) lead to inconsistent results and backpropagation style
methods \citep{bach2015pixel,shrikumar2017learning} have a stronger performance in general.

To find typical inputs to hidden units \cite{poerner2018interpretable} develops an alternative
to our mutual information based approach and applies it to our \texttt{IMAGINET} architecture.
They perform gradient ascent on maximizing the activation values for particular dimensions.
Even though their technique is quite different, their results reproduce our findings that the hidden 
units of \texttt{TEXTUAL} tend to be more syntax aware than that of \texttt{VISUAL}.

Interpretation of deep models of language have become a subfield of NLP in its' own right
with dedicated venues such as the first BlackboxNLP Workshop at EMNLP 2018
\citep{alishahi2019analyzing}. Future work in the field linguistic representation learning
can benefit from these analysis methods by shedding more light onto the differences between
learning from different cues such as the linguistic versus perceptual contexts we considered in 
this thesis.



\section{Improving translation with visual grounding}

Multi modal machine translation has been introduced as a shared task in the
First Conference of Machine Translation (WMT16) \citep{specia2016shared} initially with
German and English  \citep{elliott2016multi30k} data and
later with added French and Czech data \citep{elliott2017findings}. The task is 
machine translation with added side information in the form of images. 
Through a review of the state-of-the-art we observe in Chapter~\ref{ch:IJCNLP} 
that the multi-modal and text-only versions of 
the top performing systems during the first competition
have very similar performances \citep{specia2016observed}. 
Furthermore, when systems do perform better it is unclear if they successfully use images as 
context to aid translation or they learn better representations though visual grounding.

In Chapter~\ref{ch:IJCNLP} rather than conditioning translation on the additional image context we 
introduce an architecture to learn visually grounded representations jointly with translation.
More precisely we train a shared
encoder which provides input representation to an image--sentence ranking and an
attentional translation decoder. The two sub-models are trained jointly with multi-task learning, much
like \texttt{IMAGINET} in \cite{chrupala2015learning}.

We considered two setups:
1.) \emph{aligned} setting where the input is made up of triplets of an English sentence, 
it's German translation and a corresponding image, 2.) \emph{disjoint} where we have a
separate English-German parallel corpus and an image--sentence corpus in English.
We find that the multi-task model significantly outperforms the text-only baseline with
no significant difference in performance between the \emph{aligned} and \emph{disjoint}
settings. This result provides evidence that visual grounding can provide a useful inductive bias to 
improve translation quality. 
We also performed a follow up experiment where we set a stronger baseline by 
improving our text-only model performance with 
training on an additional English-German parallel corpus. We observed that even when
extra translation data was available visual grounding still provided improvements in performance.
Finally, our full system with the extra parallel corpus and an additional image--sentence data set achieved
2nd place according the human judgments in the WMT Shared Task on Multimodal Translation 
and Multilingual Image Description \citep{elliott2017findings} 
on the unconstrained task out of 16 submissions. 

On this Shared Task for some
systems the multi-modal variant achieved better performance
\citep{caglayan2017lium}, while the text-only was better for others \citep{ma2017osu}. 
The results of the third shared task are in line with that of the previous years:
adding images as extra context to translation systems resulted in marginal 
differences in translation quality \citep{barrault2018findings}.

\cite{elliott2018adversarial} investigate the issue further and introduces
a measure of the image awareness of multi-modal translation models and conclude that the current 
version of the Multi30K data set probably does not contain many training samples where the models 
need to take the visual modality into account for translation. The role of visual context in translation
remains an active area of research at the time of writing this thesis: the "Best Short Paper" title
at the North American Chapter of Association for Computational Linguistics 2019 was awarded to
\citep{caglayan2019probing} who show that multi-modal systems are insensitive to images in general
, however, the visual contexts can help models recover from corruptions to the source sentences.



\section{Multilingual visually grounded sentence representations}

We show in Chapter~\ref{ch:IJCNNLP} that visually grounded sentence representation learning
can provide a useful inductive bias for machine translation. \cite{kiros2018illustrative} also report 
improved translation performance on the English--German and English--French Multi30K 
and IWSLT 2014 German-English  \citep{cettolo2014report} translation tasks, when initializing
the translation models with their visually grounded Picturebook word-embeddings. 

They also show that
combining pre-trained GloVe \citep{pennington2014glove} and Picturebook embeddings lead 
to improvements on natural language inference on the SNLI 
\citep{bowman2015large} and MultiNLI \cite{williams2017broad} data sets and on predicting
semantic relatedness on the SICK data set \citep{marelli2014semeval}. 
\citep{kiela2017learning} through transfer experiments on
entailment and sentence classification tasks find that the representations learned by an 
image--sentence ranking model train on COCO -- similar to the ones trained in Chapters~\ref{ch:IJCNLP}, 
\ref{ch:ConLL}, \ref{ch: EMNLP}  -- outperform that of their SkipThought \citep{kiros2015skip} 
re-implementation. They conclude that visual grounding leads to qualitatively different representations, which
can be beneficial many tasks.

Given the promising results on the benefit of visual grounding on sentence level tasks it seems
to be an important avenue to explore techniques to learn better quality and more 
general multi-modal representations. To this end in Chapters~\ref{ch:ConLL} and ~\ref{ch:EMNLP} 
we turned to learning grounded representations for multipe languages jointly. 
The benefit of multilingual joint learning have been demonstrated for example in dependency parsing
 \citep{ammar2016many}, machine translation \citep{johnson2016google} and grapheme-to-phoneme
 conversion \citep{peters2017massively}. 
 In the visually grounded representation learning \citep{gella2017image} showed improved 
 performance in some cases for image--sentence ranking and semantic textual similarity when training
 on two languages jointly.
 
  In Chapter~\ref{ch:ConLL} we improve and expand the results of 
 \cite{gella2017image}. Firstly, contrary to their results we report consistent performance gains in the
 bilingual setting in our setup: bilingual joint training improves for both English and German 
 image--sentence ranking experiments and the cross--lingual caption--caption ranking 
 objective provides further benefits in both cases. 
 Furthermore, we show that the performance bilingual model can be improved by training
 with two additional languages: French and Czech. We further show that the image--sentence ranking
 results on the lower resource French and Czech data sets can be improved by jointly training with the
 larger English and German data sets. 
Our experiments suggest the clear benefits of multilingual visually grounded representations over 
their mono- and bilingual counterparts.
 
These results are all based on a setting where the same images are annotated with captions 
 of different languages. To test whether this \emph{alignment} is crucial or performance, we generate
 a synthetic \emph{disjoint} set using the data set that we used for the alignment experiments. We find
 that the models perform equally well in both setups.

Chapter~\ref{ch:EMNLP} is dedicated to explore the \emph{disjoint} setting under more realistic 
circumstances, where we have two data sets that were collected separately. 
Specifically we use the English-German Multi30K and the English COCO for our experiments.
Training on COCO resulted in a low test performance on the English Multi30K leading us to  conclude that 
though the two data sets appear fairly similar there is a considerable shift in domains. When training
the \emph{disjoint} model on the German Multi30K and English COCO we find that the test performance
is lower on the German Multi30K test set compared to training the \emph{aligned model} i.e.: training
on the English-German Multi30K. This difference might be partially due to domain-shift. Alternatively,
the lack of \emph{alignment} makes it challenging to find correspondences between languages that
is crucial to exploit complementary information between them.

The motivation for our \emph{pseudopairs} method in Chapter~\ref{ch:EMNLP} 
was to improve the models' capacity to learn from \emph{disjoint} data sets by 
creating a \emph{synthetic alignment}. In the fully \emph{disjoint} experiment our method
starts by training a bilingual model on the German Multi30K and the English COCO. Then we 
use the sentence similarities under this \emph{parent model} to generate a synthetic 
English-German COCO: For each English sentence in COCO find the most similar sentence in Multi30k
and use it as the German description for that image. We add an extra experiment we call \emph{aligned
plus disjoint} setting where both the bilingual Multi30k and the additional larger English COCO is available.
In both cases we find improvements when we use the pseudopair method.

We demonstrated in Chapter~\ref{ch:ConLL} that using our setup we can learn better 
visually grounded representations when training on multiple languages. Our results suggest that the
more languages the larger the gain, however, we only considered the same images annotated with 
four languages. Future work can shed more light on under what circumstances this observation holds
by running experiments with different resources such as the Chinese Flickr8k \citep{li2016adding}
or the Japanese COCO \citep{miyazaki2016cross}. 

Even though the pseudopair method presented in Chapter~\ref{ch:EMNLP} provides consistent improvements
we find that using an off-the-shelf model to translate the captions to the desired language to create 
synthetic pairs leads to larger improvements. In future work we will focus on narrowing this gap by applying
better matching algorithms to generate pseudopairs that mitigate the hubness problem \cite{radovanovic2010existence,tomavsev2011influence,tomavsev2011probabilistic,dinu2014improving}. 

 

\section{Future directions and limitations}

\subsection{Embedding geometry}

The visually grounded sentence representation learning approaches 
we presented in this thesis learn to represent sentences as points
in an embedding space through minimizing symmetric distance functions. 
An alternative we have not explored is the asymmetric order-embeddings 
\citep{vendrov2015order} which has shown to outperform cosine similarity based 
image--sentence ranking models \citep{faghri2017vse++}. 
Richer semantic representations could be also explored in the future such as Gaussian 
distributions over a latent embedding 
space \citep{vilnis2014word}, which allows for the modeling of asymmetric relationship between sentences
such as entailment, specificity/inclusion and uncertainty. 
Another less well known approach in the literature is to embed objects in a
hyperbolic space, which naturally represents hierarchical relationships and can be thought of as a
continuous version of trees \citep{krioukov2010hyperbolic}.  In a similar setup to ours 
\cite{tay2018hyperbolic} 
trains a simple hyperbolic sentence embedding architecture for question-answer 
ranking using a pairwise ranking loss without any attention/interaction layer, 
which performs at state-of-the-art level. 
Future work in visually grounded and multilingual representation learning can benefit from exploring
spaces of various topology for the embedding of sentences. 



\subsection{Multi-task learning}

Progress towards learning better visually grounded and multilingual representations
can be accelerated by exploring different multi-task learning strategies. 
In chapters \ref{ch:COLI}, \ref{ch:IJCNLP}, \ref{ch:ConLL} and \ref{ch:EMNLP}
we used hard parameter sharing \citep{caruana1997multitask,collobert2011natural}
and share the whole encoder across all languages. This strategy is expected to work when there is a close
relationship between tasks \citep{baxter2000model} -- or languages in our case  -- and worse results 
are expected as the distance between tasks grow \citep{maurer2006bounds}. 
The other most common approach to parameter sharing in neural networks is soft parameter sharing:
each task has its
own set of parameters, but a regularization penalty is added forcing these parameter sets similar \citep{duong2015low,yang2016trace}.

Furthermore, in chapters \ref{ch:ConLL} and \ref{ch:EMNLP} we follow the common practice of
pre-defining uniform task sampling probabilities 
before training time and keep them fixed during training \citep{alonso2016multitas,bingel2017identifying}.
However, \cite{kiperwasser2018scheduled} observe benefits from applying fixed schedules to 
the task sampling probabilities, while \cite{sanh2018hierarchical} shows the benefits of adjusting the
sampling probabilities to the size of the training sets of different tasks.  Recently \cite{ruder2017sluice}
presented a method to jointly learn the task-weighting or sampling probabilities and which parameters
to share.

\subsection{Local image descriptors}

The approaches presented in the thesis extracted global image and sentence features through separate
encoders and learned to associate them. Another line of work focuses on learning latent alignments
between sentence fragments -- usually words -- and image regions. The architecture presented in 
\citep{karpathy2015deep} first encodes the sentence with a bidirectional recurrent network and extracts
image region features from a pre-trained convolutional neural network. The dot product between 
the hidden states of the recurrent network and local region features are interpreted as the similarity
between regions and words. This formulation offered a blue-print for later models that define various
more sophisticated attention mechanisms to compute region--word interactions in a multi-step fashion
 \citep{nam2017dual,huang2017instance}.


%\paragraph{Input representation}
%We mainly focused on learning sentence representations from word-level input. 
%Character-level image--sentence ranking \citep{wehrmann2018order}.

%\subsection{kjh}





%Given this results, apart from exploring visually grounded learning as an inductive bias for translation,
%we intended to provide empirical evidence that in the way the challenge was setup the images are not
%necessarily useful contexts. In fact, our odel has achieved state-of-the-art level performance without 
%conditioning translation on image input.


%In fact, we observe consistent benefits of visually grounded 
l%earning for translation in the Multi30K challenge. 




\subsection{Trasnferring grounded representations}
Does groudned learning improve anything really?



