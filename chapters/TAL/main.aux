\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Learning word meanings from images of natural scenes}{17}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {paragraph}{Abstract}{17}{chapter.2}}
\@writefile{toc}{\contentsline {paragraph}{This chapter is based on}{19}{chapter.2}}
\citation{Quine1960,Carey1978,pinker.89}
\citation{yu.smith.07,smith.yu.08,vouloumanos.08,vouloumanos.werker.09}
\citation{Li.etal.2004,regier.05}
\citation{siskind.96,frank.etal.07,yu.08,fazly.etal.10csj}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Introduction}{20}{section.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Cross-situational learning}{20}{subsection.2.1.1}}
\citation{macwhinney2014childes}
\citation{siskind.96}
\citation{fellbaum1998wordnet}
\citation{fazly.etal.10csj}
\citation{matusevych2013automatic,beekhuizen2013word}
\citation{FleischmanRoy2005,skocaj2011system}
\citation{karpathy2014deep}
\citation{mao2014explain}
\citation{kiros2014unifying}
\citation{donahue2014long}
\citation{vinyals2014show}
\citation{venugopalan2014translating}
\citation{chen2014learning}
\citation{fang2014captions}
\citation{vinyals2014show}
\citation{szegedy2014going}
\citation{hochreiter1997long}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Learning meanings from images}{22}{subsection.2.1.2}}
\newlabel{subsec:meanings-from-images}{{2.1.2}{22}{Learning meanings from images}{subsection.2.1.2}{}}
\citation{bruni2014multimodal}
\citation{lazaridou2015combining}
\citation{deng2009imagenet}
\citation{rashtchian2010collecting}
\citation{young2014image}
\newlabel{rev:synset}{{2.1.2}{24}{Learning meanings from images}{subsection.2.1.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Our study}{24}{subsection.2.1.3}}
\newlabel{sec:out-study}{{2.1.3}{24}{Our study}{subsection.2.1.3}{}}
\newlabel{why-incremental}{{2.1.3}{24}{Our study}{subsection.2.1.3}{}}
\citation{yu2007unified,fazly.etal.10csj}
\citation{BrownPPM94}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Word learning model}{25}{section.2.2}}
\newlabel{sec:models}{{2.2}{25}{Word learning model}{section.2.2}{}}
\citation{simonyan2014very}
\citation{jia2014caffe}
\citation{deng2009imagenet}
\citation{fazly.etal.10csj}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Visual input}{26}{subsection.2.2.1}}
\newlabel{rev:cnndetail}{{2.2.1}{26}{Visual input}{subsection.2.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces \textit  {Dimensions with three most closely aligned images from F8k.}\relax }}{27}{figure.caption.4}}
\@cons\caption@pkg@list{{ragged2e}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:dims}{{2.1}{27}{\textit {Dimensions with three most closely aligned images from F8k.}\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Learning algorithm}{28}{subsection.2.2.2}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Sentence-vector alignment model (\textsc  {Visual})\relax }}{29}{algorithm.1}}
\newlabel{algo:ibm-vec}{{1}{29}{Sentence-vector alignment model (\textsc {Visual})\relax }{algorithm.1}{}}
\citation{mikolov2013efficient,mikolov2013distributed}
\citation{rashtchian2010collecting}
\citation{young2014image}
\citation{karpathy2014deep}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Baseline models}{30}{subsection.2.2.3}}
\newlabel{sec:baseline}{{2.2.3}{30}{Baseline models}{subsection.2.2.3}{}}
\citation{ILSVRCarxiv14}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Experiments}{31}{section.2.3}}
\newlabel{sec:experiments}{{2.3}{31}{Experiments}{section.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Image datasets}{31}{subsection.2.3.1}}
\newlabel{sec:dataset}{{2.3.1}{31}{Image datasets}{subsection.2.3.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces \textit  {Flickr image caption datasets.}\relax }}{31}{table.caption.5}}
\newlabel{tab:flickr}{{2.1}{31}{\textit {Flickr image caption datasets.}\relax }{table.caption.5}{}}
\citation{rubenstein1965contextual}
\citation{miller1991contextual}
\citation{yang2006verb}
\citation{wordsim353}
\citation{radinsky2011word}
\citation{bruni2014multimodal}
\citation{hill2014simlex}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Word similarity experiments}{32}{subsection.2.3.2}}
\newlabel{sec:experiments-wsj}{{2.3.2}{32}{Word similarity experiments}{subsection.2.3.2}{}}
\newlabel{rev:similarity_example}{{2.3.2}{32}{Word similarity experiments}{subsection.2.3.2}{}}
\citation{recchia2012semantic}
\citation{bruni2014multimodal}
\citation{bruni2014multimodal}
\citation{bruni2014multimodal}
\citation{turney2011literal}
\citation{bruni2014multimodal}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Effect of concreteness on similarity judgments}{33}{subsection.2.3.3}}
\newlabel{sec:effect-concrete}{{2.3.3}{33}{Effect of concreteness on similarity judgments}{subsection.2.3.3}{}}
\citation{nelsonuniversity}
\newlabel{rev:partition_concreteness}{{2.3.3}{34}{Effect of concreteness on similarity judgments}{subsection.2.3.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.2}{\ignorespaces \textit  {Summary of the word-similarity benchmarks, showing the number of word pairs in the benchmarks and the size of their overlap with the F8k and F30k data sets. The table also reports the average concreteness of the whole, concrete and abstract portions of the benchmarks.}\relax }}{34}{table.caption.6}}
\newlabel{tab:benchmarks}{{2.2}{34}{\textit {Summary of the word-similarity benchmarks, showing the number of word pairs in the benchmarks and the size of their overlap with the F8k and F30k data sets. The table also reports the average concreteness of the whole, concrete and abstract portions of the benchmarks.}\relax }{table.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}Word production}{35}{subsection.2.3.4}}
\newlabel{sec:experiments-production}{{2.3.4}{35}{Word production}{subsection.2.3.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{Multi-word image descriptions.}{35}{subsection.2.3.4}}
\newlabel{rev:stopword}{{2.3.4}{35}{Multi-word image descriptions}{subsection.2.3.4}{}}
\newlabel{eq:proba}{{2.1}{35}{Multi-word image descriptions}{equation.2.3.1}{}}
\citation{ILSVRCarxiv14}
\newlabel{eq:likelihood}{{2.2}{36}{Multi-word image descriptions}{equation.2.3.2}{}}
\newlabel{eq:logmle}{{2.3}{36}{Multi-word image descriptions}{equation.2.3.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{Single-concept image descriptions}{36}{figure.caption.7}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces \textit  {Multiword image description example. Below the image are the 5 captions describing the image, the union of words that we take as targets, the top 5 predicted and the list of correct words and the P@5 score for the given test case.}\relax }}{37}{figure.caption.7}}
\newlabel{fig:multiword-descriptors}{{2.2}{37}{\textit {Multiword image description example. Below the image are the 5 captions describing the image, the union of words that we take as targets, the top 5 predicted and the list of correct words and the P@5 score for the given test case.}\relax }{figure.caption.7}{}}
\citation{efron1982jackknife}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces \textit  {Example of the Single-concept image description task from the validation portion of the ILSVRC2012 subset of ImageNet. The terms "sea anemone" and "anemone" are unknown to \textsc  {Visual} and "animal" is the first word among it's hypernyms that appear in the vocabulary of F30k.}\relax }}{38}{figure.caption.8}}
\newlabel{fig:synset-descriptors}{{2.3}{38}{\textit {Example of the Single-concept image description task from the validation portion of the ILSVRC2012 subset of ImageNet. The terms "sea anemone" and "anemone" are unknown to \textsc {Visual} and "animal" is the first word among it's hypernyms that appear in the vocabulary of F30k.}\relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Results}{39}{section.2.4}}
\newlabel{sec:results_intro}{{2.4}{39}{Results}{section.2.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Word similarity}{39}{subsection.2.4.1}}
\newlabel{sec:res_wordsim}{{2.4.1}{39}{Word similarity}{subsection.2.4.1}{}}
\newlabel{rev:word2vec}{{2.4.1}{39}{Word similarity}{subsection.2.4.1}{}}
\citation{bruni2014multimodal}
\newlabel{rev:wordsim_details}{{2.4.1}{40}{Word similarity}{subsection.2.4.1}{}}
\newlabel{rev:variability}{{2.4.1}{40}{Word similarity}{subsection.2.4.1}{}}
\newlabel{ref:small_correlation}{{2.4.1}{40}{Word similarity}{subsection.2.4.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{Concreteness}{40}{table.caption.10}}
\newlabel{sec:concreteness}{{2.4.1}{40}{Concreteness}{table.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces \textit  {Comparison of models on approximating word similarity judgments. The length of the bars indicate the size of the correlation measured by Spearman's $\rho $, longer bars indicate better similarity between the models' predictions and the human data. The labels on the y-axis contain the names of the data sets and indicate the number of overlapping word pairs with the vocabulary of the F30k data set. All models were trained on the training portion of the F30k data set.}\relax }}{41}{figure.caption.9}}
\newlabel{fig:wsj-online}{{2.4}{41}{\textit {Comparison of models on approximating word similarity judgments. The length of the bars indicate the size of the correlation measured by Spearman's $\rho $, longer bars indicate better similarity between the models' predictions and the human data. The labels on the y-axis contain the names of the data sets and indicate the number of overlapping word pairs with the vocabulary of the F30k data set. All models were trained on the training portion of the F30k data set.}\relax }{figure.caption.9}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.3}{\ignorespaces \textit  {Word similarity correlations with human judgments measured by Spearman's $\rho $. Models were trained on the training portion of the F30k data set. The * next to the values marks the significance of the correlation at level $p<0.05$. The confidence intervals for the correlation are estimated using bootstrap.}\relax }}{42}{table.caption.10}}
\newlabel{table:wsj-online}{{2.3}{42}{\textit {Word similarity correlations with human judgments measured by Spearman's $\rho $. Models were trained on the training portion of the F30k data set. The * next to the values marks the significance of the correlation at level $p<0.05$. The confidence intervals for the correlation are estimated using bootstrap.}\relax }{table.caption.10}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.4}{\ignorespaces \textit  {The table reports the Spearman rank-order correlation coefficient on the abstract and concrete portions of the data sets separately as well as the confidence intervals around the effect-sizes estimated by using bootstrap. The * next to the values indicates significance at level $p < 0.05$.}\relax }}{43}{table.caption.11}}
\newlabel{tab:conca}{{2.4}{43}{\textit {The table reports the Spearman rank-order correlation coefficient on the abstract and concrete portions of the data sets separately as well as the confidence intervals around the effect-sizes estimated by using bootstrap. The * next to the values indicates significance at level $p < 0.05$.}\relax }{table.caption.11}{}}
\newlabel{rev:concrete-rank}{{2.4.1}{43}{Concreteness}{table.caption.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Word production}{43}{subsection.2.4.2}}
\newlabel{fig:concreteness}{{\caption@xref {fig:concreteness}{ on input line 161}}{44}{Concreteness}{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces \textit  {Models' performance on word similarity judgments as a function of the concreteness of the word pairs.}\relax }}{44}{figure.caption.12}}
\@writefile{toc}{\contentsline {subsubsection}{Multi-word image descriptors}{44}{subsection.2.4.2}}
\newlabel{sec:multi-word}{{2.4.2}{44}{Multi-word image descriptors}{subsection.2.4.2}{}}
\citation{ILSVRCarxiv14}
\newlabel{rev:comined metric}{{2.4.2}{45}{Multi-word image descriptors}{subsection.2.4.2}{}}
\newlabel{rev:intuitive multiword}{{2.4.2}{45}{Multi-word image descriptors}{subsection.2.4.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{Single-concept image descriptors}{45}{table.caption.13}}
\@writefile{lot}{\contentsline {table}{\numberline {2.5}{\ignorespaces \textit  {Results for the multi-word image descriptors experiments reported on the test sets of F8k and F30k. Words@5 the number of correctly retrieved word types in the top 5. The confidence intervals below P@5 scores were estimated using bootstrap.}\relax }}{46}{table.caption.13}}
\newlabel{tab:precision}{{2.5}{46}{\textit {Results for the multi-word image descriptors experiments reported on the test sets of F8k and F30k. Words@5 the number of correctly retrieved word types in the top 5. The confidence intervals below P@5 scores were estimated using bootstrap.}\relax }{table.caption.13}{}}
\citation{ILSVRCarxiv14}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces \textit  {The caption above the images show the target labels, the hypernyms that were considered as a new target if the original was not in the vocabulary and the top $N$ predicted words. In a large number of cases the guesses of the model are conceptually similar to the images, although, do not actually overlap with the labels or the hypernyms.}\relax }}{47}{figure.caption.14}}
\newlabel{fig:pretty}{{2.6}{47}{\textit {The caption above the images show the target labels, the hypernyms that were considered as a new target if the original was not in the vocabulary and the top $N$ predicted words. In a large number of cases the guesses of the model are conceptually similar to the images, although, do not actually overlap with the labels or the hypernyms.}\relax }{figure.caption.14}{}}
\citation{louwerse2011symbol}
\citation{bruni2014multimodal}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Discussion and conclusion}{48}{section.2.5}}
\newlabel{sec:discussion}{{2.5}{48}{Discussion and conclusion}{section.2.5}{}}
\citation{alishahi2012concurrent}
\@setckpt{chapters/TAL/main}{
\setcounter{page}{50}
\setcounter{equation}{3}
\setcounter{enumi}{0}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{9}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{2}
\setcounter{section}{5}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{6}
\setcounter{table}{5}
\setcounter{ContinuedFloat}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{r@tfl@t}{0}
\setcounter{parentequation}{0}
\setcounter{float@type}{8}
\setcounter{algorithm}{1}
\setcounter{ALG@line}{18}
\setcounter{ALG@rem}{0}
\setcounter{ALG@nested}{0}
\setcounter{ALG@Lnr}{2}
\setcounter{ALG@blocknr}{10}
\setcounter{ALG@storecount}{0}
\setcounter{ALG@tmpcounter}{0}
\setcounter{treecount}{0}
\setcounter{branchcount}{0}
\setcounter{dt@labelid}{0}
\setcounter{DefaultLines}{2}
\setcounter{DefaultDepth}{0}
\setcounter{L@lines}{0}
\setcounter{L@depth}{0}
\setcounter{Item}{0}
\setcounter{Hfootnote}{9}
\setcounter{Hy@AnnotLevel}{0}
\setcounter{bookmark@seq@number}{11}
\setcounter{eu@}{0}
\setcounter{eu@i}{0}
\setcounter{mkern}{0}
\setcounter{lofdepth}{1}
\setcounter{lotdepth}{1}
\setcounter{@pps}{0}
\setcounter{@ppsavesec}{0}
\setcounter{@ppsaveapp}{0}
\setcounter{NAT@ctr}{0}
\setcounter{section@level}{0}
}
